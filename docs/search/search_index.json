{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The DS2 Software Documentation","text":"<p>To simplify the understanding of DS2 modules and the reference architecture, the modular components of DS2 have been divided into Tiers.</p>"},{"location":"#tier-0-ds2-support-orientated","title":"Tier 0: DS2 Support Orientated","text":"<p>These represent a range of modules that are perceived as cross-cutting to the other tiers and their modules.</p>"},{"location":"#tier-1-ds2-marketplace-and-deployment-orientated","title":"Tier 1: DS2 Marketplace and Deployment orientated","text":"<p>These support the acquisition, porting, and deployment of modules at participants or service intermediaries</p>"},{"location":"#tier-2-ds2-in-data-space-enablement","title":"Tier 2: DS2 In-Data Space Enablement","text":"<p>The modules facilitate consumer -provider participants in sharing their data. The modules are in general used by a single participant only in their local environment but in some cases, there are additional features to be used between data provider and consumer.</p>"},{"location":"#tier-3-ds2-inter-data-space-enablement","title":"Tier 3: DS2 Inter Data Space Enablement","text":"<p>To an extent, the logic for this layer is the reverse of tier 2.  These modules can be allocated to the Tier 2 situation but their added value is much greater in a cross-sector, cross dataspace scenario. </p>"},{"location":"modules/CAT/","title":"CAT","text":"<p>Use the table below as an example and replace with the links to the module's DS2 GitHub repository and GitHub project.  </p> Project Links Software GitHub Repository https://github.com/ds2-eu/cat.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/38"},{"location":"modules/CAT/#general-description","title":"General Description","text":"<p>Paste in here the Introduction section from your module's architecture document Purpose and Description.</p>"},{"location":"modules/CAT/#architecture","title":"Architecture","text":"<p>Leave the following text referencing the figures and replace the names for the corresponding images of your module. Both images can be found in your architecture document.  The reference to the IDT image is left for information purpose so that you know how to add images.  Please delete them once you add your image references.</p> <p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/CAT/#component-definition","title":"Component Definition","text":"<p>Paste in here the Component Definition section from the module's architecture document without the Architecture diagram, since it already in the previous section.</p>"},{"location":"modules/CAT/#screenshots","title":"Screenshots","text":"<p>Display an animated gif with 4 representative screenshots of the module. You can build a gif from a list of images with different utilities ie. power point and export to gif. The idt reference is left for information. Please delete it once you add your module reference. </p>"},{"location":"modules/CAT/#commercial-information","title":"Commercial Information","text":"<p>Table with the organisation, license nature (Open Source, Commercial ... ) and the license. Replace with the values of your module.</p> Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/CAT/#top-features","title":"Top Features","text":"<p>Add 8 - 12 relevant features of the module. This features are not the functionalities defined in the architecture document and GitHub project, even though some could be. This features are more like business or marketing oriented features. Please see example of the idt in the on-line doc at https://ds2-eu.github.io/documentation/modules/idt/ .</p>"},{"location":"modules/CAT/#how-to-install","title":"How To Install","text":"<p>The how to install  has the following sections: requirements, software, summary of installation steps and detailed steps. In some cases, you may not have some of it, like for instance, the software section, or maybe you can't add this yet. Please leave the sections but add the text \"N/A\" or \"To Be Done\" depending on the module.</p> <p>So far, the how to install will be pretty much clone repo and run docker in some cases. Maybe in some others build software and run ...</p> <p>At a later stage, when idt integration is done, there will be a section for Module IDT installation.</p>"},{"location":"modules/CAT/#requirements","title":"Requirements","text":"<p>Add the minimum and recommended list of requirements in terms of CPU, RAM and Storage.</p>"},{"location":"modules/CAT/#software","title":"Software","text":"<p>Add a list of software utilities that form the module. This may not be necessary for most modules, but it is for the idt for instance.</p>"},{"location":"modules/CAT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>Add a summary of installation steps.</p>"},{"location":"modules/CAT/#detailed-steps","title":"Detailed steps","text":"<p>Add the detailed steps of installation including screenshots or code snippets.</p>"},{"location":"modules/CAT/#how-to-use","title":"How To Use","text":"<p>Add in here the steps to use the module from the perspective of your target user. Similar to how to install, add screenshots and code snippets.</p>"},{"location":"modules/CAT/#other-information","title":"Other Information","text":"<p>Add in this section other information relevant to your module that you may want to add to the on-line documentation. If you don't have any, please add the following text: No other information at the moment for MODULE.</p>"},{"location":"modules/CAT/#openapi-specification","title":"OpenAPI Specification","text":"<p>Add your open api specification, if you have any. The idea is to be able to include here the swagger UI, but so far just a snapshot or the yaml specification.</p>"},{"location":"modules/CAT/#additional-links","title":"Additional Links","text":"<p>Add in here relevant links for your module. In this section we will also add the link to the module video.</p> <p>Video https://youtube.com/idt</p>"},{"location":"modules/CONT/","title":"Containerisation","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/containerisation Progress GitHub Project https://github.com/orgs/ds2-eu/projects/7"},{"location":"modules/CONT/#general-description","title":"General Description","text":"<p>To allow easy and automated packaging and deployment of modules on the IDT Kubernetes runtime subcomponent environment. The containerisation module leverages on custom Helm Chart descriptors to automatically convert them into full Kubernetes Helm Charts representing the module, based on standard base templates located in the DS2 Portal Marketplace. The Helm Charts are then deployed on the IDT Module. </p> <p>The Containerisation module is a core module to the IDT Broker module that enables deployment of all the DS2 modules in the IDT Broker Kubernetes sub-component. The Containerisation module uses Helm Chart standard base templates describing a DS2 module. Those templates are provisioned by the IDT Broker module and provide the standard for DS2 module deployment in IDT Broker. Base templates are stored in the DS2 Portal Marketplace. Then, when uploading a DS2 module by module developers, to the DS2 Portal Marketplace, a custom Helm Chart descriptor that includes values for those base templates needs to be provided with the module. The Containerisation module will use the descriptor together with the base templates to create the Helm Chart for the DS2 module during deployment time on the IDT Broker.</p> <p>The Containerisation module can work in two different modes:</p> <ul> <li>The standard DS2 working mode: developers upload module Helm Chart descriptor to the DS2 Portal Marketplace. Participants use the IDT Broker Kubernetes UI to deploy the descriptor on the IDT. The Containerisation module is triggered when detecting the deployment of that descriptor, retrieves the base templates from the DS2 Portal Marketplace, creates the full Helm Chart and deploys it on the IDT Kubernetes Runtime sub-component</li> <li>The GitOps way: automatic deployment of the Helm Chart descriptor is triggered by the Source controller sub-component upon detecting a change on the descriptor in the DS2 Portal Marketplace. Then as in the previous mode, the Containerisation module, create the full Helm Chart and deploys it on the IDT. This could be the deployment mode of the DS2 Portal</li> </ul> <p>In both cases, the only difference is how the Helm Chart descriptor is deployed on the IDT either by the participant manually deploying the descriptor, or being automatically deployed by the Source Controller sub-component.</p>"},{"location":"modules/CONT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/CONT/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li> <p>ChartController: The ChartController is a Kubernetes controller, following the Kubernetes controller pattern which keeps track of a new Kubernetes custom resource definition - the \u201cHelmChartDescriptor\u201d. When changes are detected on a descriptor, ie. addition, update, the Controller connects to a configured location ie. GitHub repository, to download the corresponding Helm Chart base templates. Then, together with the HelmChartDescriptor, the Chart Controller will create a full Helm Chart describing the module. This Helm Chart will be deployed into the IDT Kubernetes Runtime subcomponent using the Installer component. </p> </li> <li> <p>ChartManager: The ChartManager is mainly used to monitor the Helm Charts and HelmChartDescriptors deployed in the system. It will query the IDT Module\u2019s Kubernetes subcomponent to retrieve current Charts and descriptors. The Chart Manager can also be used to create a HelmChartDescriptor using some input parameters and install it via the Installer component. Once installed, the ChartController will detect the new ChartDescriptor and will convert it to a Chart deploying it back into the IDT Module\u2019s Kubernetes subcomponent. </p> </li> <li> <p>Installer: This is the component responsible for installing Helm Charts and HelmChartDescriptors in the IDT Kubernetes subcomponent. It will receive the corresponding Charts and HelmChartDescriptors and will apply them in the IDT Kubernetes subcomponent. The Installer also takes care of installing new Sources created by the Source Manager component.  </p> </li> <li> <p>Containerisation UI: This is the main module UI that allows users to monitor current existing Charts, ChartDescriptors and Sources in the system. Users will have an overview of what is installed in the system and its current status regarding to those specific resources. The UI can also be used to create, update or delete ChartDescriptors via the ChartManager and Sources via the Source Manager.  </p> </li> <li> <p>GitOps Source Controller: The Source Controller, similar to the ChartController,  is a Kubernetes controller that keeps track of the custom resource definition Source. A Source mainly represents a reference to a repository where ChartDescriptors are stored. The Source Controller monitors the status of the Source and reacts to changes by reflecting those changes in the IDT Kubernetes subcomponent. The Source Controller is an optional subcomponent, and users can just install the ChartDescriptors using the IDT or via Kubernetes standard kubectl. </p> </li> <li> <p>(DS2) GitOps Source Manager: The Source Manager, similar to the ChartManager is mainly used to monitor the Source in the system and is customised to DS2. It can also be used to create, update, and delete new sources that will be installed via the Installer component. As the Source Controller, this is an optional component. </p> </li> <li> <p>Tier 1 Service Stack for Marketplace and deployment and API: The full stack will be implemented as generically described elsewhere in this document. Exceptions: This module runs in the IDT and uses the IDT Kubernetes subcomponent for Chart and ChartDescriptor installations. The DS2 Portal Marketplace component and its repository system is used to store the Chart base templates. Since the DS2 Portal is also a DS2 module, it is deployed and run on the IDT, so Containerisation module can also be used for the DS2 Portal and other intermediary services. </p> </li> </ul>"},{"location":"modules/CONT/#screenshots","title":"Screenshots","text":"<p>The Containerisation UI development has not yet been started, so no screenshots. </p>"},{"location":"modules/CONT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/CONT/#top-features","title":"Top Features","text":"<ol> <li>Kubernetes Native: The CONT module is a Kubernetes native solution based on open-source system Flux and The GitOps Toolkit</li> <li>Kubernetes Application Deployment Control: Provides control to Kubernetes administrators or SREs over what and how a module or application is deployed on a given Kubernets cluster ie. IDT2. </li> <li>Kubernetes Application Abstraction: The CONT module abstracts developers from the Kubernetes complexity when creating a Kubernetes application, leveraging on a templating system based on Helm Charts and Flux HelmRelease CRD</li> <li>Helm Chart Templates: Ability to create Helm Chart templates for different types of applications</li> <li>Helm Release CRDs Templates: Based on the HelmRelease CRDs from Flux, the CONT module enables the creation of HelmRelease templates that will make use of the Helm Chart Templates</li> <li>Application Management using API: Manage application (module) lifecycle (create, install, uninstall, delet) using the CONT Chart Manager API</li> <li>Containerisation UI: Manage the Containerisation module using a modern web based UI</li> <li>Operator vs Developer View: Access the Containerisation module features with different views depending on the role Operators vs Developer  </li> </ol>"},{"location":"modules/CONT/#how-to-install","title":"How To Install","text":"<p>The Containerisation module will be part of the IDT installation, but a standalone installer is so far provided in order to be able to work with it, which installs Flux Helm and Source Controllers in order to create the Helm Chart from the HelmRelease and Chart templates. </p>"},{"location":"modules/CONT/#requirements","title":"Requirements","text":"<p>The IDT or a Kubernetes cluster is required.</p> <p>Resources:</p>"},{"location":"modules/CONT/#software","title":"Software","text":"<p>Containerisation module so far installs these software utilities and specific tested compatible versions:</p> <ul> <li>Flux Helm Controller (Chart Controller + Installer)</li> <li>Flux Source Controller (Source Controller)</li> <li>Flux Notification Controller (default)</li> </ul> <p>To Be Implemented: * Chart Manager * Source Manager * Containerisation UI</p>"},{"location":"modules/CONT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ol> <li>Clone the repo</li> <li>Install the Containerisation module<ul> <li>Run the command installflux.sh </li> </ul> </li> </ol>"},{"location":"modules/CONT/#detailed-steps","title":"Detailed steps","text":"<p>Installer is not yet fully ready: To Be Done</p>"},{"location":"modules/CONT/#how-to-use","title":"How To Use","text":"<p>When first set of Helm Chart and HelmRelease Templates are ready: To Be Done</p>"},{"location":"modules/CONT/#other-information","title":"Other Information","text":"<p>No other information at the moment for Containerisation</p>"},{"location":"modules/CONT/#openapi-specification","title":"OpenAPI Specification","text":"<p>To Be Done: No API</p>"},{"location":"modules/CONT/#additional-links","title":"Additional Links","text":"<p>Video https://youtube.com/cont</p> <p>Flux https://fluxcd.io/</p> <p>Containerisation Repository https://github.com/ds2-eu/containerisation (Private Link for Project Members)</p>"},{"location":"modules/CUR/","title":"CUR - DS2 Data Curation module","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/cur.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/39"},{"location":"modules/CUR/#general-description","title":"General Description","text":"<p>Purpose: Data obtained from disparate sources runs the risk of remaining siloed unless it can be joined with similar data from other data sets. The process of joining data may require curation of data, such as the conversion of data formats or a matching of disparate column names.  Manual curation of datasets, however, can be a labour-intensive task, and not suited to DS2\u2019s dynamic nature of federating dataspaces.  Additionally, filtering data based on a set of conditions from even a single data source is a task that typically will require programming skills and is both time-intensive and costly.</p> <p>Instead, the CUR module establishes automatic curatation and federation of data through machine learning, allowing users to query data through natural language, without having to understand how to connect or convert data obtained from different sources.</p>"},{"location":"modules/CUR/#description","title":"Description","text":"<p>In order to allow querying of data obtained from various sources, the Curation model harnesses the power of two different technologies, namely relational databases and machine learning Large Language Models (LLMs).  The module stores obtained data in a relational database, and then uses machine learning combined with a knowledge of the database schemas to convert user queries in natural language to database queries (i.e. SQL queries). Given the proper prompts, the LLM is able to automatically determine how to match columns in different tables, even when column names or data formats may differ.  This module uses several different techniques to increase the chances of correctly answering a user query, such as evaluating the results of the generated query before they are returned to the user, and RAG technology to seed LLM SQL generation with examples of similar user queries and their correctly generated SQL calls from previous history.</p> <p>There still will be cases where the generation of an SQL query corresponding to a user's question will fail.  For example, if the available data sources relate to weather conditions in Africa and the user asks a question about food prices in Europe, query formulation will naturally - and correctly - fail.  However, there will be other cases where a query which is appropriate for the data sets will not be properly generated due to reasons of complexity.  In this case, the execution chain of the module will detect the failure and request human-in-the-loop intervention.  At this point, the original query can either be abandoned, or the generated SQL query can be manually corrected and re-executed.  If the subsequent execution succeeds, the user query and corrected SQL will be stored in a vector store to seed future, similar user queries.  Of course it is not expected that the typical user will correct SQL queries, but rather that the CUR module will be trained by an expert in advance to cover anticipated user queries.</p>"},{"location":"modules/CUR/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/CUR/#component-definition","title":"Component Definition","text":"<p>The CUR module consists of two main parts - a configuration component to set up the database tables from the data sources and the component which is given a reference to the stored database base and performs the pipeline required to allow a user to get information from the data sources using natural language queries. In particular, this latter component is composed of a number of stages which include:</p> <ul> <li>The creation of a GUI for user input</li> <li>The creation of an SQL query from a natural language query</li> <li>The execution of the generated SQL query</li> <li>Evaluation of results of the SQL query</li> <li>Human-in-the-loop injection of a corrected SQL query</li> <li>Reformulation of the query results to generate a more readable answer</li> </ul> <p>Within the above stages, there are a number of substages such as RAG retrieval, vector store population, prompt creation etc.</p>"},{"location":"modules/CUR/#screenshots","title":"Screenshots","text":"<p>The following shows how a natural language query asking about pollution levels in the vicinity of the monitoring station, \"MS Raki\u010dan\" can be executed. </p>"},{"location":"modules/CUR/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License IBM Consortium Confidential TBD"},{"location":"modules/CUR/#top-features","title":"Top Features","text":"<ul> <li>Extraction of data based on natural language queries</li> <li>Support for a wide variety of user languages</li> <li>Automatic federation of disparate data sources</li> <li>Human-in-the-loop intervention allows the LLM to self-learn</li> <li>RAG technology supporting hybrid retrieval with both semantic and keyword search</li> <li>Easy to add additional data sources</li> </ul>"},{"location":"modules/CUR/#how-to-install","title":"How To Install","text":""},{"location":"modules/CUR/#setting-up-the-database","title":"Setting up the database","text":"<p>Data obtained from the data sources will accessed from their URLs and stored into a database.  The recommended database is postgresql due to it support for geospatial queries.  The configuration of the database is as follows:</p> <ol> <li>Install postgresql.  See https://www.postgresql.org/download</li> <li>After installing postgresql, it is recommended to add the environment variables, DS2_DB_USERNAME and DS2_DB_PASSWORD to your shell, with username and database password values which were used in installing postgresql in the previous step.</li> <li>Edit the file, \"federation_config.json\" as follows:<ul> <li>Change the value of the \"database\", \"host\" and \"port\" entries as required to match the values used installing the postgresql database.</li> <li>This configuration file is pre-configured with data sources for the DS2 Slovenian, Romanian and Greek use cases.  Other use cases or additional data sources can be added by following the same format.  Note that currently the \"open_api\" field is not used by the software.</li> <li>Any API tokens required by the REST call are shown within brackets, for example:'Authorization: Bearer '.  At runtime, the software will attempt to replace the bracketed value with an environment variable of the same name - for example the environment variable, \"DADS_BEARER_TOKEN\" in this example.  Therefore, configure these values in your environment. <li>To create the SQL database the first time, run <code>python create_database.py --prefix [name]</code>.  The value for [name] needs to correspond exactly to a URI entry name in the federation_config.json file (for example, \"Slovenia\", \"Romania\" or \"Greece\").  As an example, to build the database for the DS2 Slovenian use case, we would run:  <code>python create_database.py --prefix Slovenia</code>.</li>"},{"location":"modules/CUR/#running-the-cur-module","title":"Running the CUR module","text":"<p>In order to run the CUR module, the correct python environment needs to be created.  It is recommended that you create a virtual environment as follows:</p> <ul> <li><code>python -m venv [name]</code>  where \"name\" will be the name of the virtual environment, for example, \"venv\".</li> <li>Activate the virtual environment: On Mac or Linux this will be: <code>source [name]/bin/activate</code>, where the value of \"name\" is from the previous step.</li> <li>Install inside the virtual environment the required modules: <code>pip install requirements.txtx</code></li> </ul> <p>Once the environment is set up, the CUR module can be run.  Note that the CUR module requires access to an LLM. The access key for the LLM can be defined by configuring and environment variable, LLM_APIKEY, in your sheel.  If this is not configured, then at runtime the module will request the key value.</p> <p>The CUR module is invoked as follows:</p> <p><code>python ds2_federation_main.py --prefix [prefix] --url [url]</code></p> <p>where the value for \"prefix\" is the as described in the previous section for setting up the db, and the value for \"url\" is the base address of the LLM (i.e without the model name).  CUR uses llama-3-3-70b-instruct as its default model.  This can be overridden by using the --name and --id command line values (e.g. --name llama-3-3-70b-instruct --id meta-llama/llama-3-3-70b-instruct).  Note that the accuracy of the NL2SQL translation in the CUR module may depend on the model used.</p>"},{"location":"modules/CUR/#requirements","title":"Requirements","text":"<p>An external LLM is required.  No particular requirements for CPU, memory or storage have been identified.</p> <p>Using the dashbutton requires a configured keycloak server to be running.</p>"},{"location":"modules/CUR/#how-to-use","title":"How to use","text":"<p>The CUR GUI is web-based.  When CUR is started as described in the previous step, it will print out where it can be accessed, typically this will be at: <code>http://127.0.0.1:5000</code> for an installation on the local host.</p> <p>To search for data, enter in the What would you like to know? box your query and then click the Ask button.  Note that the query needs to relate to values in the data sets configured in the federation_config.json file.  If the query succeeds, the answer will appear in the Answer box.  If the query fails, then you will be instructed to either correct the generated SQL query, or enter \"skip\".  If you choose to correct the SQL, enter this in the Corrected SQL box and then click on Execute, otherwise click on the \"Skip\" button to move on to a new query.</p>"},{"location":"modules/CUR/#using-the-dashbutton","title":"Using the dashbutton","text":"<p>If the dashbutton is used, then a keycloak server needs to be configured.</p> <p>Reference: <code>https://github.com/ds2-eu/portal/tree/main/dash-button/web-component</code></p> <p>Run keycloak with:</p> <p><code>docker run -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:25.0.2 start-dev</code></p> <p>Go to the keycloak admin screen:</p> <p><code>http://localhost:8080/admin/master/console/</code></p> <p>Click on \"Keycloak\" in the upper left corner to get to \"Create realm\" </p> <p>Create a realm with name \"demo\".</p> <p>Click on \"Clients\" and then \"Create Clients\"</p>"},{"location":"modules/CUR/#additional-links","title":"Additional Links","text":""},{"location":"modules/DDT/","title":"Data Detection and Transformation (DDT)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/ddt.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/22"},{"location":"modules/DDT/#general-description","title":"General Description","text":"<p>Purpose: Dataspaces allow for data to be shared between data providers and data consumers. A lot of data comes from sensors and devices at a high rate. To allow for a well-defined data structure and quality during the data generation and exchange, DDT is a module that can analyse data on the fly.</p> <p>Description: DDT integrates into the data pipeline where data is collected from edge devices and forwarded to cloud platforms. To ensure that this data is trustworthy and shareable across different dataspaces, it is critical to verify its quality. The module subscribes to incoming MQTT topics and evaluates each message against predefined data quality rules.</p>"},{"location":"modules/DDT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/DDT/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li>Core (Designtime)</li> <li> <p>DDT provides a user interface by which</p> <ul> <li>the AI algorithms are developed, tested and deployed to the AI detection component</li> <li>the transformation algorithms are developed, tested and deployed to the transformation component.</li> </ul> </li> <li> <p>Core (Runtime)</p> </li> <li> <p>This is the heart of the Data Detection and Transformation Module. It is responsible for collecting the data input from the external source and pass it on to the AI detection and the transformation components. This uses either the HTTP or the MQTT APIs. In a similar fashion it also collects the information provided by the Cultural and Language module and passes it on to the other components. Once processed and used, the data and results of the data analysis will be sent out to the Information Consumer and the Data Consumer, also via the same HTTP or the MQTT APIs.</p> </li> <li> <p>AI Detection</p> </li> <li>This component supports the execution of AI models developed using Python or other programming languages supported by Apama. It operates on data and returns information on the data worked on like if an anomaly has occurred or if the data quality has decreased. Many models will be provided to the user and the model selection is done together with the T4.2 project partners INDRA and DIGI.</li> <li> <p>It has the two subcomponents Data Inspection and Model Selection where based on the result of the Data Inspection, the appropriate AI model is selected.</p> </li> <li> <p>AI Transformation</p> </li> <li>This component operates on the data passed on by the Core (Runtime) directly. It will detect schema changes and can transform the data into the correct format on the fly. It can also use the information provided by the DS2 Culture and Language module to define rules and limits what the data values should be and act accordingly, e.g. by eliminating out-of-range values.</li> <li> <p>It has the three subcomponents Analyze Data, Select Transformation and Transform Data.</p> </li> <li> <p>APIs: The Core runtime component has the following interfaces</p> </li> <li>HTTP Server: This module will provide a HTTP interface to send and receive data and information from the DS2 Culture and Language module. Different URLs will be specified to distinguish between input and output channels and if the AI Detection or Transformation component should be used.</li> <li>MQTT Broker: This is the de-facto standard for machine-to-machine communicate so an interface is provided to send and receive data from this module via publish and subscribe methods to certain topics. This can be used to specify the input and output channels of this module and also if the AI Detection or Transformation component should be used</li> </ul> <p>External Components Used * Data Source   * The data provider or data consumer, depending on where this module is deployed, configures and selects where the data to be analysed and transformed comes from</p> <ul> <li>Tier 1 Service Stack for Marketplace and Development</li> <li> <p>The module uses the portal to publish its configuration</p> </li> <li> <p>Tier 0 DS2 Support</p> </li> <li> <p>The information on the data like format and schema comes from the Cultural and Language module</p> </li> <li> <p>Data Consumer</p> </li> <li> <p>The data provider or data consumer, depending on where this module is deployed, configures where the transformed data is sent to.</p> </li> <li> <p>Information Consumer</p> </li> <li>The data provider or data consumer, depending on where this module is deployed, configures where the information on anomalous behaviour, possible error conditions and the data quality is sent to.</li> </ul> <p>External Interaction   * User:     The user uses the UI to develop, test and deploy the AI detection algorithms and the transformation algorithms to the corresponding components.</p>"},{"location":"modules/DDT/#screenshots","title":"Screenshots","text":""},{"location":"modules/DDT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Software GmbH Open Source Apache 2.0 (pending)"},{"location":"modules/DDT/#top-features","title":"Top Features","text":"<p>1. Ensuring trust:  By analysing data streamed to or from data spaces in realtime, data quality can be assessed in realtime, increasing the reliability of shared data.</p> <p>2. Configurable data analysis: In order to allow for a realtime quality inspection, DDT enables users to flexibly define data analysis measures by  UI based configuration.</p> <p>3. Attribute based expectations: To define criteria for data quality, users can define expectations based on data attribute level.</p> <p>4. Anomaly detection &amp; correction strategies:  The DDT module can be configured to detect and correct anomalies in the data stream through a transformation such as  interpolation between data points.</p> <p>5. Improving data consumablity:  Syntactic conversion to handle differences in source and target schemas allows for an easier use and integration of dataspace data into existing systems and enhances data federation across disparate data sources.</p> <p>6. Data Syntax Check: In addition to quality rules, DDT will provide an automated mechanism to infer attribute types \u2014 distinguishing  identifiers, measurements, and context data. This supports more context-aware syntax validation.</p> <p>7. Alerting:  DDT can be configured to send alerts, i.e. to inform about encountered data quality issues.</p> <p>8. E2C support: To allow for an easy integration of edge data, DDT supports DS2 E2C module by importing E2C configuration files to  preconfigure attributes in the DDT UI.</p>"},{"location":"modules/DDT/#how-to-install","title":"How To Install","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#requirements","title":"Requirements","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#software","title":"Software","text":"<p>N/A</p>"},{"location":"modules/DDT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#detailed-steps","title":"Detailed steps","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#how-to-use","title":"How To Use","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#other-information","title":"Other Information","text":"<p>No other information at the moment for DDT.</p>"},{"location":"modules/DDT/#openapi-specification","title":"OpenAPI Specification","text":"<p>N/A</p>"},{"location":"modules/DDT/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/DINS/","title":"Data Inspector (DINS)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/dins_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/28"},{"location":"modules/DINS/#general-description","title":"General Description","text":"<p>The Data Inspection Module (DINS) facilitates the configuration and deployment of processes for real-time data analysis, ensuring data quality and compliance with thresholds set by the parties involved. It performs several key functions: generating notifications based on the values of the exchanged data, executing reactions such as sending requests and notifications to external tools, and integrating with models developed to enhance its capabilities. It is a complement to the Data Share Controller which focuses on control information with both modules using the Data Interceptor.</p> <p>The Data Inspection is responsible for analysing the data provided and/or consumed by participants, ensuring the quality of the data shared amongst them. For example, a consumer can use DINS to set up analysis jobs that define rules for data validation. This includes establishing thresholds for specific data values; when these thresholds are exceeded, issues are generated and recorded for monitoring. Additionally, the DINS module can trigger notifications to other modules and components when issues are detected. For instance, it can notify the Data Share Controller Module, which may respond by blocking data sharing if certain limits are reached.</p>"},{"location":"modules/DINS/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment.</p> <p></p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module.</p> <p></p>"},{"location":"modules/DINS/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions: * Data Jobs IDE: This group of components is shared with the T4.1 Policy Enforcement Modules. Some of them will be developed based on existing technology, specifically the Dataflow component of the Onesait Platform open-source product. While T4.1 Policy Enforcement focuses on defining and executing data transformation jobs, the Data Inspection module will enhance the current capabilities of Dataflow to include data inspection, monitoring, and notifications. T6.2 will lead this component development, and T4.1 will be built on the improved version from T6.2. Therefore, T4.1 has a dependency on T6.2, but the opposite is not true.   * IDE UI: This graphical user interface allows users to define Data Inspection Jobs for monitoring data and triggering notifications and alerts. For example, it can evaluate values against specified thresholds. This component is based on the existing INDRA software, with updates needed to support new features for DS2.   * IDE Controller: This component manages Data Inspection Jobs during design time and oversees their deployment and monitoring at runtime. It is based on current INDRA software but requires significant upgrades to split the tool into the IDE component and the Runtime component (potentially several).    * Job Definition Storage: This component stores definitions of Data Inspection Jobs, based on INDRA software, with extensions planned to improve version control of the definitions. The definitions are created through a graphical user interface, enabling users to design Data Inspection Jobs as data pipelines using a drag-and-drop approach. Each data pipeline will include, at a minimum, the configuration of data sources (e.g., data formats) and the rules for inspecting the data (e.g., detecting specific fields and value thresholds). In the end, each data pipeline definition is a JSON document and a set of parameters. They will be stored in a distributed persistence engine.   * Testing Framework: This component will include test definitions and the storage of small datasets for automatic testing. This component will enable users to define automated tests that are executed to validate Data Inspection Jobs before deployment. * Inspection Runtime:    * Runtime Controller: This component will manage the execution of Data Inspection Jobs during runtime. It will define the interface for integrating job execution with external components and will handle the job lifecycle: deployment, upgrade, removal, start, and stop.   * Data Inspection Job: This component represents the runtime execution of a data inspection definition. Each type of data inspection supported will require a Data Inspection Job definition. One instance of this component will be created for each Data Inspection Job needed at runtime, even for the same definition. The creation of these instances will be managed by the Runtime Controller. The Data Inspection Job includes the definition of an SDK and interfaces that facilitate the extension of capabilities, such as supporting additional data formats.   * Job Logger: This component logs all relevant information about each job execution. Based on INDRA infrastructure, it will require minimal development to adapt to changes in other module components.   * Job States Storage: This component stores the states of job executions throughout their lifecycle, enabling job resumption in the event of failures during execution. Based on INDRA infrastructure, it will require minimal development. * Monitoring and Historic Data: This component will store historic metrics gathered by the analysis jobs. This set of elements will be based on the open-source stack of monitoring tools Grafana.   * Metrics/Alert Collector: This component will collect all the data from the Analysis Jobs.   * Alert Manager: It collects alerts, manages them by categorizing and prioritizing, and allows for the configuration of notification channels such as email and Teams. It also provides a centralized interface for tracking and managing active alerts.   * Inspector Dashboard: It allows the visual analysis of the data inspected. * Model Deployed: If a complex analysis is required for data inspection, the Data Inspection Jobs will have the capability to use data models deployed with the T4.2 Data Model Development Toolkit Module. For example, a field value could be used as input in a prediction model, and based on the result, a decision can be made whether an alert should be generated. * Data Share Controller: The Data Interception part of the Data Share Controller connects data in the data pipeline with the Data Inspection. If the Data Share Controller is not present, any other software that implements the Data Inspection API can provide the data. The Data Inspection will trigger notifications to the Data Share Controller Module (and other modules or components, if required) based on rules defined by the participants. For example, \u201cstop data sharing\u201d. * Other DS2 Modules / Other Participant\u2019s Software: The Module can notify any software that needs to receive these notifications, provided that the software is compatible with the notification mechanisms implemented by DINS. This capability facilitates the integration of systems that can react to data shared between participants, such as the Data Share Controller.</p>"},{"location":"modules/DINS/#screenshots","title":"Screenshots","text":""},{"location":"modules/DINS/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Indra Open Source Apache 2.0"},{"location":"modules/DINS/#top-features","title":"Top Features","text":"<ul> <li>Connect with a large of data origins to obtain data.</li> <li>Definition of rules to analyze data transfer based on data values, data format, etc.</li> <li>Real time monitoring of the processes.</li> <li>Process template management.</li> <li>Integrated with EDC Connector.</li> <li>Capability to perform many data transformations: data format, enrich data with other sources, anonymization, masking, etc.</li> </ul>"},{"location":"modules/DINS/#how-to-install","title":"How To Install","text":"<p>TBC</p>"},{"location":"modules/DINS/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/DINS/#software","title":"Software","text":"<p>TBC</p>"},{"location":"modules/DINS/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/DINS/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/DINS/#how-to-use","title":"How To Use","text":"<p>TBC</p>"},{"location":"modules/DINS/#other-information","title":"Other Information","text":"<p>TBC</p>"},{"location":"modules/DINS/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/DINS/#additional-links","title":"Additional Links","text":"<p>Add in here relevant links for your module. In this section we will also add the link to the module video.</p> <p>Documentation https://dataflow-docs.onesaitplatform.com/</p>"},{"location":"modules/DRM/","title":"DRM","text":"<p>Use the table below as an example and replace with the links to the module's DS2 GitHub repository and GitHub project.  </p> Project Links Software GitHub Repository https://github.com/ds2-eu/drm.git Progress GitHub Project https://github.com/ds2-eu/DRM/issues"},{"location":"modules/DRM/#general-description","title":"General Description","text":"<p>Purpose: To enhance the management and security of digital asset transactions through a robust blockchain-based Data Rights Management (DRM) system. It is designed to perform critical functions, including the notarization, tracking, and validation of all data rights transactions both within individual Dataspaces and across multiple participating Dataspaces </p> <p>Description: The Blockchain-based Data Rights Management module (based on the IDSA clearing house module specifications) will be a two-fold solution that will provide both inter (between Data Spaces) and intra (within Data Spaces) connectivity for data-sharing between Data providers and Data consumers of the participating Dataspaces.</p>"},{"location":"modules/DRM/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/DRM/#component-definition","title":"Component Definition","text":"<p>These modules have the following subcomponent and other functions: \u2022   DRM UI: This component is a dashboard that allows users to view logs and transactions occurring between or across dataspaces. Users can easily access detailed records of their interactions with other partners. This dashboard interacts with the DRM UI Backend subcomponent, which performs the necessary transformations of data extracted from the ledger extractor to be appropriately displayed on the dashboard and supports all necessary CRUD operations.</p> <p>\u2022   DRM UI Backend: This subcomponent retrieves log information extracted from the ledger and performs additional transformations to make the data viewable on the dashboard. It interacts with the Ledger Extractor to access the requested data and communicates with the DRM UI to display the transformed data to users.</p> <p>\u2022   Ledger Extractor: This subcomponent securely extracts data from the ledger, converts it into a readable format, forwards it to other services that require access to ledger information. It securely interacts with the DRM Blockchain Manager to retrieve the necessary logs that are stored to the Blockchain\u2019s ledger, DRM UI Backend to provide information that need to be viewed in UI and with Policy Agreement and Enforcement through API to provide information that is needed for the app.</p> <p>\u2022   Connector monitoring: This subcomponent is designed to retrieve logs from the connector and forward them to the blockchain for secure storage on the ledger. It interacts with the DRM Blockchain Manager to ensure that these logs are effectively recorded in the ledger. It is compatible with EDC connector and IDSA connector. This component is optional, allowing participants to use it solely for DRM to log transactions through supported APIs or restrict its use to DS2 tools if preferred. </p> <p>\u2022   DRM Blockchain Core: This subcomponent is the core functionality of the blockchain incorporating essential components to initiate the blockchain network. It provides the functionality of storing immutable, verify information and encrypt information to the ledger. It interacts directly with the ledger to store information securely and works in conjunction with the Blockchain Manager to store additional logs provided by other DS2 modules.</p> <p>\u2022   DRM Blockchain Manager: The subcomponent is responsible for fetching from the blockchain, transforming incoming data logs into the correct schema so they can be stored on the blockchain, and ensuring they are placed correctly within the blockchain's ledger. Furthermore, it interacts with the DS2 Policy Agreement and Enforcement Module via API to access and provide logs essential for its operations. Additionally, it interacts with other DS2 modules, that interact with data, via API to store logs about data process. </p>"},{"location":"modules/DRM/#screenshots","title":"Screenshots","text":""},{"location":"modules/DRM/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ATC Open Source Apache 2.0"},{"location":"modules/DRM/#top-features","title":"Top Features","text":"<ol> <li> <p>End-to-End Data Accountability: Provides full traceability and transparency of data usage events, ensuring organizations can demonstrate compliance with data responsibility and sharing agreements.</p> </li> <li> <p>Trusted Log Verification: Enables organizations to verify and audit logs in a tamper-evident way, fostering trust between data providers, consumers, and intermediaries.</p> </li> <li> <p>Cross-Organizational Visibility: Offers a unified interface for viewing data-related events across systems and partners, improving governance in federated or multi-stakeholder environments.</p> </li> <li> <p>Plug-and-Play Integration: Designed to integrate with existing DS2 components and Eclipse Dataspace Connectors with minimal effort, reducing the barrier to adoption.</p> </li> <li> <p>Configurable Policy Monitoring: Collects and displays policy-related events (e.g., usage constraints, violations), supporting proactive governance and dynamic enforcement of data policies.</p> </li> <li> <p>User-Centric Access to Ledger Insights: Through a visual interface, users can easily access and interpret ledger data without needing technical knowledge, democratizing access to compliance insights.</p> </li> <li> <p>Modular and Scalable Architecture: Built with modular sub-components, allowing organizations to deploy the module in stages or scale it based on operational needs.</p> </li> <li> <p>Designed for Interoperability in Data Spaces: Addresses real-world challenges of interoperability and trust in data spaces by aligning with emerging data sovereignty and governance requirements.</p> </li> <li> <p>Enhances Digital Trust for Data Sharing: By proving that data usage is being tracked and governed, the DRM module helps build confidence and reduce friction in data sharing collaborations.</p> </li> </ol>"},{"location":"modules/DRM/#how-to-install","title":"How To Install","text":""},{"location":"modules/DRM/#requirements","title":"Requirements","text":"<p>CPU: 8 vCPUs</p> <p>RAM: 16 GB</p> <p>Storage: 100 GB SSD (expandable depending on ledger size and log retention)</p> <p>OS: Ubuntu 22.04 LTS or equivalent</p> <p>Docker: Docker Engine \u2265 20.x, Docker Compose \u2265 2.x</p>"},{"location":"modules/DRM/#software","title":"Software","text":"<ol> <li>Hyperledger Fabric</li> <li>Docker</li> <li>Docker Compose</li> <li>Node.js</li> <li>Express.js</li> <li>CouchDB</li> <li>Swagger / OpenAPI</li> <li>Postman (Optional, used during development)</li> </ol>"},{"location":"modules/DRM/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>To Be Done</p>"},{"location":"modules/DRM/#detailed-steps","title":"Detailed steps","text":"<p>To Be Done</p>"},{"location":"modules/DRM/#how-to-use","title":"How To Use","text":"<p>To Be Done</p>"},{"location":"modules/DRM/#other-information","title":"Other Information","text":"<p>No other information at the moment for DRM.</p>"},{"location":"modules/DRM/#openapi-specification","title":"OpenAPI Specification","text":"<p>\u2022   DRM UI: N/A</p> <p>\u2022   DRM UI Backend: open-api/swagger-ui-backend.json</p> <p>\u2022   Ledger Extractor: open-api/swagger-ledger-extractor.json</p> <p>\u2022   Connector monitoring: To Be Done</p> <p>\u2022   DRM Blockchain Core: N/A</p> <p>\u2022   DRM Blockchain Manager: open-api/swagger-blockchain-manager.json</p>"},{"location":"modules/DRM/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/DSHARE/","title":"Datashare (DSHARE)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/dshare.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/3"},{"location":"modules/DSHARE/#general-description","title":"General Description","text":"<p>Purpose: To provide a user-orientated view of control plane information related to a specific exchange of data to monitor its status and to potentially limit or block it. It will access data through a Data Interceptor component which it shares with the DS2 Data Inspection component (DINS) which operates more at the data level. It can be seen as an In-Dataspace enablement module. Its role is especially important in an Inter-DS environment to provide extra monitoring and control of the data exchanges when partners are less known. Description: The DS2 DSHARE is for it to access control data regarding an exchange via the common Data Interceptor component and an API to the used connector - either within IDT or a specific Dataspace one. It will then log and monitor this information and allow it to be presented in user-friendly form. For short duration one-shot type transactions, this is more of an after-the-event easy-viewer. However, for longer duration transactions (e.g., querying records over a period of time) then it allows the user themselves to monitor the flow and perform control-type actions such as limiting or blocking the transaction.</p>"},{"location":"modules/DSHARE/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/DSHARE/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponents and other functions (as detailed in Data Share.pdf, pages 3-4): * Data Share Controller     * Data Share Manager: The primary module that onboards control data (from Connector, Interceptor, Trust environment), stores it in the DSC DB, correlates it, and handles triggers for data actions (limit/block).     * Data Share UI: For configuration, visualization of exchange-related data, and control actions (limiting, blocking).     * DSC DB: Stores component data for use by the UI and Data Share Management. * Connector and API: Primarily the connector within IDT; other local connectors will be explored. APIs (existing or extensions) service data to the Data Share Manager. * Tier 1 Service Stack for Marketplace and deployment and API: Generic DS2 stack implementation (Platform not used by DSHARE). * Tier 2: Data Inspector Manager and API: DINS may trigger DSHARE if anomalies suggest blocking data transfers. * Tier 3: Trust Environment and API: Feeds static agreement information to the Data Share Manager for visualization and control decisions. * Data Share Interceptor and API     * Interceptor: Intercepts data/query streams between IDT/Connector and participant's Business Application/Datastores. Interfaces with DSHARE (DSC) and DINS. Capable of receiving block/limit commands. Research ongoing for interception techniques (man-in-the-middle vs. duplicator).     * Interceptor UI: For configuring the Interceptor (I/O). * Participant DB/Application: Represents business applications feeding data to/receiving queries from the connector.</p>"},{"location":"modules/DSHARE/#screenshots","title":"Screenshots","text":""},{"location":"modules/DSHARE/#commercial-information","title":"Commercial Information","text":"<p>Table with the organisation, license nature (Open Source, Commercial ... ) and the license. Replace with the values of your module.</p> Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/DSHARE/#top-features","title":"Top Features","text":"<ul> <li>Consumer analytics: Comprehensive Data Exchange Monitoring, tracking Real-time of data exchanges between a provider and a consumer. The provider of the app can:<ul> <li>Know active contracts of the consumer</li> <li>Know how much data has been exchanged associated to a given contract</li> <li>Granular Control: Ability to monitor, limit, or block data transfers based on defined policies or user intervention.</li> <li>Advanced Analytics: Features weekly data charts, consumption pattern monitoring, and a consumer ranking system.</li> </ul> </li> <li>Assets analytics: DSHARE provides a cockpit to identify most used assets offered by the provider, weekly and monthly consumption trends helpful for identifying inner problems</li> <li>Alerting System: Notifies users or administrators about approaching or exceeded transfer limits. Alerts are also raisen when consumers have problems accessing an asset (insufficient credentials, etc)</li> </ul>"},{"location":"modules/DSHARE/#how-to-install","title":"How To Install","text":"<p>The module is installed as part of the IDT.</p>"},{"location":"modules/DSHARE/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/DSHARE/#software","title":"Software","text":"<ul> <li>Eclipse Dataspace Connector (EDC) - Specify version if applicable.</li> <li>PostgreSQL Database - Specify version if applicable.</li> <li>Java Development Kit (JDK) - Specify version.</li> <li>Apache Maven - Specify version.</li> </ul>"},{"location":"modules/DSHARE/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/DSHARE/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/DSHARE/#how-to-use","title":"How To Use","text":"<p>Accessing the DSHARE Dashboard: * Navigate to the DSHARE UI URL (e.g., http://:/dshare-ui).  * Analyse consumer data usage: * View real-time data on active transfers. * Analyze weekly data charts for usage trends.  * Data Assets Analytics:  * Alerts Manager:"},{"location":"modules/DSHARE/#other-information","title":"Other Information","text":"<p>No other information at the moment for IDM</p>"},{"location":"modules/DSHARE/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/DSHARE/#additional-links","title":"Additional Links","text":"<p>TBC</p>"},{"location":"modules/DVM/","title":"Data Visualisation Module (DVM)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/dvm.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/56"},{"location":"modules/DVM/#general-description","title":"General Description","text":"<p>NOTE: DVM is a new module and pending an Amendment, hence this document is preliminary and will be subject to change once development starts.</p> <p>Purpose:  DVM provides visualisation functionalities in order to enhance the  presentation and communication capabilities and  facilitate a better understanding of raw data by means of graphical  analysis.</p>"},{"location":"modules/DVM/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p>"},{"location":"modules/DVM/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li>Core (Designtime)</li> <li> <p>User interface to define dashboards, configure data sources for widgets, access control, etc.</p> </li> <li> <p>Core (Runtime)</p> </li> <li>User interface for showing dashboards.</li> <li>Backend for data ingestion.</li> </ul> <p>External Components Used * Data Source   * Data may be streamed from use case partners or developers.   * Module E2C can be configured to stream data to DVM.   * Module DDT can be configured to stream data to DVM.</p> <ul> <li>Information Consumer</li> <li>Data providers</li> <li>Data consumers</li> <li>Module developers</li> </ul> <p>External Interaction   * User:     DVM provides dashboarding functionalities to it's users. Users can     create dashboards, configure widgets, manage data ingestion and view      data using the created dashboards.</p>"},{"location":"modules/DVM/#screenshots","title":"Screenshots","text":""},{"location":"modules/DVM/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License bluebridgesolutions UG (pending Amendment) Open Source Apache 2.0 (pending)"},{"location":"modules/DVM/#top-features","title":"Top Features","text":"<p>1. Data visualisation:  Visualisation of data for data producers and consumers.</p> <p>2. Flexible configuration of dashboards: Adding of new charts for time series data and KPIs by configuration of widgets.</p> <p>3. Enhanced visual analytics: Visualisation simplifies the analysis of data, trends and outlier detection for humans.</p> <p>4. Visualisation of processing results: Modules like DDT produce data such as smoothing of sensor data. DVM is an easy way of  visualizing such results.</p>"},{"location":"modules/DVM/#how-to-install","title":"How To Install","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#requirements","title":"Requirements","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#software","title":"Software","text":"<p>N/A</p>"},{"location":"modules/DVM/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#detailed-steps","title":"Detailed steps","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#how-to-use","title":"How To Use","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#other-information","title":"Other Information","text":"<p>No other information at the moment for DVM.</p>"},{"location":"modules/DVM/#openapi-specification","title":"OpenAPI Specification","text":"<p>N/A</p>"},{"location":"modules/DVM/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/E2C/","title":"Edge to Cloud Connector (E2C)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/e2c.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/24"},{"location":"modules/E2C/#general-description","title":"General Description","text":"<p>Purpose: Dataspaces allow for data to be shared between data providers and data consumers. This includes data coming from sensors and devices at a high rate. This module is used to establish a secure edge-to-cloud connectivity for data providers to cloud-based IoT platforms like Azure IoT or Cumulocity IoT or AWS IoT via a MQTT bridge. The data providers will decide which data to share and with whom. In addition, the data quality can be monitored as well.</p> <p>Description: The DS2 Edge-to-Cloud Connector module (DS2 E2C) will use Software AG\u2019s open-source product thin-edge.io as background. The underlying communication relies on MQTT, a standards-based messaging protocol used for machine-to-machine communication. The data is locally collected on the edge device, mainly IoT-based but not exclusively, and published to the MQTT broker supplied by this module. Depending on the specific cloud platform used by the data provider or data consumer, appropriate rules will get applied to map the generic format to the specific cloud format. The data will constantly be monitored for loss of data quality which will be reported. Using the MQTT-bridge functionality the data and quality information gets mapped to the chosen cloud platform. The communication is encrypted via SSL/TSL using local certificates.</p>"},{"location":"modules/E2C/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/E2C/#component-definition","title":"Component Definition","text":"<p>In E2C, data is collected via a MQTT broker and can be sent to different cloud environments such as Cumulocity, Azure or AWS via adapters built on top of the open-source software thinedge-io. Thin-edge.io is supporting any Linux distribution, in most cases even with the appropriate package manager. In DS2, thin-edge.io will mainly be used to connect on-premises edge devices to cloud environments of the Data Providers.</p> <p>This module has the following subcomponent and other functions:</p> <p>Edge-to-Cloud Connector Module \u2013 Core * MQTT Broker   * This is the central message bus of the module, based on the open-source Mosquitto implementation. * Data Converter   * It reads data, which was published under a certain broker topic, and transforms it to the specific format for the configured cloud platform   * The data converter can use a pipeline of custom routines for specific tasks before ...   * The transformed data is re-published to the MQTT broker under a different topic * Data Quality Module   * The data read by the Data Converter component is constantly analysed by the Analyze Data component of this module to check if the data quality goals are met. The information is collected and sent back to the MQTT broker via the Report Data Quality component. * Other Custom Routines   * Users can write their own routines to perform tasks on their data published to the MQTT broker before it is sent to the cloud platform via the cloud connector. This could be e.g. calculate moving averages, many examples are available in the thin-edge.io plugins directory. * Cloud Connector   * Is a message bus where data can be published under a topic and data consumer can subscribe to these topics</p> <p>External Components Used * Data Source   * The data provider configures and selects where the data to be offered and shared comes from, most likely an edge device * Tier 1 Service Stack for Marketplace and Development   * The module uses the marketplace especially for the configuration of the desired cloud adapter * Tier 3 Inter-Dataspace Sharing   * The module uses the Discover and Trust components for the configuration of the desired cloud adapter  * Cumulocity, Azure, AWS, Custom   * These components represent the possible cloud adapters that can be configured. Besides connecting to the commercial clouds Cumulocity, Azure and AWS, one can write a custom adapter for any IoT platform having a MQTT connector, e.g. the open-source ThingsBoards platform.</p>"},{"location":"modules/E2C/#screenshots","title":"Screenshots","text":""},{"location":"modules/E2C/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Software GmbH Open Source Apache 2.0 (pending)"},{"location":"modules/E2C/#top-features","title":"Top Features","text":"<p>1. Data streaming: Upload relevant data to be shared in dataspaces from devices or existing APIs.</p> <p>2. Cloud platform agnostic: E2C allows to stream data to any cloud platform by it's extensible connectivity feature.</p> <p>3. No vendor lock-in: Most platforms provide dedicated software to connect devices and data sources - by using those, migrating to another platform is a tedious process. In E2C, switching to another supported platform can be as easy as UI-led configuration.</p> <p>4. OpenAPI specification: Import your existing OpenAPI specs to define new data streams from existing APIs.</p> <p>5. Query Builder: Build queries and filters in the user interface to select only the data you need on an attribute level.</p> <p>6. Time windows: Define time windows for data streaming.</p> <p>7. Data mapping: Map data from source to target data models.</p> <p>8. Configure once: Once the streaming services are configured, they can be executed on demand.</p>"},{"location":"modules/E2C/#how-to-install","title":"How To Install","text":"<p>To Be Done</p>"},{"location":"modules/E2C/#requirements","title":"Requirements","text":"<p>To Be DOne</p>"},{"location":"modules/E2C/#software","title":"Software","text":"<p>N/A</p>"},{"location":"modules/E2C/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>To Be Done</p>"},{"location":"modules/E2C/#detailed-steps","title":"Detailed steps","text":"<p>To Be Done</p>"},{"location":"modules/E2C/#how-to-use","title":"How To Use","text":"<p>To Be Done</p>"},{"location":"modules/E2C/#other-information","title":"Other Information","text":"<p>No other information at the moment for E2C.</p>"},{"location":"modules/E2C/#openapi-specification","title":"OpenAPI Specification","text":"<p>N/A</p>"},{"location":"modules/E2C/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/IDM/","title":"Identity Management Module (IDM)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/idm.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/3"},{"location":"modules/IDM/#general-description","title":"General Description","text":"<p>Purpose: The DS2 Identity Module (IDM) is a Foundation module that provides a practical framework for creating, managing, and validating participant and module identities for inter-Dataspace activities. It leverages Verifiable Credentials (VCs) and integrates with technologies like the EDC IdentityHub to ensure secure and trustworthy identity management. The IDM aims to reuse existing identities from different companies and provide robust mechanisms to verify their membership and participation in various dataspaces. Description: The IDM facilitates the secure interaction between dataspaces by managing identities and access rights. Key development efforts have focused on: Verifiable Credentials (VCs): Core to the IDM, VCs allow companies to prove their identity and attributes in a secure and verifiable manner. Wallet Visualisation: Functionality enabling companies to view and manage their VCs. Credential Issuer: DS2 has developed its own credential issuer capable of generating signed VCs for companies, based on dataspace agreements. IDM Portal: A comprehensive web interface (backend and frontend) for managing dataspaces, collaboration agreements, and company registrations within DS2. EDC IdentityHub Integration: Successful integration allows for the hosting and management of VCs. The module supports the registration of dataspaces, the establishment and management of collaboration agreements between them, and the registration of companies as DS2 members, all underpinned by a system of verifiable digital credentials.</p>"},{"location":"modules/IDM/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/IDM/#component-definition","title":"Component Definition","text":"<p>the IDM comprises the following key components: IDM Portal: * Backend: Manages the business logic for dataspace registration, agreement workflows, company registration, and interactions with the credential issuer and wallet functionalities. * Frontend (UIs): Provides user interfaces for:     * Registration of dataspaces in DS2.     * Requesting collaboration and forming agreements between dataspaces.     * Searching for registered dataspaces.     * Accepting or rejecting collaboration agreements.     * Visualizing dataspace policies.     * Registering companies as members of DS2. * DS2 Credentials Issuer:     * An implemented service capable of generating cryptographically signed Verifiable Credentials.     * Issues credentials to companies based on established dataspace agreements and their DS2 membership. * Wallet Visualisation Functionality:     * A user-facing component allowing companies to view and manage their issued Verifiable Credentials.     * Integrated with the EDC IdentityHub for hosting credentials. * EDC IdentityHub Integration Layer:     * Manages the interaction with the EDC IdentityHub, enabling the storage and retrieval of Verifiable Credentials. * Core Identity Logic:     * Underlying mechanisms for verifying membership of companies in their respective dataspaces and managing the lifecycle of identities and credentials within the DS2 ecosystem.</p>"},{"location":"modules/IDM/#screenshots","title":"Screenshots","text":""},{"location":"modules/IDM/#commercial-information","title":"Commercial Information","text":"<p>Table with the organisation, license nature (Open Source, Commercial ... ) and the license. Replace with the values of your module.</p> Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/IDM/#top-features","title":"Top Features","text":"<ul> <li>Verifiable Credential Management: Core functionality for issuing, hosting (via EDC IdentityHub), and visualizing VCs for companies.</li> <li>DS2 Native Credential Issuer: Custom-built service to generate signed credentials based on dataspace agreements.</li> <li>Comprehensive IDM Portal:<ul> <li>Dataspace registration and discovery.</li> <li>Inter-dataspace collaboration agreement management (requests, acceptance/rejection).</li> <li>Visualization of dataspace policies.</li> <li>Company registration for DS2 membership.</li> </ul> </li> <li>EDC IdentityHub Integration: Successfully achieved for robust VC hosting.</li> <li>Wallet Functionality: Allows companies to manage and present their digital credentials.</li> <li>Decentralized Identity Principles: Aligns with modern approaches to identity management, enhancing security and user control.</li> <li>Membership Verification: Mechanisms to verify company membership within specific dataspaces.</li> <li>Secure Inter-Dataspace Communication: Facilitates trust by ensuring participants are authenticated and authorized.</li> </ul>"},{"location":"modules/IDM/#how-to-install","title":"How To Install","text":"<p>The module is installed as part of the IDT.</p>"},{"location":"modules/IDM/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/IDM/#software","title":"Software","text":"<p>TBC</p>"},{"location":"modules/IDM/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/IDM/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/IDM/#how-to-use","title":"How To Use","text":"<p>The IDM is primarily interacted with via the global portal. After the user logs in it provides two main functionality: Management of Dataspaces and Company Verifiable Credentials: * Dataspace Authorities:     * Use the IDM Portal to register their dataspace within the DS2 ecosystem.          * Search for other registered dataspaces to explore potential collaborations.     * Initiate collaboration requests with other dataspaces.          * Review and accept/reject incoming collaboration requests.          * View policies associated with registered dataspaces.      * Companies:     * Register their organization as a member of DS2 via the IDM Portal.          * Once approved and relevant dataspace agreements are in place, the DS2 Credentials Issuer will generate Verifiable Credentials for the company.     * Access the Wallet Visualisation Functionality to view and manage their issued VCs. These VCs can then be used to prove their identity and membership in inter-dataspace interactions.      *System Administrators:     * Manage the overall configuration and operation of the IDM components.</p>"},{"location":"modules/IDM/#other-information","title":"Other Information","text":"<p>No other information at the moment for IDM</p>"},{"location":"modules/IDM/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/IDM/#additional-links","title":"Additional Links","text":"<p>TBC</p>"},{"location":"modules/IDT/","title":"IDT","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/idt.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/3"},{"location":"modules/IDT/#general-description","title":"General Description","text":"<p>IDT is the core enabler of DS2 who purpose is to be deployed in front of participants data source/spaces and network connected to any other IDT-enabled data source. As such its aim is to run all DS2 modules, including the DS2 Connector, the core module for Inter-Dataspace communication and data transfer, and the Containerisation module for DS2 module deployment. The IDT contains the core Kubernetes runtime to run all containerised modules and a series of additional open-source software for module management.</p> <p>The IDT contains the Kubernetes runtime that is the core service to run all modules in a containerised way. Those modules descriptors are uploaded to the DS2 Portal Marketplace and then, using the IDT Kubernetes UI, are deployed to the Kubernetes runtime. The Containerisation module kicks-in and then converts those module descriptors to full module charts effectively deploying the DS2 modules. The Kubernetes UI, alongside the Management and Monitoring Controller, will be used to manage and monitor all DS modules running on a participants IDT. Additional services will also be run as part of the IDT such as certificate management, ingress network traffic management using traditional ingress controllers with ingress resources to expose modules and eventually transitioning to service mesh and gateway API, storage management and possibly other useful open-source tools. The other key component of the IDT is the DS2 Connector used for Dataspace like communications following current IDSA and Gaia-X standards. An additional IDT UI will be provided for module navigation and Connector management.</p>"},{"location":"modules/IDT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/IDT/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li> <p>Kubernetes (Module Runtime): Kubernetes is the leading technology in container orchestration and the choice and key component of the IDT for deployment and integration of the DS2 modules. This is the core IDT subcomponent that runs and orchestrates the DS2 modules and all the other IDT subcomponents, as containers. This is Open-Source software and the current distribution being used is K3s, a lightweight version of Kubernetes easy to install, half the memory, all in a binary, in less than 100 MB among other enhancements. One of the main advantages is the flexibility of installation, since it can be deployed at a participant edge, onPremise, InCloud, etc.</p> </li> <li> <p>Kubernetes UI: The Kubernetes UI is open-source software based on Rancher that allows to deploy and manage Kubernetes in a more user-friendly way both onPremise and InCloud. The Kubernetes UI will provide the management interface for platform administrators and the module deployment interface for participants running the IDT. This interface will be used to deploy the module ChartDescriptors,  configuration files that describe how a module runs on Kubernetes, from the DS2 Portal Marketplace in the IDT. The Containerisation module will then transform the descriptors into full Helm Charts and deploy them to the Kubernetes subcomponent. The Kubernetes UI will also provide the monitoring interface to the IDT Kubernetes subcomponent and the DS2 modules.</p> </li> <li> <p>Management and Monitoring Controller: This is the main interface from the Kubernetes UI to the Kubernetes subcomponent and also for external integrations. The Management and Monitoring Controller is open-source software based on the Kubernetes and Rancher API and the Rancher agents. It is used as the primary interface to the IDT Kubernetes subcomponent for management and deployment of modules. It will also be used as the primary interface for monitoring which will potentially be integrated with the DRM Blockchain module for traceability. In addition, further research on using other modules for monitoring such as Prometheus-Grafana will be conducted for enhanced monitoring.</p> </li> <li> <p>Ingress \u2013 Gateway: The ingress or gateway resource provides the entry point to the IDT Kubernetes subcomponent via the Ingress controller, thus, the IDT network, for all network traffic from external apps, being an external app, any system external to the IDT. It describes how the DS2 modules are exposed outside of the IDT. Initially the modules will use a Kubernetes Ingress resource to expose the modules but further research will be conducted to examine the use of the Gateway API and Service Mesh technology.</p> </li> <li> <p>Ingress Controller \u2013 Service Mesh: Based on Open Source, the Ingress Controller is the Kubernetes controller dealing with Ingress resources, that is, managing the entry point to the IDT and how the DS2 modules are exposed outside of IDT. Further research is expected for replacement of the Ingress Controller with Service Mesh technology and the Kubernetes Gateway API, that adds a transparent layer to provide the IDT with enhanced connectivity, security, control and observability. The use of the Service Mesh could also be a key feature for using more secure communication via mutual TLS protocol (mTLS) in all DS2 communications which provides and additional trust layer. This could also be integrated with the DS2 trust and identity system.</p> </li> <li> <p>Storage Manager: Open-source software to provide the interface between the IDT Kubernetes subcomponent and the physical storage for DS2 stateful modules. This will use Kubernetes native storage technology to allow highly available stateful module deployments in IDT. When data from DS2 modules need to be persisted in a participant backend storage system, the Storage Manager will be used to map current deployment and Kubernetes Persistent Volumes to external storage systems. This is not a storage system or technology for modules\u2026 If DS2 modules need to use storage, the DS2 modules need to provide them by packaging them in their module Chart.</p> </li> <li> <p>CertManager: Based on Open-source software, it provides management of SSL certificates for secure connectivity ie. HTTPS, with verified signed certificates using Let\u2019s Encrypt CertificateAuthority (CA) and configures them for the Ingress or Gateway resource. The CertManager integrates with the Ingress Controller and/or Service Mesh subcomponents and in addition, further research on integration with DS2 Trust system will be explored.</p> </li> <li> <p>DS2 Connector: The DS2 Connector in the IDT is the key element that will allow for DS2 transactions and data exchange, following the IDSA and Gaia-X standards. The Open-source Eclipse EDC Connector (or the Tractus-X extension) will be used to provide interoperability between Dataspaces and secure, trustworthy exchange of data. Following existing Dataspace principles and protocols, the DS2 Connector will use the DS2 Trust system for identity management and will connect to other participants IDT DS2 Connectors in other Dataspaces for data exchange. The Connector will also integrate with the DS2 Catalog or a Dataspace level Metadata Broker for participant and data discovery. </p> </li> <li> <p>Local Identity: This module is optional and it provides local identity, authentication and authorization to access a participant IDT and its modules by the various users types within a company. Based on Open Source Keycloak identity provider software, further research will be done in order to explore the possibility of linking the Local Identity with the DS2 Trust system.</p> </li> <li> <p>Tier 0 Support Service Stack:</p> <ul> <li>DRM and API: For further exploration integration of the monitoring controller with the Blockchain will be considered.</li> </ul> </li> <li> <p>Tier 1 Service Stack for Marketplace and deployment and API: The full stack will be implemented as generically described elsewhere in this document. Exceptions: The IDT uses the DS2 Portal and Marketplace to retrieve the ChartDescriptors of modules and deploy them via the Kubernetes UI. Then the Containerisation module uses the descriptors to deploy the full module Helm Chart. In addition, The DS2 Connector in the IDT integrates with other IDT DS2 Connectors for data exchange.</p> </li> <li> <p>Tier 3 Trust Stack and Catalog and API: The IDT will make use of the relevant parts of the DS2 Trust Stack for certificates in the Ingress Controller \u2013 Service Mesh and identities in the DS2 Connector. The IDT will also connect via the DS2 Connector to the Catalog.</p> </li> <li> <p>External Apps: External Apps refer to any software application external to the IDT and DS2 ecosystem that uses the DS2 Connector in the IDT for any DS2 data transaction. It\u2019s the application that can trigger a data exchange via the Connector, either as a consumer or producer.</p> </li> <li> <p>External Storage Systems: This refers to any external storage system, physical or software defined, that a participant has already in place and where data from the IDT and DS2 ecosystem can be persisted, thus, is mapped via the Storage Manager into the IDT Kubernetes</p> </li> </ul>"},{"location":"modules/IDT/#screenshots","title":"Screenshots","text":""},{"location":"modules/IDT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/IDT/#top-features","title":"Top Features","text":"<ol> <li>Kubernetes Platform: The IDT provides a Kubernetes based platform for ease of integration and deployment of the modules</li> <li>Flexible Installation Process: The IDT provides a user friendly installation process for easy installation to non-experienced users.    In any case, management of the platform itself, will require some expertise. In addition, it supports different installation modes from on-cloud to on-prem and edge.</li> <li>Management Interface: A Rancher based UI is provided for the Kubernetes cluster management. Management of Kubernetes itself will require some expertise. The Management UI  provides the interface and API for module management : deployment, deletion, upgrade</li> <li>Monitoring of Platfom and Apps: Provides the interface and API to monitor the cluster itself and the modules</li> <li>Seamless Integration with Containerisation Deployment: IDT integrates seamlessly with the Containerisation module for ease of module deployment</li> <li>Networking: Provides secure networking and connectivity among the installed apps and to and from outside the cluster</li> <li>Log management: Ability to retrieve module logs for troubleshooting and debugging</li> <li>Native Storage: Provides Kubernetes native storage for stateful applications</li> <li>IDT Portal: Provides a local version of the DS2 Portal as the entry point to the IDT, single sign on and module navigation </li> <li>DS2 Connector: The IDT incorporates the DS2 Connector for DS2 data exchange</li> </ol>"},{"location":"modules/IDT/#how-to-install","title":"How To Install","text":"<p>The IDT installs a pre-packaged enterprise ready Kubernetes cluster along with some extra features for management and deployment.</p>"},{"location":"modules/IDT/#requirements","title":"Requirements","text":"<p>Provision a Linux VM (Ubuntu 18.04 or 20.04) Resources:</p> <p>Kubernetes Node</p> <ul> <li>Minimum: 2 cpu cores, 4 GB RAM and 10 GB disk capacity.</li> <li>Recommended: 4 cpu cores, 8 GB RAM and 50 GB disk capacity.</li> </ul> <p>These numbers may change since a number of IDT components will be deployed, check specific requirements for specific components.</p>"},{"location":"modules/IDT/#software","title":"Software","text":"<p>IDT installs these software utilities and specific tested compatible versions:</p> <ul> <li>Docker</li> <li>K3s (Kubernetes)</li> <li>Helm</li> <li>Cert-manager</li> <li>Rancher</li> <li>Creates a self signed certificate to use by the ingress controller</li> <li>Nginx Ingress Controller</li> <li>Nginx docker (load balancer - optional)</li> <li>DS2 Connector (to do)</li> <li>Core DS2 modules (to do)</li> </ul>"},{"location":"modules/IDT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ol> <li>Clone the repo</li> <li>Deploy IDT<ul> <li>Run the command mini-idt.sh nodeip iface as per instructions. This deploys the Kubernetes platform</li> <li>nodeip: the ip of the node</li> <li>iface: the network interface of the node ip</li> </ul> </li> <li>Access Rancher at https://rancher.$nodeip.ds2.sslip.io where $nodeip uses '-' instead of '.' (The domain can be changed if needed)</li> <li>Register the catalog(s)</li> <li>If http access is needed, run patch_nginx.sh</li> <li>Deploy IDT modules from the Rancher UI (Using Containerisation module at a later stage)<ul> <li>Any script required before deploying any component should be in the idt/modules folder</li> <li>Upon deployment of first module import the self-signed certificate in the trusted CA's store</li> </ul> </li> </ol>"},{"location":"modules/IDT/#detailed-steps","title":"Detailed steps","text":"<ul> <li> <p>Clone the repository <pre><code>git clone https://github.com/ds2-eu/idt.git\n</code></pre></p> </li> <li> <p>Navigate to idt folder <pre><code>cd idt\n</code></pre></p> </li> <li> <p>Run idt <pre><code>./idt.sh ip iface\n</code></pre> where ip is the ip of the vm where k3s will be installed and iface is the network interface of the nodeip for instance ./idt.sh 192.168.50.5 enp0s8</p> </li> </ul> <p>The script will install the software utilities in this order:</p> <ul> <li> <p>First, IDT installs Docker</p> </li> <li> <p>Then, k3s Kubernetes cluster is installed. Once installed, the installation process will wait and check for K3s to be up and running. Helm is installed together with K3s and kubectl.</p> </li> <li> <p>Next cert-manager is deployed in order to provide a ssl certificate for Rancher. Process will wait and check that cert-manager is running.</p> </li> <li> <p>Then Rancher is deployed in the cluster. Process will wait and check that Rancher is running.</p> </li> <li> <p>Next, the nginx ingress controller is deployed. Before this, a self signed ssl certificate is created using certmanager. Notice that the domain that is configured in the certificate, is the one to be used as domain for the modules when deployed to IDT which defaults to *.$nodeip.modules.ds2.sslip.io . This domain can be changed.</p> </li> <li> <p>DS2 IDT is now ready.</p> </li> </ul> <p></p> <ul> <li>Access Rancher by accessing the Rancher url in the browser (https://rancher.$nodeip.ds2.sslip.io)</li> <li>Once in the Rancher UI, the admin password is set</li> <li>Then navigate to the workloads in the system project</li> </ul> <p></p> <ul> <li> <p>The nginx ingress controller is by default set to only accept https connections and redirect to https. In order to use http, run the script patch_nginx.sh which will configure nginx ingress controller to accept http (optional). </p> </li> <li> <p>Now that the cluster is up and running, the Rancher UI can be accessed in order to manage the cluster and install modules. </p> </li> </ul>"},{"location":"modules/IDT/#how-to-use","title":"How To Use","text":"<p>Once the IDT has been installed, the Rancher UI along with the IDT Portal (local Portal) and the core modules (to be done) can be accessed.</p>"},{"location":"modules/IDT/#rancher-ui","title":"Rancher UI","text":"<p>The Rancher UI is the main entry point for Kubernetes cluster management and configuration. </p> <ul> <li> <p>Inspect the cluster and check number of nodes, resources, etc       </p> </li> <li> <p>Create a Module Repository: A user can register a module catalog or repository in order to be able to deploy modules from that module repository. A catalog is just a repository, git or helm, where helm charts are stored. In general, users won't need to create any new repository in the IDT since a default DS2 catalog will be created for the organisation pulling from the organisation repository in DS2 intermediary platform.  If needed, in order to create a new repository, navigate to the cluster, apps, repositories, and click on the \"Add Catalog\" button. Fill in the form with the credentials for a private repository and click \"Create\". The Repository is added to Rancher and the modules will be displayed in the Apps view       </p> </li> <li> <p>Module Deployment: modules can be deployed from the registered Repository, but in general this will be done via the Containerisation module. If needed, navigate to Apps, Charts and the list of available charts (apps) is displayed. Select the chart to be deployed, click on Install, select the Namespace and Name for the instance of the chart and select whether to customize the Helm options before install. If customization is selected, fill in the configuration form and or yaml. Click Next then click Install. The application will be deployed to the platform.       </p> </li> <li> <p>Module logs: In order to review the modules logs, navigate to Apps, Installed Apps, and the list of installed modules is displayed. Select the module to be monitored and the list of Kubernees resources of that module are displayed. Select the Deployment and the Pod is displayed. Click on the three dots on the right and select View Logs. The logs of the module are displayed.        </p> </li> <li> <p>Module deletion: modules can be deleted by navigating to Apps, Installed apps and clicking on the three dots on the right and click on Delete. In general, this will be done via the Containerisation module       </p> </li> </ul>"},{"location":"modules/IDT/#idt-portal","title":"IDT Portal","text":"<p>To Be Done</p>"},{"location":"modules/IDT/#connector-ui","title":"Connector UI","text":"<p>To Be Done</p>"},{"location":"modules/IDT/#other-information","title":"Other Information","text":"<p>No other information at the moment for IDT</p>"},{"location":"modules/IDT/#openapi-specification","title":"OpenAPI Specification","text":""},{"location":"modules/IDT/#additional-links","title":"Additional Links","text":"<p>Video https://youtube.com/idt</p> <p>Kubernetes https://v1-26.docs.kubernetes.io/docs/home/</p> <p>Helm https://helm.sh/</p> <p>K3s https://k3s.io/</p> <p>Rancher https://ranchermanager.docs.rancher.com/v2.7/getting-started/quick-start-guides</p> <p>EDC Connector https://eclipse-edc.github.io/</p> <p>Portal Repository https://github.com/ds2-eu/portal (Private Link for Project Members)</p>"},{"location":"modules/MDT/","title":"Model Development Toolkit (MDT)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/mdt_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/32"},{"location":"modules/MDT/#general-description","title":"General Description","text":"<p>The main purpose of the DS2 Model Development Toolkit Module (MDT) is to provide a set of tools to allow the users to develop Algorithms based on the CRISP-DM standard to assist in the whole development cycle (training, test, etc.) and package the algorithms which can be deployed as executable software component.</p> <p>MDT provides a suite of integrated tools to develop algorithms that adhere to CRISP-DM, a widely accepted methodology for data mining projects. It includes features for understanding the business objectives and the data that are the focus of the analysis. It offers tools for preparing datasets from raw data, developing, and training models, and testing them. It allows packaging models as containers with a REST API for easy deployment and integration into other systems. Additionally, it provides monitoring capabilities to evaluate the performance of the models in runtime when deployed. The module supports the use of external libraries for data analysis and comes with preloaded libraries that are particularly useful for anomaly detection.</p>"},{"location":"modules/MDT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/MDT/#component-definition","title":"Component Definition","text":"<ul> <li>Module IDE: This set of components provides all the functionality needed to develop data models and algorithms during design time.</li> <li>Exploration &amp; Validation: This component allows visual exploration and profiling of data by analysts. It incorporates aspects of the Onesait Platform\u2019s profiling and dashboard engine components, which will be extended to enhance profiling and tagging of input data. It supports structured data in various formats (CSV, JSON, XML, etc.) as well as non-structured data such as text, images, and video. This module read data from the repositories directly. It supports the most used technologies as SQL database, MongoDB, S3, FTP, etc. This component temporally stores the data in a staging area. This component together with Wrangling component help to create datasets for training.</li> <li>Wrangling: This component enables the definition and execution of rules to clean data of malformed or undesirable values. It leverages an existing component from the Onesait Platform, based on the Open Refiner open-source project, which is useful for structured data. Additional development will be required to handle non-structured data. The Wrangling component can process data in the staging area of the Exploration &amp; Validation module. Both modules collaborate to define datasets for training. Once the cleaning and formatting process is complete, the resulting dataset is stored in the Minio Storage component.</li> <li>Monitoring, Logging, &amp; Alerts: Provides real-time monitoring and data visualization, including functionality for creating dashboards to track request flows and performance issues in model execution. It also defines alerts for detecting anomalous behaviour during model execution and is based on the Grafana software stack. Key tasks include creating monitoring dashboards, defining alerts, and integrating with the Unified User Interface. </li> <li>MinIO Storage: Datasets used for training and testing models will be stored in MinIO object storage. Minimal integration work is needed. The Wrangling component stores the datasets, and MinIO Storage provides them to the components that train and test the models. Any component that requires datasets can request them from MinIO Storage. For example, the Model Evaluation component will request data for validating models.</li> <li>Model Engineering: This is used to define and create models, based on two open-source projects. The first is Apache Zeppelin, a web-based notebook to perform interactive data analytics and visualization, supporting multiple programming languages such as Scala, Python, SQL, and more. The second is a tool for defining simple models using SQL language. Some minor enhancements will be necessary for better integration and pre-installed libraries, with additional tools required for picture and video analysis during the project execution. </li> <li>Model Evaluation: This component handles the training-evaluation cycle and is based on MLflow, an open-source platform for managing the end-to-end machine learning lifecycle. MLflow enables tracking experiments, packaging code into reproducible runs, and sharing and deploying models across different environments. The main task is to integrate MLflow with the Model Engineering and Model Packaging components. </li> <li>Model Packaging: Once a model is ready for production deployment, it will be packaged into a model container with clear interfaces for integration with other systems. This component is responsible for creating such deployable and executable modules. </li> <li>Model Repository: Stores model definitions, including versions and metadata about model development, such as historical information about datasets and tests performed in the training process. It also stores the necessary data for building the model, such as libraries and their versions, additional parameters, and more. </li> <li>Code Repository: Stores the code required to build the model as a deployable component, based on GitLab software, which provides Git repositories. Only configuration and integration tasks with the Unified User Interface are needed. </li> <li>Build &amp; Integration Testing: Defines and executes the construction of models as deployable software. It uses the code provided by the Code Repository together with the Model Repository data to build a software that can be deployed and executed.</li> <li>Model Deployed: As well as the containerised model, relevant code, each runtime-deployed model will include the following components: </li> <li>Model Controller: Manages the lifecycle of the module at runtime and provides an interface for external applications to use the module. </li> <li>Model: Executes the model definition, including all required software dependencies, runtime engines, and libraries for normal module execution. </li> <li>Unified User Interface: By interfacing with the other components of the module, this component enables the development of data models and algorithms during design time, as well as the visualization of alerts and monitoring data during runtime. Facilitates the usage and management of all integrated tools, providing single sign-on access to all module components. It is based on the existent Control Panel component of Onesait Platform that will be extended to support all the new capabilities introduced by this module and described in this section.</li> <li>Data Inspector: The analysis jobs executed by the T6.2 Data Inspector Module will have the capability to use models created and deployed with MDT. For more details, refer to the T6.2 Data Inspector description.</li> <li>Additional Libraries: As part of the execution of T4.2, DIGI will analyse and identify additional libraries to be included and supported for the MDT.</li> </ul>"},{"location":"modules/MDT/#screenshots","title":"Screenshots","text":""},{"location":"modules/MDT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Indra Open Source Apache 2.0"},{"location":"modules/MDT/#top-features","title":"Top Features","text":"<ul> <li>Model Development: Gather, clean, and transform data for training.</li> <li>Storage: Store and manage datasets in MinIO.</li> <li>Experimentation: Run experiments and keep a record of all executions.</li> <li>Model Packaging: Package trained models into Docker images to facilitate deployment.</li> <li>Monitoring and Validation: Track metrics such as accuracy, latency, and throughput to detect when retraining is needed.</li> </ul>"},{"location":"modules/MDT/#how-to-install","title":"How To Install","text":"<p>TBC</p>"},{"location":"modules/MDT/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/MDT/#software","title":"Software","text":"<p>TBC</p>"},{"location":"modules/MDT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/MDT/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/MDT/#how-to-use","title":"How To Use","text":"<p>TBC</p>"},{"location":"modules/MDT/#other-information","title":"Other Information","text":"<p>TBC</p>"},{"location":"modules/MDT/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/MDT/#additional-links","title":"Additional Links","text":"<p>Onesait Platform GitHub https://github.com/onesaitplatform/onesaitplatform-cloud</p>"},{"location":"modules/ORC/","title":"ORC - Orchestration Module","text":"<p>Use the table below as an example and replace with the links to the module's DS2 GitHub repository and GitHub project.  </p> Project Links Software GitHub Repository https://github.com/ds2-eu/orchestration Progress GitHub Project https://github.com/orgs/ds2-eu/projects/9"},{"location":"modules/ORC/#general-description","title":"General Description","text":"<p>The Orchestration module is made to design and then orchestrate at runtime In-Dataspace, Inter-Dataspace, internal, and third-party services which facilitate common data-orientated operations such as transformation of data, checks on data, data updates etc. The orchestrator contains a flexible GUI to design workflows and decision points on these services, and run time component to implement the workflow</p> <p>Services are added to and then selected from a Service catalog from a participant\u2019s local service catalog (In-Dataspace deployment), the DS2 service intermediary catalog (Inner-Dataspace), or other available catalog/service knowledge. These services can be graphically linked together to form a workflow and where decision pathways, decision points, and other operators can be deployed to determine the workflow. Error and exit points should be predetermined with defaults ensuring that failures and error conditions allow flows to be closed automatically. One class of operator is for user defined forms for human input but most often the flows contain backend services. In the context of DS2 the operators will, if necessary, be expanded based on novel usecase peculiarities as will the forms designer. Primarily the design interface is orientated around service interconnectivity, but this will be augmented with a data pre-viewer to help interconnect and understand the results of interconnecting data-orientated services. The orchestrator will be available as a module and for interparticipant service orchestration will extend the connectors.</p>"},{"location":"modules/ORC/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/ORC/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <p>Orchestration Module - Core - Orchestration Core: This is the runtime heart of orchestration which conducts a process (workflow) triggering other services and via a BPMN module from the Service Composition designer repository. For the tier 1 standard connections (Portal etc) it can be perceived as the entry point. If new orchestration design methods are needed it will use them. Runtime events are connected to the logging components and for inter-participant dataflows it will interface with the DS2 Cross Participant Orchestration subcomponent. This is currently ICE background and will see little development except for a DS2 compliant UI.</p> <ul> <li>DS2 Service Registry: </li> <li>In-participant: This is a local registry of all services which a participant may potentially use in a workflow, composed together in the designer, and executed in the runtime. Registration can be automatic in the case of IDT installed services. It will also expose services in the inter-participant DS2 registry. It exists now but will be rebuilt in the context of DS2 and IDT.</li> <li>Between Participant: 95% the same functionality but can function similarly to a metadata broker to host services from multiple participants which can be shared in a controlled way to the In-participant registry to allow participant-participant service interactions</li> <li>Service Composition Designer (DS2 Upgrade): This is the main UI for the Orchestration Designer based on existing ICE background. It allows a user to select or drag various elements from a toolbox (services/APIs from the DS2 Service Registry, methods), which can be placed on a canvas where they can then begin to start designing their orchestration by dragging and connecting various elements together. The saved BPMN2.0 notation model will then be used by the runtime orchestration core. The DS2 upgrade will be mainly for UI and inclusion of New Data Previewer and Forms designer blocks.</li> <li>Forms designer: Many orchestrations have a need for user input and whilst some might come from other systems this can be complex when only limited information. The forms designer will allow the easy inclusion of simple form in any Service composition and also ensure that it respect the data flow as well as service needs.</li> <li>Data Previewer: This is also a new subcomponent which will be rendered via the Service composition designer. Currently services are connected but when designing it is useful to know at design time what might be the inputs and the expect result. In a data orientated project this is especially useful, and this utility will allow some rendering of data to help show flow operations between building blocks before they are deployed.</li> <li>New Orchestration methods (from pilots): Many methods \u2013 eg choice boxes, selections are already implemented in the orchestrator, but it is possible that the pilot might suggest further ones that could be interesting to implement \u2013 although at this stage of the analysis it seems there is not. The new methods will be exposed in the orchestrator runtime &amp; designer.</li> <li>Orchestration track/log: Currently this is rudimentary and especially in the trustworthy context of dataspace an major overhaul is necessary to extract more granular logging information at runtime. </li> <li>Services and API: These are the services that can be orchestrated, and the API block is the interface to each</li> <li>Other External (non DS2) Modules/Services:</li> <li>DS Service Intermediaries:</li> <li>Tier 2 In dataspace DS Modules/Service:</li> <li>Tier 0 Support Service Stack:</li> <li>DRM and API: For further exploration, but if room to implement and a match of requirements to feature the blockchain part of the DRM module to enhance logging</li> <li>DARC &amp; API: As with DRM but in this case to use DARC to help configuration</li> <li>Culture and Language Module and API: As with DRM but in this case to use this modules ontology engine to help auto-link services \u2022   Tier 1 Service Stack for Marketplace and deployment and API: The full stack will be implemented as generically described elsewhere in this document. Exceptions: The Platform will only be needed for inter-participant service orchestrations if used</li> </ul> <p>Inter-Participant orientated: - DS2 Cross DS Orchestration: This is a new runtime module which will act as a bridge between the orchestration within each participant through interconnections to the Inter-Participant Service Registry and the Orchestration core at each participant - Tier 3 Trust Stack and API: For interparticipant service the module will use relevant parts of the DS2 trust stack \u2013 see diagram below</p>"},{"location":"modules/ORC/#screenshots","title":"Screenshots","text":""},{"location":"modules/ORC/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/ORC/#top-features","title":"Top Features","text":"<ol> <li>Process Designer: A custom Camunda BPMN Modeller </li> <li>API Repository: repository of API services</li> <li>Service API: an API that allows use of services in the API Repository externally</li> <li>User Tasks: a unique type of task that allows for workflows to request user input</li> <li>User Task Manager: a front end that allows for management of user tasks</li> <li>Control Panel: a web app that allows users to view, manage and interact with workflows </li> <li>Process View: a view that allows the user to monitor the status of workflows</li> <li>Single-Sign On: integration with the Dash Button that allows the ORC module to be linked to the Portal</li> </ol>"},{"location":"modules/ORC/#how-to-install","title":"How To Install","text":"<p>To Be Done</p> <p>The how to install  has the following sections: requirements, software, summary of installation steps and detailed steps. In some cases, you may not have some of it, like for instance, the software section, or maybe you can't add this yet. Please leave the sections but add the text \"N/A\" or \"To Be Done\" depending on the module.</p> <p>So far, the how to install will be pretty much clone repo and run docker in some cases. Maybe in some others build software and run ...</p> <p>At a later stage, when idt integration is done, there will be a section for Module IDT installation.</p>"},{"location":"modules/ORC/#requirements","title":"Requirements","text":"<p>Add the minimum and recommended list of requirements in terms of CPU, RAM and Storage.</p>"},{"location":"modules/ORC/#software","title":"Software","text":"<p>Add a list of software utilities that form the module. This may not be necessary for most modules, but it is for the idt for instance.</p>"},{"location":"modules/ORC/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>Add a summary of installation steps.</p>"},{"location":"modules/ORC/#detailed-steps","title":"Detailed steps","text":"<p>Add the detailed steps of installation including screenshots or code snippets.</p>"},{"location":"modules/ORC/#how-to-use","title":"How To Use","text":"<p>Add in here the steps to use the module from the perspective of your target user. Similar to how to install, add screenshots and code snippets.</p> <p>To Be Done</p>"},{"location":"modules/ORC/#other-information","title":"Other Information","text":"<p>No other information at the moment for MODULE.</p>"},{"location":"modules/ORC/#openapi-specification","title":"OpenAPI Specification","text":"<p>Add your open api specification, if you have any. The idea is to be able to include here the swagger UI, but so far just a snapshot or the yaml specification.</p>"},{"location":"modules/ORC/#additional-links","title":"Additional Links","text":"<p>Add in here relevant links for your module. In this section we will also add the link to the module video.</p>"},{"location":"modules/PAE/","title":"Policy Agreement and Enforcement (PAE)","text":"<p>Use the table below as an example and replace with the links to the module's DS2 GitHub repository and GitHub project.  </p> Project Links Software GitHub Repository https://github.com/ds2-eu/pae_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/21"},{"location":"modules/PAE/#general-description","title":"General Description","text":"<p>The primary function of the Policy Agreement and Enforcement Module (DS2 PAE) is to ensure compliance with the established policies and regulations governing data exchange among users in different data spaces. Henceforth, policies, regulations, and agreements are synonymous with the term policy. The policies are evaluated as the control plane stage of data sharing in the Connector. The policies serve two main purposes: Access Control and for Usage Control. Access Control determines whether access to data is granted or denied. Usage Control dictates how the data can be used once access is granted.</p> <p>Policies in dataspaces define who can access the data and the restrictions on data use for those with access. A policy, in this context, is a set of rules governing data sharing within a dataspace and, more specifically for DS2, between dataspaces as well as their participants. The main function of this module is to enforce policies associated with a data-sharing contract, an agreement between a provider and a consumer. The agreed policy, the contract, specifies the rules both parties must follow. This module focuses on rules that can be automatically enforced by software. Policies are evaluated before data transmission begins. Rules that cannot be automatically enforced are logged alongside the contract agreement for accountability. There will be two types of rules, Access Control and Usage Control. </p> <ul> <li>Access Control rules define who is authorized to access the data. If these rules are not met, data sharing is not permitted. </li> <li>Usage Control rules define how the data can be used. Usage rules may require additional actions during or after data sharing, which might need to be executed by other software components or through human intervention. </li> </ul> <p>The Policy and Agreement Enforcement Module will notify the relevant modules about the need for these actions.</p>"},{"location":"modules/PAE/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment.</p> <p></p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module.</p> <p></p> <p></p> <p></p>"},{"location":"modules/PAE/#component-definition","title":"Component Definition","text":"<p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module.</p> <p>The following figure expands on the previous one by detailing the subcomponents for policy enforcement and their relationships with other modules.</p> <p>This module has the following subcomponent and other functions: Note that the large green box represents a connector. It is depicted this way to clarify which components are connector extensions deployed within the connector, and which are deployed outside it.</p> <ul> <li> <p>Policy and Agreement Enforcement Module:</p> </li> <li> <p>Policy Extension (PDP): This extension will be integrated into the Policy Enforcement Point of the Connector. It will oversee the coordination of all policies supported by DS2. </p> </li> <li>PAE Metadata Translator: This optional extension will translate metadata to ensure compatibility with rule enforcement across different dataspaces with varying participant and service descriptors. As of this document's edition, it is to be decided if this will be handled by the Connector itself, through the CLM Module, or through the Catalog Module therefore, it will be implemented as needed. </li> <li>Sovereignty Transformation Interceptor: This extension will enable policy-oriented data transformations during data exchange - for example anonymising data. It serves as the implementation of a Policy Execution Point. Note that the Data Transformation feature of DDT module is not used to do this since it is more generic</li> <li>Sovereignty Transformation Policy - Runtime: This component provides an example of Usage Control rule enforcement. It is expanded and described in detail later in this section.</li> <li>Policy Execution Point (PXP): This component interfaces with the execution of additional actions required when a policy is enforced and after policy evaluation. An example is the Sovereignty Transformation Interceptor, which will be used to enforce certain usage control rules.</li> <li>Policy Information Point (PIP): This component is responsible for implementing interfaces with all relevant modules and external software (e.g., retrieving participant information from a CRM application) from which additional context data is needed for policy evaluation. A mandatory interface is with the Data Marketplace module to validate asset purchases. </li> <li>Policy Logger: This component will handle all logging related to policy enforcement. In addition to internal logging, policy decisions can be recorded in the Data Rights Management module. </li> <li>Policy Management Point (PMP): This component stores all the policies that must be enforced for each share of data. Specifically, it will store the contract offers, including the reference to the data offered and the associated policies.</li> <li>Rule Implementation: Each rule within a policy must have a corresponding software component that supports its evaluation. A Rule Implementation component may support several rules and might rely on an existing rule engine or be implemented from scratch, depending on the rules it needs to support. Access Control rules will be enforced based on these implementations. ODRL will be used by T3.2 for defining the types of rules allowed. Within PAE an example of Usage Control rules will be implemented using the Sovereignty Transformation Policy. In these cases, the Rule Implementation will evaluate the rule, but the needed actions to enforce the rule are delegated, in the mentioned example to the Sovereignty Transformation Policy. The responsibility of PAE is to notify to the required component. If there is not such a component to automatically handle rule actions, the evaluation will be logged with the Policy Logger, but any further verification or action is out of the scope of this module.</li> <li>Policy Administration Point (PAP): This is a UI that allows for the definition of policies to be used in data sharing. Policy definitions are stored in the PMP. Policies will be defined using templates developed based on the types of rules identified in WP3. The tool will assist users in easily defining rules and will generate machine-readable policies.</li> <li>Sovereignty Transformation Policy: </li> <li>Sovereignty Transformation Policy - IDE: This group of components is shared with the T6.2 Data Inspector. Some of them will be developed based on existing technology, specifically the Dataflow component of the Onesait Platform open-source product. While T4.1 Policy Enforcement focuses on defining and executing transformation jobs for policy enforcement, the Data Inspector will enhance the current capabilities of Dataflow to include data inspection, monitoring, and notifications. T6.2 will lead this component development, and T4.1 will be built on the improved version from T6.2. Therefore, T4.1 has a dependency on T6.2, but the opposite is not true.<ul> <li>Sovereignty Transformation IDE UI: This graphical user interface allows users to define Sovereignty Transformation Jobs for enforcing policy rules that require data modifications. Examples include field removal and data anonymization. A Sovereignty Transformation Job definition is a data pipeline with an input, an output, and a set of transformation stages in between. This component is based on existing INDRA software, with updates needed to support new features for DS2. </li> <li>Sovereignty Transformation IDE Controller: This component manages Sovereignty Transformation Jobs during design time and oversees their deployment and monitoring at runtime. It is based on current INDRA software but requires significant upgrades to split the tool into the IDE component and the Runtime component (potentially several). </li> <li>Testing and Validation: This component will encompass test definitions and include the storage of small datasets specifically for automatic testing purposes as well as providing validation functionality. The automated tests will be designed to thoroughly validate the accuracy and correctness of the sovereignty transformation jobs. This validation process is essential to ensure that the jobs meet the required standards and perform as expected before they are deployed. By conducting these tests beforehand, potential issues can be identified and addressed, ensuring a smooth and reliable deployment of the sovereignty transformation jobs.</li> <li>Job Definition Storage: This component stores definitions of Sovereignty Transformation Jobs, based on INDRA software, with extensions planned to improve version control of the definitions. </li> </ul> </li> <li>Sovereignty Transformation Policy - Runtime:<ul> <li>Runtime Controller: This component will manage the execution of Sovereignty Transformation Jobs during runtime. It will define the interface for integrating job execution with the Sovereignty Transformation Interceptor and will handle the job lifecycle: deployment, upgrade, removal, start, and stop.</li> <li>Sovereignty Transformation Job: This component represents the runtime execution of a data pipeline definition. Each transformation supported will require a Sovereignty Transformation Job definition. One instance of this component will be created for each Sovereignty Transformation Job needed at runtime, even for the same definition. The creation of these instances will be managed by the Runtime Controller. </li> <li>Job Logger: This component logs all relevant information about each job execution. Based on INDRA software, it will require minimal development to adapt to changes in other module components. </li> <li>Job State Storage: This component stores the states of job executions throughout their lifecycle, enabling job resumption in the event of failures during execution. Based on INDRA software, it will require minimal development.</li> </ul> </li> </ul> <p>Interfaces with other modules:</p> <ul> <li>T3.2 Types of rules allowed (ODRL): This will define the types of rules that might be used in policy definition. The types of rules will be defined using the ORDL standard, which has been selected by IDSA for policy enforcement in dataspaces. Once these rules are defined, the Policy and Agreement Enforcement module will be responsible for providing software support to enforce all rules that can be evaluated without human intervention.</li> <li>Data Right Management (DRM): DRM will be used by the Policy and Agreement Enforcement to log the decisions about policy.</li> <li>Data Marketplace (DMK): The Data Marketplace will be queried by the Policy and Agreement Enforcement module to obtain additional information if a policy requires a purchase to obtain data.</li> <li>Culture and Language Module (CLM): Integration with the T5.1 Knowledge Base will be implemented to translate metadata, ensuring compatibility across different dataspaces. If it is determined later in the project that the Culture and Language Module is unsuitable for this task, simple mappings using the metadata from the project's use cases will be employed to prevent blocking the implementation.</li> <li>Other Software / Other DS2 Modules: The PAE module defines APIs to obtain additional information needed for the policy enforcement process, such as metadata about participants or services that may not be accessible via the connector. In such cases, PAE will acquire the necessary data. Additionally, if any rules require actions to be executed by other software or DS2 modules, PAE will notify the requirement when evaluating the rules.</li> </ul>"},{"location":"modules/PAE/#screenshots","title":"Screenshots","text":"<p>N/A</p>"},{"location":"modules/PAE/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Indra Open Source Apache 2.0"},{"location":"modules/PAE/#top-features","title":"Top Features","text":"<ul> <li>Integrated with EDC Connector</li> <li>Enforcement of policies defined in DS2</li> <li>Use of contextual information about participants obtained from other DS2 modules.</li> <li>Capabilities to enforce not only access rules but also some usage rules.</li> </ul>"},{"location":"modules/PAE/#how-to-install","title":"How To Install","text":"<p>The Policy and Agreement Enforcement Module is provider together with the DS2 Connector.</p> <p>The software for Sovereignty Transformation Policy ...</p> <p>TBC</p>"},{"location":"modules/PAE/#requirements","title":"Requirements","text":"<p>The same that for DS2 Connector.</p> <p>The software for Sovereignty Transformation Policy is highly dependent on the volume of data to process. It is based on DINS module. </p>"},{"location":"modules/PAE/#software","title":"Software","text":"<ul> <li>DS2 Connector</li> <li>DINS module for Sovereignty Transformation Policy</li> </ul>"},{"location":"modules/PAE/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/PAE/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/PAE/#how-to-use","title":"How To Use","text":"<p>TBC</p>"},{"location":"modules/PAE/#other-information","title":"Other Information","text":"<p>TBC</p>"},{"location":"modules/PAE/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/PAE/#additional-links","title":"Additional Links","text":"<p>TBC</p>"},{"location":"modules/Portal/","title":"Portal","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/portal Progress GitHub Project https://github.com/orgs/ds2-eu/projects/11"},{"location":"modules/Portal/#general-description","title":"General Description","text":"<p>To provide a user and developer friendly portal allowing dataspace participants to register and select DS2 modules which can then be packaged into a IDT environment subsequently deployed by participants enabling both In-Data Space and Inter-Data Space operations. As such it includes functionality for developers to include modules, users to find those modules, to trigger the packaging through links with the containerisation module, as well as supporting functionality for dataspace support, dataspace resources, registration and identity management, and administration. It also provides support for the Data Marketplace. </p> <p>The portal will operate in the DS2 cloud hosted by i4RI. Parties interested in DS may register \u2013 this includes participants, dataspace governance/operators, DS2 module developers as well as, potentially, service intermediaries. Developers will provide modules to the portal marketplace which can be selected/purchased by participants. Selected modules will then be  downloaded and installed by and at a participant IDT. All parties involved will be granted unique IDs including DS-Pair IDs where DS\u2019s agree to cooperate. If manpower resources allow ,a subcomponent will also support user-developer and developer-developer interactions as well as additional static resources such as useful materials or links to IDSA etc. The portal will run on the DS2 platform as a central node but in itself will not be involved in any participant-participant process either at the control or data plane levels. It operates at Tier 1, so is not a \u201cmodule\u201d in-itself, even though it will be deployed and run on top of IDT like a module, so participants would not deploy it. However, conceptually it could be run locally. It could be run by any one as an additional service \u2013 for example dataspace operators. Its main interfaces are to the Containerisation module for module description and packaging, IDT for deployment, the Platform where it is both deployed and will interface to some system modules, and the Data Marketplace which uses it for base marketplace functions. </p>"},{"location":"modules/Portal/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/Portal/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li> <p>Portal Front End: This is the initial web front end entry point seen by any user of the portal, regardless of participant type or if an external party (eg developer). If unregistered, it directs them to the Registration and Management component and if registered a login process follows. Dynamic menus then allow users to navigate to the elements they wish to see and have access for and as such there is an inferred link to all subcomponents with some form of UI. Of particular relevance is the Dashbutton which is a kind-of mini-Portal Front End embedded in all subcomponents and modules including the Portal itself.</p> </li> <li> <p>Registration and Management: This allows the registration of any actor which wishes to take advantage or use DS2. This includes human actors such as user\u2019s requiring IDT/modules and developers through to system actors such as dataspaces. This will be a customised version of ICE\u2019s existing portal registration system. It\u2019s primary link is to the DS2 Identity Manager.</p> </li> <li> <p>Dash button: The Dashbutton is a DS2 library feature used by most modules and is a dynamic shortcut menu to things of most relevance to the user (eg mainly their installed modules). It allows users to quickly get awareness of their personal DS2 environment as well as control it. The Dashbutton dynamic is detailed within the IDT Module. The base functionality of the Dashbutton exists now but needs to be highly customised for the DS2 environment. It relies on embedded HTML vs an API and enables a holistic environment for DS2 in an easy way. It is detailed in the IDT module.</p> </li> <li> <p>Resource Connect: This will have no deep functionality but allows the users to navigate to further web/document/video type resources both within DS2 and externally (eg IDSA). For example, to read more technical details, look up example dataspaces or any other material. This component is an \u201cif-time-allows\u201d option.</p> </li> <li> <p>Developer Connect: This is intended to simplify casual chat-like interaction between users and developers or in fact any set of participants subscribed to DS2. For example, for support or to discuss new ideas. If included it will be based on existing third part components. This component is an \u201cif-time-allows\u201d option.</p> </li> <li> <p>Module Marketplace: Listing, Cart, Purchase: DS2 modules need to be selected from a catalogue of all modules, purchased (which may be for 0 EUR), and licensed which then makes them available to be packaged into IDT. Except for the latter this is classic marketplace functionality and is mainly configuration of ICE\u2019s existing marketplace module. Its primary links are to the Module uploader to populate its shopfront and once a module is \u2019purchased\u2019 the License and Use Manager.</p> </li> <li> <p>Module Uploader: Beneficiaries (or third parties) develop data-orientated DS2 modules which can be put on DS2 Marketplace. A main link is to the DS2 Identity Manager to give unique identities to the module uploaded. This subcomponent ensures all relevant information is provided and packaged in the right way. This includes software, module details (eg price, licenses, description, logo...), and accompanying knowledge (eg How To\u2019s, Videos\u2026). It ensures a module is K8s/Helm compliant so that it can be potentially packaged into IDT. It will be based on ICE Asset upload being customised for the DS2 environment and all modules must confirm to its needs.</p> </li> <li> <p>Payment: Once agreed the product is paid for and both the portal and the developer compensated. An existing payment service (stripe) will be used and a cart-mediator service is used to link to that (strip link is not shown in architecture). This service will also be used by the Data Marketplace</p> </li> <li> <p>License and Use Manager: Following potential purchase from the marketplace the asset is then licensed for use (fixed licenses, subscription etc \u2013 however developer determines) and a Payment is then due. Once licensed it becomes available for the Containerisation and Deployment grouping to either issue a new IDT or to upgrade and existing IDT. The components is based on existing ICE Marketplace technology upgraded for DS2. Other modules can also use the licensing system \u2013 eg the Data Marketplace (TBD). Note that on first use core modules such as containerisation will also be packaged</p> </li> <li> <p>Administration: A UI for Portal (and Platform) administrator to use to configure other elements and functions of the portal based on access credentials.</p> </li> <li> <p>Portal Identity Manager: Provides identities for actors (parties and modules) relevant to the Portal. All DS2 users must have a DS2 ID, as should Dataspaces, and Dataspace Pairs. The Portal Identity Manager will where necessary interface with the DS2 Identity Module (IDM) to validate participant IDs which will run on the DS2 Platform</p> </li> <li> <p>DS2 Containerisation and API: This provides two basic services to the portal: A) Assisting and ensuring a developer uploads a containerisation compliant module. B) Once modules are selected and licensed, they can be packaged and deployed via this module to downloadable/upgradable IDTs</p> </li> <li> <p>DS2 S/W Platform and API: This platform is available to all modules and is the cloud home of the Portal as well as the Portal hosting an administrative interface to configure it</p> </li> <li> <p>DS2 Data Marketplace and API: The cloud base data marketplace may take advantage of some of the portal common administration function \u2013 user management, licensing, payment</p> </li> <li> <p>DS2 IDT Broker: The licensed modules are packaged via DS2 Containerisation and deployed as a bespoke IDT to the participant</p> </li> <li> <p>DS2 Portal Operator: This represents the administrator and operator of the system who use the Administration UI</p> </li> <li> <p>Modules, s/w, details, knowledge: This represents the upload of the module and accompanying information to become an asset in the portal which can be later explored and purchased. The upload is to the Module Uploaders.</p> </li> <li> <p>Developers: Technicians and business partners representing a developed module to be uploaded</p> </li> <li> <p>User: Any participant of any data space, or future potential participant who wishes to explorer/download modules available</p> </li> <li> <p>External Modules: Other modules which could be applicable to show on a users Dashbutton</p> </li> </ul>"},{"location":"modules/Portal/#screenshots","title":"Screenshots","text":"<p>To Be Done </p>"},{"location":"modules/Portal/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/Portal/#top-features","title":"Top Features","text":"<ol> <li>Organisation Registration and Management: The Portal provides the functionality to register an organisation in DS2 and manage its profile</li> <li>User and Role Management: Create additional users and roles to the organisation</li> <li>Main DS2 Entry Point: The Portal is the UI main entry point to DS2</li> <li>DS2 Navigation: Allows seamless navigation between intermediary modules in the DS2 Cloud Platform</li> <li>Global and Local Portal: The Portal has two different views, the Global Portal running as an intermediary service in the DS2 Cloud Platform and the Local Portal which is meant for the IDT as the IDT Portal</li> <li>Dash Button: A web component to be integrated by all modules that provide the interface to the DS2 security and identity system and allows module navigation</li> <li>Developer Access: Generate API Key to use the Portal API for integration</li> <li>Portal Marketplace: The Portal includes a fully featured Marketplace, which could be considered a module on its own, with functionalities to publish and purchase modules, module search, feedback and ratings, flexible license models, secure payment integration, etc.</li> <li>GitHub Backend Storage: The Marketplace feature is backed by GitHub storage, meaning that modules and container images are stored in GitHub</li> <li>Containerisation Integration: The Module creation view of the Containerisation module is integrated into the Portal</li> </ol>"},{"location":"modules/Portal/#how-to-install","title":"How To Install","text":"<p>Even though the Portal (Global Portal) is a DS2 intermediary service, and users don't need to install it, it is packaged as a regular DS2 module and can be installed on top of the IDT, same as the Local Portal. The Marketplace is also installed as an additional module. In addition, the Dash Button is a web component that needs to be integrated in all DS2 module UIs. Details on how to integrate it are explained next.</p>"},{"location":"modules/Portal/#requirements","title":"Requirements","text":"<p>N/A</p>"},{"location":"modules/Portal/#software","title":"Software","text":"<p>N/A</p>"},{"location":"modules/Portal/#summary-of-installation-steps","title":"Summary of installation steps","text":""},{"location":"modules/Portal/#portal_1","title":"Portal","text":""},{"location":"modules/Portal/#marketplace","title":"Marketplace","text":""},{"location":"modules/Portal/#dash-button","title":"Dash Button","text":"<ol> <li> <p>Add script file path to index.html</p> </li> <li> <p>Add the font-awesome library (starting from v0.0.16)</p> </li> <li> <p>Configure according frontend framework ie. Angular ...</p> </li> <li> <p>Place the web-component button tag at the top of the navigation bar </p> </li> </ol>"},{"location":"modules/Portal/#detailed-steps","title":"Detailed steps","text":""},{"location":"modules/Portal/#portal_2","title":"Portal","text":""},{"location":"modules/Portal/#marketplace_1","title":"Marketplace","text":""},{"location":"modules/Portal/#dash-button_1","title":"Dash Button","text":"<p>To install the dash-button web component, copy and paste it into your frontend application.</p> <p>Note: Please refer to the npm package page to find the latest version, and replace <code>X.X.X</code> in the import script accordingly: <pre><code>&lt;script type=\"module\" src=\"https://unpkg.com/dash-button-web@X.X.X/dist/esm/web-compnont.js\"&gt;&lt;/script&gt;\n</code></pre></p> <p>Note: Starting from v0.0.16, you need to add the FontAwesome stylesheet file to your application. If your application is already using this package, you can ignore this.</p> <pre><code>&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css\" /&gt;\n</code></pre> <p>You can find the NPM package named dash-button-web.</p> <p>If it is a framework, you will need to configure more options. Therefore, follow the Stencil.js documentation. </p> <ul> <li>Setup Keycloak Server Application</li> </ul> <p>First, you need to start your own Keycloak instance or use the DS2 version. We recommend using the DS2-based Keycloak instance.</p> <pre><code>* Option 1 \u2013 Use DS2 Keycloak details\n\n  You can use predefined Keycloak credentials to configure DashButton. The required details are summarized below.\n\n  - Keyclaok URL: **https://keycloak.ds2.icelab.cloud**\n  - Realm: **ds2**\n  - Client ID: **dashbtn**\n\n* Option 2 \u2013 Configure your own Keycloak client\n\n  If you use your own version, you need to configure the realm, client, and test users.  \n  Make sure the client is set up with a **client ID** and that **client authentication is disabled**.\n</code></pre> <ul> <li>Configure frontend application</li> </ul> <p>Here, we are demonstrating how to configure the application on an Angular-based application and a basic HTML-based application. If you need to install React or Vue.js, follow the Stencil.js documentation.</p> <p>First, you need to enable the custom component support feature in the Angular project. To do that, the following code block needs to be added to the <code>src/app/app.module.ts</code> file.</p> <pre><code>@NgModule({\n  \u2026\n  schemas: [\n    CUSTOM_ELEMENTS_SCHEMA\n  ],\n})\n</code></pre> <p>Once the above addition is completed, it is required to import the below module as follows.</p> <pre><code>import { CUSTOM_ELEMENTS_SCHEMA} from '@angular/core';\n</code></pre> <p>Next, you need to put the script file path in the src/index.html file header section as follows.</p> <pre><code>&lt;head&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n  &lt;link rel=\"icon\" type=\"image/x-icon\" href=\"favicon.ico\"&gt;\n\n &lt;script type='module' src='https://unpkg.com/dash-button-web@X.X.X/dist/esm/web-compnont.js'&gt;&lt;/script&gt;\n &lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css\" /&gt;\n\n...\n</code></pre> <p>After that, you can pass configuration options as follows using the web component.</p> <pre><code>* Configuration options\n\n| Attribute         | Description | Type      | Default                   |\n| ----------------- | ----------- | --------- | ------------------------- |\n| `client-id`       | `Keycloak client ID` | `string`  | ``               |\n| `keycloak-uri`    | `Keycloak server URI` | `string`  | `\"http://localhost:8080\"` |\n| `realm`           |`Keycloak realm`| `string`  | ``                  |\n| `redirect-uri`    | `Application redirect URI` | `string`  | ``               |\n| `show-post-login-text` | `Successfully logged in shows a custom button message` | `boolean` | `false`                   |\n| `auth-method` | `Authentication method` | `string` | `check-sso`, `(login-required)`                   |\n| `app-id` | `Application unique ID` | `string` | ``                   |\n| `portal-url` | `Portal URL` | `string` | ``                   |\n| `portal-api-url` | `Portal API URL` | `string` | ``                   |\n| `show-unauthorized-modal` | `Show unauthorized modal` | `boolean` | `false`                   |\n| `other-link-type` | `Add new links for the menu` | `string` | `local or global`                   |\n| `application-display-name` | `Menu application display name` | `string` | `Applications`                   |\n| `disable-application-tab` | `disable application tab` | `boolean` | `false`                   |\n| `menu-view-type` | `the style of the menu` | `string` | `grid/list`                   |\n\n* Theme configuration options\n\n| Attribute         | Description | Type      | Default                   |\n| ----------------- | ----------- | --------- | ------------------------- |\n|  `primary-color`       | `Set primary colour` | `string`  | ``               |\n| `accent-color`    | `Set secondary or accent color` | `string`  | `` |\n\n&gt; **AuthMethod**\nUsing the auth-method attribute updates the application's authentication workflow.\n</code></pre> <p><code>login-required</code>:  User is forced to the login screen if they are not logged in. <code>check-sso</code>: User is not forced to the login screen; instead, when the user clicks the login button, they are redirected to the login screen.</p> <p>Now, you are almost done. In the final stage, you can place the web-component button tag at the top of the navigation bar using the following code block.</p> <p>Note: Replace DashButton configuration with actual values.</p> <p>To get the module link for the Dashbutton, use the portal-api-url attribute and provide the Portal backend API endpoint URL.   For the DS2 platform, you can use https://portal-api.ds2.icelab.cloud as the Portal API URL..</p> <p><pre><code>&lt;dash-button keycloak-uri=\"http://localhost:8080\" realm=\"demo\" client-id=\"testapp\" portal-api-url=\"replace_url\" auth-method=\"login-required\" show-post-login-text=\"false\"&gt;&lt;/dash-button&gt;\n</code></pre>   After that test your application can be test with user credentials and then you can get an output similar to this.</p> <p>Note: You need to create a new user. Please access the portal and create an account associated with your organization.</p> <p>Additionally you can test some of the demo applications located in the Portal repository dash-button/demo-app/ folder (see additinal links)</p>"},{"location":"modules/Portal/#how-to-use","title":"How To Use","text":"<p>The next sections describe how to use the different components part of the Portal</p>"},{"location":"modules/Portal/#portal_3","title":"Portal","text":"<p>In order to use the Portal, first navigate to the main entry page at https://portal.ds2.icelab.cloud/.  </p> <p>You need to first register your organisation to be able to log in. Click on Create an account and enter the required details. First enter the name of the new organisation and click Next to check whether it already exists or not in the system. </p> <p>Then enter all the details required for the organisation registration and click Register.  </p> <p>After that, organisation is registered. </p> <p>User can now log in with newly created organisation and username. </p> <p>User will receive a verification email upon first login attempt </p> <p>Once logged in there is a guided tour on the different options available that uses can follow. </p> <p>The main Dashboard of the Portal shows the list of default central modules running in the DS2 Cloud Platform ie. IDM, Catalog, Marketplace and Chat at the moment. The list will evolve as new features are added to DS2. </p> <p>In the left navigation bar users can access and edit the profile information. </p> <p>The Dataspaces option which links to the IDM (see IDM documentation). So far this is a link but researching to embedd the functionality inside the Portal.</p> <p>The Users option allows creating new users for this organisation. Click on Add new user and enter the details. </p> <p>In addition, the Portal already includes the DashButton which is described in the DashButton section.</p>"},{"location":"modules/Portal/#marketplace_2","title":"Marketplace","text":"<p>The Marketplace includes two main different views. The first one is the so called Marketplace for users to be able to purchase modules. The second one is the Product License Manager aimed at module developers to be able to publish their modules and monitor sales.</p> <ul> <li> <p>Marketplace </p> </li> <li> <p>Product License Manager</p> </li> </ul>"},{"location":"modules/Portal/#dash-button_2","title":"Dash Button","text":"<p>To Be Done</p>"},{"location":"modules/Portal/#other-information","title":"Other Information","text":"<p>No other information at the moment for Portal</p>"},{"location":"modules/Portal/#openapi-specification","title":"OpenAPI Specification","text":"<p>To Be Done</p>"},{"location":"modules/Portal/#additional-links","title":"Additional Links","text":"<p>Video https://youtube.com/portal</p> <p>Portal Repository https://github.com/ds2-eu/portal (Private Link for Project Members)</p>"}]}