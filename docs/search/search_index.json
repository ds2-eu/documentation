{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The DS2 Software Documentation","text":"<p>To simplify the understanding of DS2 modules and the reference architecture, the modular components of DS2 have been divided into Tiers.</p>"},{"location":"#tier-0-ds2-support-orientated","title":"Tier 0: DS2 Support Orientated","text":"<p>These represent a range of modules that are perceived as cross-cutting to the other tiers and their modules.</p>"},{"location":"#modules","title":"Modules","text":"<ul> <li> <p>CLM</p> </li> <li> <p>MCL</p> </li> <li> <p>SEC</p> </li> </ul>"},{"location":"#tier-1-ds2-marketplace-and-deployment-orientated","title":"Tier 1: DS2 Marketplace and Deployment orientated","text":"<p>These support the acquisition, porting, and deployment of modules at participants or service intermediaries</p>"},{"location":"#modules_1","title":"Modules","text":"<ul> <li> <p>DMK</p> </li> <li> <p>IDT</p> </li> <li> <p>PORTAL</p> </li> <li> <p>CONT</p> </li> </ul>"},{"location":"#tier-2-ds2-in-data-space-enablement","title":"Tier 2: DS2 In-Data Space Enablement","text":"<p>The modules facilitate consumer -provider participants in sharing their data. The modules are in general used by a single participant only in their local environment but in some cases, there are additional features to be used between data provider and consumer.</p>"},{"location":"#modules_2","title":"Modules","text":"<ul> <li> <p>ORC</p> </li> <li> <p>RET</p> </li> <li> <p>DDT</p> </li> <li> <p>MDT</p> </li> <li> <p>CUR</p> </li> <li> <p>DARC</p> </li> <li> <p>E2C</p> </li> <li> <p>DINS</p> </li> <li> <p>DSHARE</p> </li> <li> <p>DVM</p> </li> </ul>"},{"location":"#tier-3-ds2-inter-data-space-enablement","title":"Tier 3: DS2 Inter Data Space Enablement","text":"<p>To an extent, the logic for this layer is the reverse of tier 2.  These modules can be allocated to the Tier 2 situation but their added value is much greater in a cross-sector, cross dataspace scenario. </p>"},{"location":"#modules_3","title":"Modules","text":"<ul> <li> <p>SDS</p> </li> <li> <p>DRM</p> </li> <li> <p>PAE</p> </li> <li> <p>CAT</p> </li> <li> <p>IDM</p> </li> <li> <p>PCR</p> </li> </ul>"},{"location":"modules/CAT/","title":"CAT","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/cat.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/38"},{"location":"modules/CAT/#general-description","title":"General Description","text":"<p>The Catalogue module is a module designed to support the exchange of data within and across different data spaces. It ensures robust data governance, secure data exchanges, and compliance with sovereignty requirements. The main goal of DS2 catalogue is to enhance the functionalities of catalogue systems within existing reference architectures, enabling them to support both intra-data space and inter-data space operations. This includes defining data models for data product offers, data product offer searches, and interactions with members of other data spaces, thereby fostering collaboration across different data spaces. It is listed as an optional module since it is technically possible to handle this at participant level but this creates a lot of overhead for data consumers and providers, however in reality it is a core essential module of DS2 for most practical dataspace sharing scenarios.</p> <p>The core function of the module is to support creating data assets (Data Products), and to provide publication and search interfaces with associated metadata, and data model schemas to support validation. The module ensures that data products are appropriately created, described, and maintained within the catalogue. This includes defining metadata and access policies. It provides intuitive user- and technical interfaces for the publication and discovery of data assets so users can search for and access relevant data products efficiently. It also supports robust data models defined with schemas to ensure data integrity together with description of data service interfaces to access it (mainly based on IDSA reference architecture connectors). Key features of the module include trust building, governance compliance and Interoperability. Catalogues contribute to trust by ensuring that data products and their metadata are accurately described and reliably managed. Catalogues help enforce data governance policies by providing controlled access and visibility to data assets. By adhering to standardized data models like DCAT (Data Catalog Vocabulary, https://www.w3.org/TR/vocab-dcat-3/ ), catalogues ensure seamless data exchange and integration across various data spaces. </p>"},{"location":"modules/CAT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module.</p> <p></p>"},{"location":"modules/CAT/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponents and other functions:</p> <ul> <li> <p>Catalogue UI: Support different users of DS2 to access and manage DS2 offer catalogue. While shown as separate components this implemented as single web UI and backend component using the Catalogue Microservices API.  Depending on participant and user role, UI provides different type of user experience:</p> </li> <li> <p>Catalogue UI for data providers: Extends existing UIs provided by connector implementations with better support for DS2 metadata descriptions. Also supports sharing of offers into multiple dataspaces using Catalogue Microservices. May also provide support for creating metadata based on selected vocabularies.</p> </li> <li> <p>Catalogue UI for data consumers: Extends existing UIs provided by connector implementations with better metadata descriptions and enables browsing and consuming offers shared by multiple dataspaces. Supports presenting user extended metadata associated with DS2 catalogue and offerings.</p> </li> <li> <p>Catalogue UI for data cross-dataspace data sharing: Provides UI for supporting sharing of offerings between dataspaces by e.g. definition of catalogues of catalogues or interoperable data schemas and vocabularies.</p> </li> <li> <p>Catalogue Microservices: Contains extensible set of microservices providing Catalogue Module API for querying and managing catalogue offerings. Initially including:</p> </li> <li> <p>Access Control: Using DS2 trust system services ensures authorization of what functionalities and data provided by other Catalogue microservices is the client UI allowed to access.</p> </li> <li> <p>Catalogues: Using Open-Source catalogue interface provides creation and management DS2 specific catalogue types verifying against the data models defined in DS2 for interoperable data sharing cross data spaces.</p> </li> <li> <p>Metadata: Provides access to metadata related properties in catalogs and datasets for other DS2 components. Provides API functionality for inferred hierarchical ontologies for the UI component.</p> </li> <li> <p>Catalogue Federation: Supports IDSA Catalogue Protocol Specification to share catalogues in DCAT format. Service Can access catalogues (typically from connectors) combine them and provide catalogues using the protocol. This is optional because it may require participant access rights to the individual Dataspaces. Federation may also be supported through Catalogue Module UI in the connector by sending the connector catalogue directly to main DS2 catalogue. Provider UI has access rights to dataspace connector APIs and using catalogue extension the p2p federated catalogue of dataspace. Alignment with older version of IDSA catalogue data model and transformation between it and DCAT may also be provided by this component.</p> </li> </ul> <p>Microservices rely on functionality of lower-level APIs that can be already provided using well stabilized APIs of Open-Source components. In order to avoid vendor lock-in the API operations needed by CAT from these APIs are documented and can be seen as a reference specification if the underlying platforms need to be re-implemented with different components.</p> <ul> <li> <p>Catalogue Management: Core operations on creating and managing catalogues and datasets in DCAT format together with resources (files) associated with them.  While developed as reference implementation on top of management API provided by CKAN portals, this depends on selected set of API operations and specific configuration of CKAN platform, so that this subcomponent could be implemented with another platform if needed.  Existing CKAN portals can also be used, potentially with restrictions on DS2 catalogue functionality. The CKAN API provides operations for managing datasets, resources, tags, organizations, and groups, allowing CAT microservices to create, update, delete, and search these objects within the catalogue. It supports user management, activity tracking, and offers flexible querying options for catalog management. Whilst CKAN provides a single catalog, separate logical catalogs can be managed by assigning them to different organizations or groups. DCAT extension helps to organize and expose datasets in a way that simulates multiple DCAT catalogs within one CKAN instance by leveraging organizations, groups, and tags. Each of these can be presented as distinct catalogs when publishing their metadata in the DCAT format. Implementing some of the functionality of DS2 catalogue may require specific configuration of CKAN or has to be implemented with CKAN extension mechanism so this is considered here as an internal component.</p> </li> <li> <p>Connector Catalogue Extension: Provides catalogue for IDSA information model-based description of catalogues, offers and agreements implemented as EDC connector extension.  CAT optionally extends this existing component to support extended metadata required by DS2. Connector Catalogue Extension should support extended metadata also for IDSA federated catalogue protocol specification data model already implemented by EDC connector as described in next chapter.</p> </li> <li> <p>Data Repository: To provide metadata UI optionally supports selected set of repository platforms providing storage of resources that document what is offered by connectors. Documentation provided for data may be used to create extended metadata and data schemas. Suitable documents can be selected by user to be added to DS2 catalogue offer to be analysed for extended metadata. At minimum simple document file upload is supported by UI.</p> </li> </ul>"},{"location":"modules/CAT/#screenshots","title":"Screenshots","text":"<p>TBD </p>"},{"location":"modules/CAT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source TBD"},{"location":"modules/CAT/#top-features","title":"Top Features","text":"<ul> <li>Provides DS Catalog website containing data products from multiple dataspaces</li> <li>Provides API to register and query catalog products (in development)</li> <li>Provides metadata store for adding and querying extended metadata to Data Products (implemented later)</li> <li>Provides UI for data product provider to read EDC connector catalogue for assets, add metadata and send using API to Catalog website (in development).</li> <li>Provides UI for data product consumer to browse Data Product offers (initially Catalog Website can be used).</li> <li>Provides a dataspace simulation for testing.  </li> </ul>"},{"location":"modules/CAT/#how-to-install","title":"How To Install","text":"<p>CAT module is not expected to be installed by use cases in this phase of project as it depends on DKAN platform and its required tools. DKAN provides the initial UI for browsing the Dataspaces and Data Products. DKAN does not need to be in the same server than rest of CAT software. You can install own installation of DKAN with its external React UI. After installing it needs to be tailored React UI for DS2 and Compoents that tailor the data models. No automatic tailoring of UI is available at this moment. </p> <p>The provider and consumer UI:s and microservices providing DS2 API, and RDF metadata storage will be installed using Docker compose. You need to configure the link to API of tailored DKAN installation togheter with its API key. </p>"},{"location":"modules/CAT/#requirements","title":"Requirements","text":"<p>DKAN requirements can be found on https://dkan.readthedocs.io/en/latest/installation/index.html#requirements. </p> <p>CAT module server requirements will be defined later. </p>"},{"location":"modules/CAT/#software","title":"Software","text":"<ul> <li>DKAN catalog platform (https://getdkan.org/) based on DRUPAL Content Management System (https://new.drupal.org/home)</li> <li>Fastapi.js for CAT API Gateway (https://fastapi.tiangolo.com/)</li> <li>Moleculer.js microservices  for API functionality (https://moleculer.services/index.html)</li> <li>React.js for UI (https://react.dev/)</li> <li>react-jsonschema-form module for editing metadata (https://rjsf-team.github.io/react-jsonschema-form/docs/)</li> <li>Fuseki as metadata store (https://jena.apache.org/documentation/fuseki2/)</li> </ul>"},{"location":"modules/CAT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ul> <li>Install DKAN platform with its external React UI and tailor UI an Data model components for DS2</li> <li>Install DS2 CAT API and UI module and configure to use the DKAN istance</li> </ul>"},{"location":"modules/CAT/#detailed-steps","title":"Detailed steps","text":"<ul> <li>A sandbox DKAN isnstallation Is easiest to be done with video guideline in https://www.youtube.com/watch?v=SnA22Lb6r_M</li> <li>This works only in local machine but you can expose it through proxy like nginx.</li> <li>A full DKAN installation can be done with general DKAN guidelines but you need to knwow how to install DRUPAL and other tools (&lt;in https://dkan.readthedocs.io/en/latest/installation/index.html#).</li> <li>Clone CAT github project. CAT module can be started with Docker Compose UP command in main project directory, tailor the link to DKAN instance. </li> </ul>"},{"location":"modules/CAT/#how-to-use","title":"How To Use","text":"<p>This guideline is prelimintary: * If you have already a connector   * Configure provider UI to use your own EDC connector. Select a Data Product, add extended metadata, and send to the catalogue.   * Check that your Product is available in the DKAN. * If you want to test with simulated connectors   * Use CAT dataspace simulation from CAT subdirectory. It is not part of docker compose yet.    *   Configure a dataspace and set of connectors and data products in the subdirectory of connectors.   *   Run start script, it runs the connectors and registers to the DKAN   * Check you data products in the DKAN. They will be in Simulated Dataspace.   * Run stop script, it stops the connectors and unregisters them from DKAN </p>"},{"location":"modules/CAT/#other-information","title":"Other Information","text":"<p>No other information at the moment for CAT.</p>"},{"location":"modules/CAT/#openapi-specification","title":"OpenAPI Specification","text":"<p>Open API documentation for CAT TBD</p>"},{"location":"modules/CAT/#additional-links","title":"Additional Links","text":"<p>TBD</p>"},{"location":"modules/CLM/","title":"WP05-CLM","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/CLM-module Progress GitHub Project https://github.com/orgs/ds2-eu/projects/36/views/1"},{"location":"modules/CLM/#general-description","title":"General Description","text":"<p>The WP05-CLM module is responsible for enabling the multilingual conceptual understanding, annotation, and enrichment of textual data across the DS2 ecosystem. This includes the creation and lifecycle management of linguistic assets such as tokenizers, morphological analyzers, part-of-speech taggers, and annotated corpora in Romanian, Slovenian, and Greek. These resources feed into downstream modules such as SNER, chatbots, and catalogues, supporting both semantic discovery and interaction across dataspaces.</p> <p>The CLM module orchestrates the domain-specific generation and refinement of ontologies, ensuring that extracted concepts align with DS2\u2019s cross-linguistic and cross-domain architecture. It also exposes APIs and automated R&amp;D workflows to allow other modules and end-users to register, share, and re-use conceptual resources.</p> <p>By M36, the module reaches full maturity across all supported languages and integrates tightly with the conversational, cataloguing, and lifecycle components of DS2, supporting both in-Dataspace and inter-Dataspace interoperability.</p>"},{"location":"modules/CLM/#architecture","title":"Architecture","text":"<p>The figure below represents how the CLM module fits into the DS2 environment, showing its integration with other modules and its role in the broader pipeline:</p> <p></p> <p>The figure below shows the internal architecture of the CLM module, including its primary subcomponents, data flows, and key interfaces with other DS2 modules:</p> <p></p>"},{"location":"modules/CLM/#component-definition","title":"Component Definition","text":"<p>This module includes the following subcomponents and functionalities:</p> <ul> <li> <p>Tokenizer   Fragments a continuous stream of characters into words.</p> </li> <li> <p>Part of Speech (POS) Tagger   Identifies each token as a part of speech (e.g., adjective, noun) and tags it accordingly.</p> </li> <li> <p>Regex   Creates ontology elements based on tokens (e.g., telephone numbers, ISBN codes).</p> </li> <li> <p>Morphology Analyzer   Assigns each token a list of possible stems, each with its current conjugation.</p> </li> <li> <p>Machine Learning   Calculates features required for downstream machine learning components.</p> </li> <li> <p>Named Entity Recognition (NER)   Identifies entities such as places, people, organizations, events, and objects \u2014 even if they were unknown during ontology creation.</p> </li> <li> <p>Lexicon   A glossary of words for each supported language, with grammatical and syntactic constraints. These are mapped to the language-independent ontology, allowing a lexical entry to correspond to one or more ontological concepts under different syntactic and semantic conditions.</p> </li> <li> <p>Rule Engine   Predicts the \"theatre\" of a document (e.g., the primary geographic location mentioned), determines the topic, and infers other document-level properties.</p> </li> <li> <p>Categorization Engine   A machine learning model pretrained using ontological representations of annotated texts. It scores new documents based on how well they align with manually specified category metadata (e.g., how well a document fits within a supplier-defined category).</p> </li> <li> <p>Output Engine   Generates summaries and digests of the analyzed text.</p> </li> </ul>"},{"location":"modules/CLM/#screenshots","title":"Screenshots","text":"<p>No screenshots currently available for this module.</p>"},{"location":"modules/CLM/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Intuview Commercial Use TBD"},{"location":"modules/CLM/#top-features","title":"Top Features","text":"<ol> <li> <p>Romanian Tokenizer Research and implementation of tokenization rules</p> </li> <li> <p>Romanian Morphology Research and building of a morphological analyser, including stems, domain-specific terminology, etc.</p> </li> <li> <p>Romanian POS models Collection, annotation, and training of models</p> </li> <li> <p>Annotation of texts in Romanian for SNER Collection, annotation, and training of models</p> </li> <li> <p>Integration and Testing (Romanian) Building the Romanian module</p> </li> <li> <p>Slovenian NLP Research into the language</p> </li> <li> <p>Slovenian Tokenizer Research and implementation of tokenization rules</p> </li> <li> <p>Slovenian Morphology Research and building of a morphological analyser, including stems, domain-specific terminology, etc.</p> </li> <li> <p>Slovenian POS models Collection, annotation, and training of models</p> </li> <li> <p>Annotation of texts in Slovenian for SNER Collection, annotation, and training of models</p> </li> <li> <p>Integration and Testing (Slovenian) Building the Slovenian module</p> </li> <li> <p>Greek NLP Research into the language</p> </li> <li> <p>Greek Tokenizer Research and implementation of tokenization rules</p> </li> <li> <p>Greek Morphology Research and building of a morphological analyser, including stems, domain-specific terminology, etc.</p> </li> <li> <p>Greek POS models Collection, annotation, and training of models</p> </li> <li> <p>Annotation of texts in Greek for SNER Collection, annotation, and training of models</p> </li> <li> <p>Integration and Testing (Greek) Building the Greek module</p> </li> <li> <p>Ontology Generation per domain Collection of data, classification and generating manual domain-specific ontologies</p> </li> <li> <p>Integration into the Catalogue Database adaptation for ontological data, optimization of the process of exchange of information between the originating dataspace, the CLM module and the Catalogue</p> </li> <li> <p>Integration into the Chatbot Rules for the exchange of information between the Chatbot and the CLM module</p> </li> <li> <p>API Building bespoke API for all DS2 modules</p> </li> <li> <p>Automated Ontology R&amp;D R&amp;D and testing of a method for automated ontology building and anomaly alerts in the existing ontology**</p> </li> </ol>"},{"location":"modules/CLM/#how-to-install","title":"How To Install","text":"<p>\u26a0\ufe0f This module is currently integrated through the central IDT system (in progress). Manual deployment is not supported. Documentation will be updated once integration is finalized.</p>"},{"location":"modules/CLM/#requirements","title":"Requirements","text":"<p>None at the moment.</p>"},{"location":"modules/CLM/#software","title":"Software","text":"<p>None at the moment.</p>"},{"location":"modules/CLM/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>This module is not yet available for manual installation. Once deployment steps are finalized, the summary will include:</p> <ul> <li>Download/setup instructions</li> <li>Environment configuration</li> <li>API exposure</li> <li>Integration notes</li> </ul>"},{"location":"modules/CLM/#detailed-steps","title":"Detailed steps","text":"<p>None at the moment.</p>"},{"location":"modules/CLM/#how-to-use","title":"How To Use","text":"<p>CLM\u2019s resources (ontologies, corpora, taggers) are available to other DS2 modules via REST APIs. Once connected through the IDT, users and modules will be able to query and utilize CLM outputs automatically.</p>"},{"location":"modules/CLM/#example-queries","title":"Example Queries","text":"<p>None at the moment.</p>"},{"location":"modules/CLM/#other-information","title":"Other Information","text":"<p>None at the moment.</p>"},{"location":"modules/CLM/#openapi-specification","title":"OpenAPI Specification","text":"<p>To be added once IDT integration is complete and API endpoints are stable.</p>"},{"location":"modules/CLM/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/CONT/","title":"Containerisation","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/containerisation Progress GitHub Project https://github.com/orgs/ds2-eu/projects/7"},{"location":"modules/CONT/#general-description","title":"General Description","text":"<p>To allow easy and automated packaging and deployment of modules on the IDT Kubernetes runtime subcomponent environment. The containerisation module leverages on custom Helm Chart descriptors to automatically convert them into full Kubernetes Helm Charts representing the module, based on standard base templates located in the DS2 Portal Marketplace. The Helm Charts are then deployed on the IDT Module. </p> <p>The Containerisation module is a core module to the IDT Broker module that enables deployment of all the DS2 modules in the IDT Broker Kubernetes sub-component. The Containerisation module uses Helm Chart standard base templates describing a DS2 module. Those templates are provisioned by the IDT Broker module and provide the standard for DS2 module deployment in IDT Broker. Base templates are stored in the DS2 Portal Marketplace. Then, when uploading a DS2 module by module developers, to the DS2 Portal Marketplace, a custom Helm Chart descriptor that includes values for those base templates needs to be provided with the module. The Containerisation module will use the descriptor together with the base templates to create the Helm Chart for the DS2 module during deployment time on the IDT Broker.</p> <p>The Containerisation module can work in two different modes:</p> <ul> <li>The standard DS2 working mode: developers upload module Helm Chart descriptor to the DS2 Portal Marketplace. Participants use the IDT Broker Kubernetes UI to deploy the descriptor on the IDT. The Containerisation module is triggered when detecting the deployment of that descriptor, retrieves the base templates from the DS2 Portal Marketplace, creates the full Helm Chart and deploys it on the IDT Kubernetes Runtime sub-component</li> <li>The GitOps way: automatic deployment of the Helm Chart descriptor is triggered by the Source controller sub-component upon detecting a change on the descriptor in the DS2 Portal Marketplace. Then as in the previous mode, the Containerisation module, create the full Helm Chart and deploys it on the IDT. This could be the deployment mode of the DS2 Portal</li> </ul> <p>In both cases, the only difference is how the Helm Chart descriptor is deployed on the IDT either by the participant manually deploying the descriptor, or being automatically deployed by the Source Controller sub-component.</p>"},{"location":"modules/CONT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/CONT/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li> <p>ChartController: The ChartController is a Kubernetes controller, following the Kubernetes controller pattern which keeps track of a new Kubernetes custom resource definition - the \u201cHelmChartDescriptor\u201d. When changes are detected on a descriptor, ie. addition, update, the Controller connects to a configured location ie. GitHub repository, to download the corresponding Helm Chart base templates. Then, together with the HelmChartDescriptor, the Chart Controller will create a full Helm Chart describing the module. This Helm Chart will be deployed into the IDT Kubernetes Runtime subcomponent using the Installer component. </p> </li> <li> <p>ChartManager: The ChartManager is mainly used to monitor the Helm Charts and HelmChartDescriptors deployed in the system. It will query the IDT Module\u2019s Kubernetes subcomponent to retrieve current Charts and descriptors. The Chart Manager can also be used to create a HelmChartDescriptor using some input parameters and install it via the Installer component. Once installed, the ChartController will detect the new ChartDescriptor and will convert it to a Chart deploying it back into the IDT Module\u2019s Kubernetes subcomponent. </p> </li> <li> <p>Installer: This is the component responsible for installing Helm Charts and HelmChartDescriptors in the IDT Kubernetes subcomponent. It will receive the corresponding Charts and HelmChartDescriptors and will apply them in the IDT Kubernetes subcomponent. The Installer also takes care of installing new Sources created by the Source Manager component.  </p> </li> <li> <p>Containerisation UI: This is the main module UI that allows users to monitor current existing Charts, ChartDescriptors and Sources in the system. Users will have an overview of what is installed in the system and its current status regarding to those specific resources. The UI can also be used to create, update or delete ChartDescriptors via the ChartManager and Sources via the Source Manager.  </p> </li> <li> <p>GitOps Source Controller: The Source Controller, similar to the ChartController,  is a Kubernetes controller that keeps track of the custom resource definition Source. A Source mainly represents a reference to a repository where ChartDescriptors are stored. The Source Controller monitors the status of the Source and reacts to changes by reflecting those changes in the IDT Kubernetes subcomponent. The Source Controller is an optional subcomponent, and users can just install the ChartDescriptors using the IDT or via Kubernetes standard kubectl. </p> </li> <li> <p>(DS2) GitOps Source Manager: The Source Manager, similar to the ChartManager is mainly used to monitor the Source in the system and is customised to DS2. It can also be used to create, update, and delete new sources that will be installed via the Installer component. As the Source Controller, this is an optional component. </p> </li> <li> <p>Tier 1 Service Stack for Marketplace and deployment and API: The full stack will be implemented as generically described elsewhere in this document. Exceptions: This module runs in the IDT and uses the IDT Kubernetes subcomponent for Chart and ChartDescriptor installations. The DS2 Portal Marketplace component and its repository system is used to store the Chart base templates. Since the DS2 Portal is also a DS2 module, it is deployed and run on the IDT, so Containerisation module can also be used for the DS2 Portal and other intermediary services. </p> </li> </ul>"},{"location":"modules/CONT/#screenshots","title":"Screenshots","text":"<p>The Containerisation UI development has not yet been started, so no screenshots. </p>"},{"location":"modules/CONT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/CONT/#top-features","title":"Top Features","text":"<ol> <li>Kubernetes Native: The CONT module is a Kubernetes native solution based on open-source system Flux and The GitOps Toolkit</li> <li>Kubernetes Application Deployment Control: Provides control to Kubernetes administrators or SREs over what and how a module or application is deployed on a given Kubernets cluster ie. IDT2. </li> <li>Kubernetes Application Abstraction: The CONT module abstracts developers from the Kubernetes complexity when creating a Kubernetes application, leveraging on a templating system based on Helm Charts and Flux HelmRelease CRD</li> <li>Helm Chart Templates: Ability to create Helm Chart templates for different types of applications</li> <li>Helm Release CRDs Templates: Based on the HelmRelease CRDs from Flux, the CONT module enables the creation of HelmRelease templates that will make use of the Helm Chart Templates</li> <li>Application Management using API: Manage application (module) lifecycle (create, install, uninstall, delet) using the CONT Chart Manager API</li> <li>Containerisation UI: Manage the Containerisation module using a modern web based UI</li> <li>Operator vs Developer View: Access the Containerisation module features with different views depending on the role Operators vs Developer  </li> </ol>"},{"location":"modules/CONT/#how-to-install","title":"How To Install","text":"<p>The Containerisation module will be part of the IDT installation, but a standalone installer is so far provided in order to be able to work with it, which installs Flux Helm and Source Controllers in order to create the Helm Chart from the HelmRelease and Chart templates. </p>"},{"location":"modules/CONT/#requirements","title":"Requirements","text":"<p>The IDT or a Kubernetes cluster is required.</p> <p>Resources:</p>"},{"location":"modules/CONT/#software","title":"Software","text":"<p>Containerisation module so far installs these software utilities and specific tested compatible versions:</p> <ul> <li>Flux Helm Controller (Chart Controller + Installer)</li> <li>Flux Source Controller (Source Controller)</li> <li>Flux Notification Controller (default)</li> <li>Flux Kustomization Controller (default)</li> </ul> <p>To Be Implemented: * Chart Manager * Source Manager * Containerisation UI</p>"},{"location":"modules/CONT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ol> <li> <p>Clone the repo containerisation</p> </li> <li> <p>Install the Containerisation module by running the installfluxghorg.sh script   </p> </li> </ol>"},{"location":"modules/CONT/#detailed-steps","title":"Detailed steps","text":"<ol> <li> <p>Clone the repo containerisation <pre><code>git clone https://github.com/ds2-eu/containerisation\n</code></pre></p> </li> <li> <p>Install the Containerisation module by running the installfluxghorg.sh script : So far the Kubernetes controllers are available, which will deploy a template helm chart from a helmrelease CRD <pre><code>./installfluxghorg.sh github_token github_organisation org\n</code></pre></p> <p>github_token: a GitHub user token</p> </li> </ol> <p>github_organisation: a GitHub organisation: this will be in a later stage  the organisation from the Marketplace ie. ds2-marketplace where modules are stored</p> <p>org: this is a repository in the GitHub organisation: this will be in a later stage a repository of the organisation id of the participant as registered in the Portal. The repository is created via the Marketplace and the acquired modules will be located in that GitHub repository -&gt; This is linked to the Marketplace purchase process</p>"},{"location":"modules/CONT/#how-to-use","title":"How To Use","text":"<p>When first set of Helm Chart and HelmRelease Templates are ready: To Be Done</p>"},{"location":"modules/CONT/#other-information","title":"Other Information","text":"<p>No other information at the moment for Containerisation</p>"},{"location":"modules/CONT/#openapi-specification","title":"OpenAPI Specification","text":"<p>To Be Done: No API</p>"},{"location":"modules/CONT/#additional-links","title":"Additional Links","text":"<p>Video https://youtube.com/cont</p> <p>Flux https://fluxcd.io/</p> <p>Containerisation Repository https://github.com/ds2-eu/containerisation (Private Link for Project Members)</p>"},{"location":"modules/CUR/","title":"CUR - DS2 Data Curation module","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/cur.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/39"},{"location":"modules/CUR/#general-description","title":"General Description","text":"<p>Purpose: Data obtained from disparate sources runs the risk of remaining siloed unless it can be joined with similar data from other data sets. The process of joining data may require curation of data, such as the conversion of data formats or a matching of disparate column names.  Manual curation of datasets, however, can be a labour-intensive task, and not suited to DS2\u2019s dynamic nature of federating dataspaces.  Additionally, filtering data based on a set of conditions from even a single data source is a task that typically will require programming skills and is both time-intensive and costly.</p> <p>Instead, the CUR module establishes automatic curatation and federation of data through machine learning, allowing users to query data through natural language, without having to understand how to connect or convert data obtained from different sources.</p>"},{"location":"modules/CUR/#description","title":"Description","text":"<p>In order to allow querying of data obtained from various sources, the Curation model harnesses the power of two different technologies, namely relational databases and machine learning Large Language Models (LLMs).  The module stores obtained data in a relational database, and then uses machine learning combined with a knowledge of the database schemas to convert user queries in natural language to database queries (i.e. SQL queries). Given the proper prompts, the LLM is able to automatically determine how to match columns in different tables, even when column names or data formats may differ.  This module uses several different techniques to increase the chances of correctly answering a user query, such as evaluating the results of the generated query before they are returned to the user, and RAG technology to seed LLM SQL generation with examples of similar user queries and their correctly generated SQL calls from previous history.</p> <p>There still will be cases where the generation of an SQL query corresponding to a user's question will fail.  For example, if the available data sources relate to weather conditions in Africa and the user asks a question about food prices in Europe, query formulation will naturally - and correctly - fail.  However, there will be other cases where a query which is appropriate for the data sets will not be properly generated due to reasons of complexity.  In this case, the execution chain of the module will detect the failure and request human-in-the-loop intervention.  At this point, the original query can either be abandoned, or the generated SQL query can be manually corrected and re-executed.  If the subsequent execution succeeds, the user query and corrected SQL will be stored in a vector store to seed future, similar user queries.  Of course it is not expected that the typical user will correct SQL queries, but rather that the CUR module will be trained by an expert in advance to cover anticipated user queries.</p>"},{"location":"modules/CUR/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/CUR/#component-definition","title":"Component Definition","text":"<p>The CUR module consists of two main parts - a configuration component to set up the database tables from the data sources and the component which is given a reference to the stored database base and performs the pipeline required to allow a user to get information from the data sources using natural language queries. In particular, this latter component is composed of a number of stages which include:</p> <ul> <li>The creation of a GUI for user input</li> <li>The creation of an SQL query from a natural language query</li> <li>The execution of the generated SQL query</li> <li>Evaluation of results of the SQL query</li> <li>Human-in-the-loop injection of a corrected SQL query</li> <li>Reformulation of the query results to generate a more readable answer</li> </ul> <p>Within the above stages, there are a number of substages such as RAG retrieval, vector store population, prompt creation etc.</p>"},{"location":"modules/CUR/#screenshots","title":"Screenshots","text":"<p>The following shows how a natural language query asking about pollution levels in the vicinity of the monitoring station, \"MS Raki\u010dan\" can be executed. </p>"},{"location":"modules/CUR/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License IBM Consortium Confidential TBD"},{"location":"modules/CUR/#top-features","title":"Top Features","text":"<ul> <li>Extraction of data based on natural language queries</li> <li>Support for a wide variety of user languages</li> <li>Automatic federation of disparate data sources</li> <li>Human-in-the-loop intervention allows the LLM to self-learn</li> <li>RAG technology supporting hybrid retrieval with both semantic and keyword search</li> <li>Easy to add additional data sources</li> </ul>"},{"location":"modules/CUR/#how-to-install","title":"How To Install","text":""},{"location":"modules/CUR/#setting-up-the-database","title":"Setting up the database","text":"<p>Data obtained from the data sources will accessed from their URLs and stored into a database.  The recommended database is postgresql due to it support for geospatial queries.  The configuration of the database is as follows:</p> <ol> <li>Install postgresql.  See https://www.postgresql.org/download</li> <li>After installing postgresql, it is recommended to add the environment variables, DS2_DB_USERNAME and DS2_DB_PASSWORD to your shell, with username and database password values which were used in installing postgresql in the previous step.</li> <li>Edit the file, \"federation_config.json\" as follows:<ul> <li>Change the value of the \"database\", \"host\" and \"port\" entries as required to match the values used installing the postgresql database.</li> <li>This configuration file is pre-configured with data sources for the DS2 Slovenian, Romanian and Greek use cases.  Other use cases or additional data sources can be added by following the same format.  Note that currently the \"open_api\" field is not used by the software.</li> <li>Any API tokens required by the REST call are shown within brackets, for example:'Authorization: Bearer '.  At runtime, the software will attempt to replace the bracketed value with an environment variable of the same name - for example the environment variable, \"DADS_BEARER_TOKEN\" in this example.  Therefore, configure these values in your environment. <li>To create the SQL database the first time, run <code>python create_database.py --prefix [name]</code>.  The value for [name] needs to correspond exactly to a URI entry name in the federation_config.json file (for example, \"Slovenia\", \"Romania\" or \"Greece\").  As an example, to build the database for the DS2 Slovenian use case, we would run:  <code>python create_database.py --prefix Slovenia</code>.</li>"},{"location":"modules/CUR/#running-the-cur-module","title":"Running the CUR module","text":"<p>In order to run the CUR module, the correct python environment needs to be created.  It is recommended that you create a virtual environment as follows:</p> <ul> <li><code>python -m venv [name]</code>  where \"name\" will be the name of the virtual environment, for example, \"venv\".</li> <li>Activate the virtual environment: On Mac or Linux this will be: <code>source [name]/bin/activate</code>, where the value of \"name\" is from the previous step.</li> <li>Install inside the virtual environment the required modules: <code>pip install requirements.txtx</code></li> </ul> <p>Once the environment is set up, the CUR module can be run.  Note that the CUR module requires access to an LLM. The access key for the LLM can be defined by configuring and environment variable, LLM_APIKEY, in your sheel.  If this is not configured, then at runtime the module will request the key value.</p> <p>The CUR module is invoked as follows:</p> <p><code>python ds2_federation_main.py --prefix [prefix] --url [url]</code></p> <p>where the value for \"prefix\" is the as described in the previous section for setting up the db, and the value for \"url\" is the base address of the LLM (i.e without the model name).  CUR uses llama-3-3-70b-instruct as its default model.  This can be overridden by using the --name and --id command line values (e.g. --name llama-3-3-70b-instruct --id meta-llama/llama-3-3-70b-instruct).  Note that the accuracy of the NL2SQL translation in the CUR module may depend on the model used.</p>"},{"location":"modules/CUR/#requirements","title":"Requirements","text":"<p>An external LLM is required.  No particular requirements for CPU, memory or storage have been identified.</p> <p>Using the dashbutton requires a configured keycloak server to be running.</p>"},{"location":"modules/CUR/#how-to-use","title":"How to use","text":"<p>The CUR GUI is web-based.  When CUR is started as described in the previous step, it will print out where it can be accessed, typically this will be at: <code>http://127.0.0.1:5000</code> for an installation on the local host.</p> <p>To search for data, enter in the What would you like to know? box your query and then click the Ask button.  Note that the query needs to relate to values in the data sets configured in the federation_config.json file.  If the query succeeds, the answer will appear in the Answer box.  If the query fails, then you will be instructed to either correct the generated SQL query, or enter \"skip\".  If you choose to correct the SQL, enter this in the Corrected SQL box and then click on Execute, otherwise click on the \"Skip\" button to move on to a new query.</p>"},{"location":"modules/CUR/#using-the-dashbutton","title":"Using the dashbutton","text":"<p>If the dashbutton is used, then a keycloak server needs to be configured.</p> <p>Reference: <code>https://github.com/ds2-eu/portal/tree/main/dash-button/web-component</code></p> <p>Run keycloak with:</p> <p><code>docker run -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:25.0.2 start-dev</code></p> <p>Go to the keycloak admin screen:</p> <p><code>http://localhost:8080/admin/master/console/</code></p> <p>Click on \"Keycloak\" in the upper left corner to get to \"Create realm\" </p> <p>Create a realm with name \"demo\".</p> <p>Click on \"Clients\" and then \"Create Clients\"</p>"},{"location":"modules/CUR/#additional-links","title":"Additional Links","text":""},{"location":"modules/DARC/","title":"DARC","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/DARC.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/29"},{"location":"modules/DARC/#general-description","title":"General Description","text":"<p>To inquire, discover and assess though the conversational UI of an AI-driven agent, the In-Dataspace and Inter-Dataspace data-oriented capabilities and limitations of DS2 Dataspaces as well as DS2 modules (software prerequisites and/or recommendations) which will compose the \u201cideal use\u201d scenarios/ paths that will fit the needs of the end-users. Then, to complement this by recommending to end-users, though the conversational UI of an AI-driven agent, the best DS2 modules for the implementation of the selected \u201cideal use\u201d scenario/ path, based on the outcomes of the T5.2 module end-user interaction. Finally, to demonstrate the autoconfiguring, through APIs, a subset of module based on these recommendations so that they can easily create their DS2 pipeline and start using and sharing data. </p> <p>The DARC Module is responsible for allowing data space participants (Data Providers and Data Consumers) to inquire and discover and receive AI-driven assessment, of prerequisites for the successful execution of complex Digital Life Cycles (DLCs) between participating Dataspaces. Then, for DARC to make recommendations of modules and their configuration based on this assessment with the end game to automatically configure modules which are suitably enabled for configuration.</p>"},{"location":"modules/DARC/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/DARC/#component-definition","title":"Component Definition","text":"<p>The subcomponents of DARC are as follows: *   AI-driven Chat Bot UI: The User interface subcomponent through which end-users interact with DARC. It allows end-users to provide inputs and receive outputs, facilitating a smooth and intuitive interaction DARC. This subcomponent directly interfaces with end-users for dialogue exchange.</p> <ul> <li> <p>Prompt Handler: The Prompt Handler facilitates the exchange of dialogue between the Large Language Model and end-users. It performs initial checks, correcting grammar errors and verifying supported languages. If a language is not supported, the module engages the Ontology/Multilingual Processor for language disambiguation before forwarding the prompt to the LLM Core. This ensures that interactions are both clear and linguistically compatible.</p> </li> <li> <p>LLM Manager: The LLM Manager subcomponent oversees and coordinates the operations of the language model. It ensures seamless integration with other DARC subcomponents and manages the flow of data to and from the LLM core. It acts as an manager and is responsible for all DARC\u2019s module operations that interact with the LLM. This subcomponent facilitates interaction with the LLM Core, managing dialogues, transferring crucial information from the Knowledge Base (using the sentence encoder and semantic search subcomponents), and providing it to the LLM Core to generate outcomes based on user requests. Furthermore, it engages with the Auto Configuration subcomponent to forward the configuration file generated by the LLM Core and reports back to the LLM Core the status of auto configuration to update the user with the status.</p> </li> <li> <p>LLM Core: The primary language model that generates outputs based on the processed inputs and configurations. The LLM Core is the heart of DARC, providing the natural language processing capabilities that drive DARC\u2019s recommendation and configuration capabilities. This subcomponent actively interacts with the LLM Manager, requesting additional details about user queries and receiving crucial information from the knowledge base to ensure dynamic and contextually aware interactions. Additionally, it works with the Prompt Handler, which forwards the tailored responses back to the user, completing the communication loop effectively.</p> </li> <li> <p>Sentence Encoder: The Sentence Encoder, an AI model, converts dialogues and knowledge base content into embeddings (a structured numerical format that makes complex linguistic data more understandable for the LLM and other AI components). This process supports LLM operations by enabling the Semantic Search subcomponent to extract and decode crucial information from these embeddings, transforming it back into text for the LLM Core. This integration enhances DARC\u2019s ability to deliver precise and contextually relevant responses to user queries.</p> </li> <li> <p>Semantic Search: This subcomponent enhances search functionality by using context-aware algorithms to analyse transformed data from the Knowledge Base and inputs derived from user prompts. It processes this information and relays it to the Sentence Encoder subcomponent , which converts the embeddings back into textual data. This allows the LLM to further process and understand the content effectively.</p> </li> <li> <p>Internal Knowledge Base: This is a central repository that stores all essential data and knowledge, such as Regulations and DS2 module documentation, configuration. Serving as the backbone of the application, it provides a robust database that DARC subcomponents can access through Knowledge Management. This ensures that DARC has a solid informational foundation for making informed decisions and configurations. The Knowledge Base works in conjunction with the Knowledge Management subcomponent , which continuously populates it with critical operational details, such as module configurations and compliance with EC regulations. It also interfaces with Semantic Search to extract necessary information for the LLM.</p> </li> <li> <p>Knowledge Management System: The Knowledge Management System subcomponent organises, stores, and retrieves knowledge within DARC. It facilitates efficient information management by structuring and indexing data such as, Dataspace regulations, DS2 modules documentation and configuration information, for easy access and distribution of said data within DARC subcomponents.  The Knowledge Management subcomponent supports the internal Knowledge Base and interacts with other DARC subcomponents, namely LLM Manager and Ontology Processor, to provide relevant information as needed. This subcomponent engages with the Data Sovereign Module API, the Culture and Language Module API, and Dataspace Meta-data Broker ensuring that the Knowledge Base is continually updated with crucial information necessary for DARC\u2019s operations. The Knowledge Management System also interacts with the internal knowledge base by incorporating previous information into the internal knowledge base for enhanced decision-making and operational efficiency.</p> </li> <li> <p>Ontology Processor:  This subcomponent plays a crucial role in managing ontology information within DARC. It is tasked with processing, storing, and updating ontology data in the Knowledge Base. Additionally, it retrieves and analyses the ontology of data already present in the Knowledge Base. By ensuring that ontology information is accurately maintained and accessible, this subcomponent enhances the DARC's ability to interpret and utilize data effectively, fostering a deeper understanding of the relationships and structures within the stored information.</p> </li> <li> <p>Multilingual Processor: This  subcomponent provides robust cross-language support, enhancing DARC's global functionality. It processes outputs from the Culture and Language subcomponent, enabling the LLM to comprehend and interact with previously unknown languages. Additionally, it interacts with the Prompt Handler subcomponent to ensure that multilingual interactions are seamlessly integrated, allowing users to receive assistance in their preferred language. This capability significantly improves accessibility and user experience across diverse linguistic backgrounds.</p> </li> <li> <p>Auto-Configuration:  This subcomponent plays a crucial role in automating the setup process for DS2 modules. It begins by receiving a configuration file that has been generated and forwarded by LLM Manager, originally derived from the LLM core. The subcomponent systematically examines this file to identify and rectify any potential errors or inconsistencies. Once it confirms that the configuration file is error-free, it proceeds to implement these settings directly on the DS2 modules\u2019 endpoint. After its successful application, it completes the process by sending a detailed status report back to the LLM through the LLM Manager subcomponent, ensuring seamless communication and integration within DARC.</p> </li> <li> <p>Auto configuration Support Database: This subcomponent maintains essential information such as module endpoints and configuration rules to facilitate the functionality of the auto-configuration subcomponent.</p> </li> <li> <p>Services and APIs: </p> </li> <li> <p>Catalog Module: Catalog Module Meta data broker will send the requested Dataspaces meta-data to DARC\u2019s Knowledge Management subcomponent for discovering data in the participating Dataspaces.</p> </li> <li> <p>Tier 2 DS Modules/Service: Auto-configuration service. For Tier 2 DS Modules via the Auto-Configuration Module.</p> </li> <li> <p>Tier 0 Support Service Stack:</p> <ul> <li> <p>Sovereignty Decision Support Module Risk Knowledge Base: API will be driven by DARC to provide information about potential security risks involved during the configuration, assessment, and definition of a participant\u2019s DLC.</p> </li> <li> <p>Culture and Language Module. Ontology and Multi-language Facility Module API \u2013 will provide DARC with multilingual support and ontology disambiguation, enhancing DARC's ability to interpret and process diverse linguistic data accurately and effectively across different languages.</p> </li> </ul> </li> <li> <p>Tier 1 Service Stack for Marketplace and deployment and API: The full stack will be implemented as generically described elsewhere in this document. Exceptions: The Platform will only be needed for inter-participant service orchestrations were used</p> </li> </ul>"},{"location":"modules/DARC/#screenshots","title":"Screenshots","text":"<p>[Changes on UI are still in progress to align with project's requirements.]</p>"},{"location":"modules/DARC/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ATC Commercial Use Apache 2.0"},{"location":"modules/DARC/#top-features","title":"Top Features","text":"<ol> <li> <p>AI-Driven Conversational UI: Provides an intelligent, user-friendly interface for end-users to interactively inquire, assess, and receive guidance about DS2 Dataspaces and modules.</p> </li> <li> <p>In-Dataspace and Inter-Dataspace Assessment: Enables analysis of capabilities and limitations within a single Dataspace and across multiple interconnected Dataspaces, supporting more complex digital collaboration.</p> </li> <li> <p>Dynamic End-User Profiling &amp; Needs Matching: Assesses user needs and recommends the best-suited DS2 modules to implement ideal data workflows and pipelines.</p> </li> <li> <p>Modular Recommendation Engine: Recommends specific DS2 modules (with configurations and software prerequisites).</p> </li> <li> <p>Auto-Configuration via APIs: Supports seamless, API-based automatic configuration of selected modules to build ready-to-use DS2 pipelines for data usage and sharing.</p> </li> <li> <p>DARC Module for Digital Lifecycle Execution: DARC facilitates AI-driven assessment of prerequisites and feasibility for complex Digital Life Cycles (DLCs) involving multiple data providers and consumers.</p> </li> <li> <p>Cross-Dataspace Collaboration Support: Allows participants from different Dataspaces to align requirements, assess interoperability, and coordinate execution of digital workflows.</p> </li> <li> <p>Lifecycle-Aware Module Configuration: Recommendations and configurations are aware of and tailored to different stages of a data lifecycle, optimizing module use and integration.</p> </li> <li> <p>Data Sharing &amp; Reusability Enablement: Streamlines the setup of reusable data workflows, enhancing data accessibility, shareability, and interoperability across modules and Dataspaces.</p> </li> <li> <p>End-to-End Guided Experience: Offers a fully guided, conversational journey from assessment to configuration and activation, lowering the technical entry barrier for users.</p> </li> </ol>"},{"location":"modules/DARC/#how-to-install","title":"How To Install","text":"<p>To install and run the DARC Assistant locally, first make sure you have the app (the docker image/code) and create the Conda environment provided in environment.yml. Once the environment is activated, start the Weaviate vector store using Docker Compose. After Weaviate is up and running, launch the conversational interface with our custom UI using Chainlit python package. Finally, open your browser and navigate to http://localhost:8000 (or the VMs IP if your are using a cloud provider for the Virtual Machine) to start using the DARC Assistant. </p>"},{"location":"modules/DARC/#requirements","title":"Requirements","text":"Azure VM NC8as T4 v3 CPU 8 RAM 56 GiB GPU 1xT4"},{"location":"modules/DARC/#software","title":"Software","text":"<ul> <li>Python 3.12</li> <li>Conda (Anaconda or Miniconda)</li> <li>OS: Linux (as indicated by your environment path)</li> <li>Docker &amp; Docker Compose</li> <li>Modern web browser (Chrome, Firefox, etc.)</li> </ul>"},{"location":"modules/DARC/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>Install/Download the app (docker image), create and activate the Conda environment, start Weaviate with Docker, run the UI, and access the platform via http://localhost:8000 in your browser.</p>"},{"location":"modules/DARC/#detailed-steps","title":"Detailed steps","text":"<ol> <li> <p>Create the Conda Environment:</p> <p>Make sure Conda is installed. Then run: <pre><code>conda env create -f environment.yml\n</code></pre> where environment.yml include all the appropriate packages needed to run DARC module. 2. Activate the Environment:</p> <pre><code>conda activate ds2\n</code></pre> </li> <li> <p>Start Weaviate via Docker:</p> <p>Make sure Docker is running, then: <pre><code>docker-compose -f docker-compose-weaviate.yml up -d\n</code></pre> 4. Run UI:</p> <p>From the root of the repo: <pre><code>chainlit run main.py -w\n</code></pre> 5. Access the Web Interface:</p> <p>Open your browser and go to:</p> <pre><code>http://localhost:8000\n</code></pre> <p>If running on a remote server, use your machine's IP address (e.g., http://192.168.x.x:8000)</p> </li> </ol>"},{"location":"modules/DARC/#how-to-use","title":"How To Use","text":"<p>As long as you have access to the Web Interface, you can type your questions in Natural Language (English) to ask about anything related to the DS2 project.</p> <p>You don\u2019t need to use technical commands or specific syntax\u2014just ask your questions like you would ask a person.</p>"},{"location":"modules/DARC/#example-queries","title":"Example Queries","text":"<p>Here are a few sample queries you can type:</p> <p>\"What is DS2?\" \u2192  This will give you a general overview of the DS2 project, including its purpose and scope.</p> <p>\"What modules are available in DS2?\" \u2192  This will list all current modules, tools, or components included in the DS2 system.</p>"},{"location":"modules/DARC/#other-information","title":"Other Information","text":"<p>No other information at the moment for MODULE.</p>"},{"location":"modules/DARC/#openapi-specification","title":"OpenAPI Specification","text":"<p>To Be Done.</p>"},{"location":"modules/DARC/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/DDT/","title":"Data Detection and Transformation (DDT)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/ddt.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/22"},{"location":"modules/DDT/#general-description","title":"General Description","text":"<p>Purpose: Dataspaces allow for data to be shared between data providers and data consumers. A lot of data comes from sensors and devices at a high rate. To allow for a well-defined data structure and quality during the data generation and exchange, DDT is a module that can analyse data on the fly.</p> <p>Description: DDT integrates into the data pipeline where data is collected from edge devices and forwarded to cloud platforms. To ensure that this data is trustworthy and shareable across different dataspaces, it is critical to verify its quality. The module subscribes to incoming MQTT topics and evaluates each message against predefined data quality rules.</p>"},{"location":"modules/DDT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/DDT/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li>Core (Designtime)</li> <li> <p>DDT provides a user interface by which</p> <ul> <li>the AI algorithms are developed, tested and deployed to the AI detection component</li> <li>the transformation algorithms are developed, tested and deployed to the transformation component.</li> </ul> </li> <li> <p>Core (Runtime)</p> </li> <li> <p>This is the heart of the Data Detection and Transformation Module. It is responsible for collecting the data input from the external source and pass it on to the AI detection and the transformation components. This uses either the HTTP or the MQTT APIs. In a similar fashion it also collects the information provided by the Cultural and Language module and passes it on to the other components. Once processed and used, the data and results of the data analysis will be sent out to the Information Consumer and the Data Consumer, also via the same HTTP or the MQTT APIs.</p> </li> <li> <p>AI Detection</p> </li> <li>This component supports the execution of AI models developed using Python or other programming languages supported by Apama. It operates on data and returns information on the data worked on like if an anomaly has occurred or if the data quality has decreased. Many models will be provided to the user and the model selection is done together with the T4.2 project partners INDRA and DIGI.</li> <li> <p>It has the two subcomponents Data Inspection and Model Selection where based on the result of the Data Inspection, the appropriate AI model is selected.</p> </li> <li> <p>AI Transformation</p> </li> <li>This component operates on the data passed on by the Core (Runtime) directly. It will detect schema changes and can transform the data into the correct format on the fly. It can also use the information provided by the DS2 Culture and Language module to define rules and limits what the data values should be and act accordingly, e.g. by eliminating out-of-range values.</li> <li> <p>It has the three subcomponents Analyze Data, Select Transformation and Transform Data.</p> </li> <li> <p>APIs: The Core runtime component has the following interfaces</p> </li> <li>HTTP Server: This module will provide a HTTP interface to send and receive data and information from the DS2 Culture and Language module. Different URLs will be specified to distinguish between input and output channels and if the AI Detection or Transformation component should be used.</li> <li>MQTT Broker: This is the de-facto standard for machine-to-machine communicate so an interface is provided to send and receive data from this module via publish and subscribe methods to certain topics. This can be used to specify the input and output channels of this module and also if the AI Detection or Transformation component should be used</li> </ul> <p>External Components Used * Data Source   * The data provider or data consumer, depending on where this module is deployed, configures and selects where the data to be analysed and transformed comes from</p> <ul> <li>Tier 1 Service Stack for Marketplace and Development</li> <li> <p>The module uses the portal to publish its configuration</p> </li> <li> <p>Tier 0 DS2 Support</p> </li> <li> <p>The information on the data like format and schema comes from the Cultural and Language module</p> </li> <li> <p>Data Consumer</p> </li> <li> <p>The data provider or data consumer, depending on where this module is deployed, configures where the transformed data is sent to.</p> </li> <li> <p>Information Consumer</p> </li> <li>The data provider or data consumer, depending on where this module is deployed, configures where the information on anomalous behaviour, possible error conditions and the data quality is sent to.</li> </ul> <p>External Interaction   * User:     The user uses the UI to develop, test and deploy the AI detection algorithms and the transformation algorithms to the corresponding components.</p>"},{"location":"modules/DDT/#screenshots","title":"Screenshots","text":""},{"location":"modules/DDT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Software GmbH Open Source Apache 2.0 (pending)"},{"location":"modules/DDT/#top-features","title":"Top Features","text":"<p>1. Ensuring trust:  By analysing data streamed to or from data spaces in realtime, data quality can be assessed in realtime, increasing the reliability of shared data.</p> <p>2. Configurable data analysis: In order to allow for a realtime quality inspection, DDT enables users to flexibly define data analysis measures by  UI based configuration.</p> <p>3. Attribute based expectations: To define criteria for data quality, users can define expectations based on data attribute level.</p> <p>4. Anomaly detection &amp; correction strategies:  The DDT module can be configured to detect and correct anomalies in the data stream through a transformation such as  interpolation between data points.</p> <p>5. Improving data consumablity:  Syntactic conversion to handle differences in source and target schemas allows for an easier use and integration of dataspace data into existing systems and enhances data federation across disparate data sources.</p> <p>6. Data Syntax Check: In addition to quality rules, DDT will provide an automated mechanism to infer attribute types \u2014 distinguishing  identifiers, measurements, and context data. This supports more context-aware syntax validation.</p> <p>7. Alerting:  DDT can be configured to send alerts, i.e. to inform about encountered data quality issues.</p> <p>8. E2C support: To allow for an easy integration of edge data, DDT supports DS2 E2C module by importing E2C configuration files to  preconfigure attributes in the DDT UI.</p>"},{"location":"modules/DDT/#how-to-install","title":"How To Install","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#requirements","title":"Requirements","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#software","title":"Software","text":"<p>N/A</p>"},{"location":"modules/DDT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#detailed-steps","title":"Detailed steps","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#how-to-use","title":"How To Use","text":"<p>To Be Done</p>"},{"location":"modules/DDT/#other-information","title":"Other Information","text":"<p>No other information at the moment for DDT.</p>"},{"location":"modules/DDT/#openapi-specification","title":"OpenAPI Specification","text":"<p>N/A</p>"},{"location":"modules/DDT/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/DINS/","title":"Data Inspector (DINS)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/dins_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/28"},{"location":"modules/DINS/#general-description","title":"General Description","text":"<p>The Data Inspection Module (DINS) facilitates the configuration and deployment of processes for real-time data analysis, ensuring data quality and compliance with thresholds set by the parties involved. It performs several key functions: generating notifications based on the values of the exchanged data, executing reactions such as sending requests and notifications to external tools, and integrating with models developed to enhance its capabilities. It is a complement to the Data Share Controller which focuses on control information with both modules using the Data Interceptor.</p> <p>The Data Inspection is responsible for analysing the data provided and/or consumed by participants, ensuring the quality of the data shared amongst them. For example, a consumer can use DINS to set up analysis jobs that define rules for data validation. This includes establishing thresholds for specific data values; when these thresholds are exceeded, issues are generated and recorded for monitoring. Additionally, the DINS module can trigger notifications to other modules and components when issues are detected. For instance, it can notify the Data Share Controller Module, which may respond by blocking data sharing if certain limits are reached.</p>"},{"location":"modules/DINS/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment.</p> <p></p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module.</p> <p></p>"},{"location":"modules/DINS/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions: * Data Jobs IDE: This group of components is shared with the T4.1 Policy Enforcement Modules. Some of them will be developed based on existing technology, specifically the Dataflow component of the Onesait Platform open-source product. While T4.1 Policy Enforcement focuses on defining and executing data transformation jobs, the Data Inspection module will enhance the current capabilities of Dataflow to include data inspection, monitoring, and notifications. T6.2 will lead this component development, and T4.1 will be built on the improved version from T6.2. Therefore, T4.1 has a dependency on T6.2, but the opposite is not true.   * IDE UI: This graphical user interface allows users to define Data Inspection Jobs for monitoring data and triggering notifications and alerts. For example, it can evaluate values against specified thresholds. This component is based on the existing INDRA software, with updates needed to support new features for DS2.   * IDE Controller: This component manages Data Inspection Jobs during design time and oversees their deployment and monitoring at runtime. It is based on current INDRA software but requires significant upgrades to split the tool into the IDE component and the Runtime component (potentially several).    * Job Definition Storage: This component stores definitions of Data Inspection Jobs, based on INDRA software, with extensions planned to improve version control of the definitions. The definitions are created through a graphical user interface, enabling users to design Data Inspection Jobs as data pipelines using a drag-and-drop approach. Each data pipeline will include, at a minimum, the configuration of data sources (e.g., data formats) and the rules for inspecting the data (e.g., detecting specific fields and value thresholds). In the end, each data pipeline definition is a JSON document and a set of parameters. They will be stored in a distributed persistence engine.   * Testing Framework: This component will include test definitions and the storage of small datasets for automatic testing. This component will enable users to define automated tests that are executed to validate Data Inspection Jobs before deployment. * Inspection Runtime:    * Runtime Controller: This component will manage the execution of Data Inspection Jobs during runtime. It will define the interface for integrating job execution with external components and will handle the job lifecycle: deployment, upgrade, removal, start, and stop.   * Data Inspection Job: This component represents the runtime execution of a data inspection definition. Each type of data inspection supported will require a Data Inspection Job definition. One instance of this component will be created for each Data Inspection Job needed at runtime, even for the same definition. The creation of these instances will be managed by the Runtime Controller. The Data Inspection Job includes the definition of an SDK and interfaces that facilitate the extension of capabilities, such as supporting additional data formats.   * Job Logger: This component logs all relevant information about each job execution. Based on INDRA infrastructure, it will require minimal development to adapt to changes in other module components.   * Job States Storage: This component stores the states of job executions throughout their lifecycle, enabling job resumption in the event of failures during execution. Based on INDRA infrastructure, it will require minimal development. * Monitoring and Historic Data: This component will store historic metrics gathered by the analysis jobs. This set of elements will be based on the open-source stack of monitoring tools Grafana.   * Metrics/Alert Collector: This component will collect all the data from the Analysis Jobs.   * Alert Manager: It collects alerts, manages them by categorizing and prioritizing, and allows for the configuration of notification channels such as email and Teams. It also provides a centralized interface for tracking and managing active alerts.   * Inspector Dashboard: It allows the visual analysis of the data inspected. * Model Deployed: If a complex analysis is required for data inspection, the Data Inspection Jobs will have the capability to use data models deployed with the T4.2 Data Model Development Toolkit Module. For example, a field value could be used as input in a prediction model, and based on the result, a decision can be made whether an alert should be generated. * Data Share Controller: The Data Interception part of the Data Share Controller connects data in the data pipeline with the Data Inspection. If the Data Share Controller is not present, any other software that implements the Data Inspection API can provide the data. The Data Inspection will trigger notifications to the Data Share Controller Module (and other modules or components, if required) based on rules defined by the participants. For example, \u201cstop data sharing\u201d. * Other DS2 Modules / Other Participant\u2019s Software: The Module can notify any software that needs to receive these notifications, provided that the software is compatible with the notification mechanisms implemented by DINS. This capability facilitates the integration of systems that can react to data shared between participants, such as the Data Share Controller.</p>"},{"location":"modules/DINS/#screenshots","title":"Screenshots","text":""},{"location":"modules/DINS/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Indra Open Source Apache 2.0"},{"location":"modules/DINS/#top-features","title":"Top Features","text":"<ul> <li>Connect with a large of data origins to obtain data.</li> <li>Definition of rules to analyze data transfer based on data values, data format, etc.</li> <li>Real time monitoring of the processes.</li> <li>Process template management.</li> <li>Integrated with EDC Connector.</li> <li>Capability to perform many data transformations: data format, enrich data with other sources, anonymization, masking, etc.</li> </ul>"},{"location":"modules/DINS/#how-to-install","title":"How To Install","text":"<p>TBC</p>"},{"location":"modules/DINS/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/DINS/#software","title":"Software","text":"<p>TBC</p>"},{"location":"modules/DINS/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/DINS/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/DINS/#how-to-use","title":"How To Use","text":"<p>TBC</p>"},{"location":"modules/DINS/#other-information","title":"Other Information","text":"<p>TBC</p>"},{"location":"modules/DINS/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/DINS/#additional-links","title":"Additional Links","text":"<p>Add in here relevant links for your module. In this section we will also add the link to the module video.</p> <p>Documentation https://dataflow-docs.onesaitplatform.com/</p>"},{"location":"modules/DMK/","title":"Data Market Place (DMK)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/dmk_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/33"},{"location":"modules/DMK/#general-description","title":"General Description","text":"<p>The Data Marketplace Module will provide a marketplace for data and data models. It will allow the registration of data from a catalog, record all transactions, and communicate transactions to any external system if required (e.g. Data Rights Management Module, Clearing House). Data will not be stored in the Data Marketplace Module. It will the capability to support not only datasets but also algorithms.</p> <p>The Data Marketplace Module allows the registration of data (datasets and APIs) and algorithms as products to be sold. The Data Marketplace will include an interface with a Catalog Module to facilitate Providers to register their products in the Data Marketplace. The Data Marketplace will store the metadata required to perform and control transactions but not the data themselves. Additionally, it will support the registration of data products not in the DS2 Catalogue as algorithms. Consumers will be able to browse available products and purchase those of interest. The Data Marketplace Module will register all transactions and allow external systems, such as the Policy Enforcement Module, to validate if a product has been purchased. For example, it can verify a data purchase before allowing data sharing. It will interface with the Portal Module marketplace to provide registration and payment functionality.</p>"},{"location":"modules/DMK/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/DMK/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li>Marketplace:</li> <li>Data Marketplace UI: This component provides the user interface where data suppliers and data shoppers can access the functionalities of the data marketplace.</li> <li>Product Registration: This feature allows suppliers to register data products for sale, including datasets, API access, and algorithms. It will include an interface for incorporating products already available in DS2 product catalogues to prevent duplication and ensure synchronization in product definitions.</li> <li>Product Repository: This component stores and provides the required information for each product offered. </li> <li>Order Management System: This subcomponent manages orders placed by providers, defines the purchasing process, records any changes in order status, and coordinates all components involved in the purchase. It manages the full life cycle of the orders. This component ensures that all steps in the purchase process are executed correctly and fulfil the required conditions.</li> <li>Order Repository: This subcomponent stores orders and their statuses. It is primarily used by the Order Management System to store and retrieve the state of orders. It also provides historical information about orders.</li> <li>Payment System: This subcomponent provides payment capabilities to the Order Management System, primarily acting as an intermediary with the payment system implemented in the Portal Module.</li> <li>Transaction Manager and Repository: The Transaction Manager will register actions in the Data Rights Management Module and maintain an additional transaction repository for detailed information.</li> <li>DS2 Portal: The DS2 Portal Module will be used to obtain and validate the identities of providers when they supply data assets to the Data Marketplace and consumers when they purchase data assets. It is also responsible of payment capabilities.</li> <li>Data Rights Management (DRM): The Data Marketplace may log all necessary information in the DRM for accountability in data provision and purchases.</li> <li>DS2 Catalog: The Data Marketplace can offer the datasets present in the DS2 Catalog Module to describe data offerings. </li> <li>Policy Agreement and Enforcement: The Data Marketplace will be queried by the Policy Enforcement module to obtain additional information if a policy requires a purchase to have been made to obtain data.</li> </ul>"},{"location":"modules/DMK/#screenshots","title":"Screenshots","text":""},{"location":"modules/DMK/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Indra Open Source Apache 2.0"},{"location":"modules/DMK/#top-features","title":"Top Features","text":"<ul> <li>DMK allows DS2 participants to create data offers from their data products in the DS2 catalog.</li> <li>Users can search for offers published by other users.</li> <li>Users can purchase access to both data and algorithms.</li> <li>Algorithms are provided as Docker images with APIs for execution. These are built by the MDT module.</li> <li>The Data Marketplace integrates with the PAE module to enable data providers to define policies that require a purchase before access.</li> <li>DMK stores all purchases in the DRM to support transaction auditing.</li> </ul>"},{"location":"modules/DMK/#how-to-install","title":"How To Install","text":"<p>It is currently under development and cannot be installed yet. The first installable version will be available by M24.</p>"},{"location":"modules/DMK/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/DMK/#software","title":"Software","text":"<p>TBC</p>"},{"location":"modules/DMK/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/DMK/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/DMK/#how-to-use","title":"How To Use","text":"<p>TBC</p>"},{"location":"modules/DMK/#other-information","title":"Other Information","text":"<p>TBC</p>"},{"location":"modules/DMK/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/DMK/#additional-links","title":"Additional Links","text":"<p>TBC</p>"},{"location":"modules/DRM/","title":"DRM","text":"<p>Use the table below as an example and replace with the links to the module's DS2 GitHub repository and GitHub project.  </p> Project Links Software GitHub Repository https://github.com/ds2-eu/drm.git Progress GitHub Project https://github.com/ds2-eu/DRM/issues"},{"location":"modules/DRM/#general-description","title":"General Description","text":"<p>Purpose: To enhance the management and security of digital asset transactions through a robust blockchain-based Data Rights Management (DRM) system. It is designed to perform critical functions, including the notarization, tracking, and validation of all data rights transactions both within individual Dataspaces and across multiple participating Dataspaces </p> <p>Description: The Blockchain-based Data Rights Management module (based on the IDSA clearing house module specifications) will be a two-fold solution that will provide both inter (between Data Spaces) and intra (within Data Spaces) connectivity for data-sharing between Data providers and Data consumers of the participating Dataspaces.</p>"},{"location":"modules/DRM/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/DRM/#component-definition","title":"Component Definition","text":"<p>These modules have the following subcomponent and other functions: \u2022   DRM UI: This component is a dashboard that allows users to view logs and transactions occurring between or across dataspaces. Users can easily access detailed records of their interactions with other partners. This dashboard interacts with the DRM UI Backend subcomponent, which performs the necessary transformations of data extracted from the ledger extractor to be appropriately displayed on the dashboard and supports all necessary CRUD operations.</p> <p>\u2022   DRM UI Backend: This subcomponent retrieves log information extracted from the ledger and performs additional transformations to make the data viewable on the dashboard. It interacts with the Ledger Extractor to access the requested data and communicates with the DRM UI to display the transformed data to users.</p> <p>\u2022   Ledger Extractor: This subcomponent securely extracts data from the ledger, converts it into a readable format, forwards it to other services that require access to ledger information. It securely interacts with the DRM Blockchain Manager to retrieve the necessary logs that are stored to the Blockchain\u2019s ledger, DRM UI Backend to provide information that need to be viewed in UI and with Policy Agreement and Enforcement through API to provide information that is needed for the app.</p> <p>\u2022   Connector monitoring: This subcomponent is designed to retrieve logs from the connector and forward them to the blockchain for secure storage on the ledger. It interacts with the DRM Blockchain Manager to ensure that these logs are effectively recorded in the ledger. It is compatible with EDC connector and IDSA connector. This component is optional, allowing participants to use it solely for DRM to log transactions through supported APIs or restrict its use to DS2 tools if preferred. </p> <p>\u2022   DRM Blockchain Core: This subcomponent is the core functionality of the blockchain incorporating essential components to initiate the blockchain network. It provides the functionality of storing immutable, verify information and encrypt information to the ledger. It interacts directly with the ledger to store information securely and works in conjunction with the Blockchain Manager to store additional logs provided by other DS2 modules.</p> <p>\u2022   DRM Blockchain Manager: The subcomponent is responsible for fetching from the blockchain, transforming incoming data logs into the correct schema so they can be stored on the blockchain, and ensuring they are placed correctly within the blockchain's ledger. Furthermore, it interacts with the DS2 Policy Agreement and Enforcement Module via API to access and provide logs essential for its operations. Additionally, it interacts with other DS2 modules, that interact with data, via API to store logs about data process. </p>"},{"location":"modules/DRM/#screenshots","title":"Screenshots","text":""},{"location":"modules/DRM/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ATC Open Source Apache 2.0"},{"location":"modules/DRM/#top-features","title":"Top Features","text":"<ol> <li> <p>End-to-End Data Accountability: Provides full traceability and transparency of data usage events, ensuring organizations can demonstrate compliance with data responsibility and sharing agreements.</p> </li> <li> <p>Trusted Log Verification: Enables organizations to verify and audit logs in a tamper-evident way, fostering trust between data providers, consumers, and intermediaries.</p> </li> <li> <p>Cross-Organizational Visibility: Offers a unified interface for viewing data-related events across systems and partners, improving governance in federated or multi-stakeholder environments.</p> </li> <li> <p>Plug-and-Play Integration: Designed to integrate with existing DS2 components and Eclipse Dataspace Connectors with minimal effort, reducing the barrier to adoption.</p> </li> <li> <p>Configurable Policy Monitoring: Collects and displays policy-related events (e.g., usage constraints, violations), supporting proactive governance and dynamic enforcement of data policies.</p> </li> <li> <p>User-Centric Access to Ledger Insights: Through a visual interface, users can easily access and interpret ledger data without needing technical knowledge, democratizing access to compliance insights.</p> </li> <li> <p>Modular and Scalable Architecture: Built with modular sub-components, allowing organizations to deploy the module in stages or scale it based on operational needs.</p> </li> <li> <p>Designed for Interoperability in Data Spaces: Addresses real-world challenges of interoperability and trust in data spaces by aligning with emerging data sovereignty and governance requirements.</p> </li> <li> <p>Enhances Digital Trust for Data Sharing: By proving that data usage is being tracked and governed, the DRM module helps build confidence and reduce friction in data sharing collaborations.</p> </li> </ol>"},{"location":"modules/DRM/#how-to-install","title":"How To Install","text":""},{"location":"modules/DRM/#requirements","title":"Requirements","text":"<p>CPU: 8 vCPUs</p> <p>RAM: 16 GB</p> <p>Storage: 100 GB SSD (expandable depending on ledger size and log retention)</p> <p>OS: Ubuntu 22.04 LTS or equivalent</p> <p>Docker: Docker Engine \u2265 20.x, Docker Compose \u2265 2.x</p>"},{"location":"modules/DRM/#software","title":"Software","text":"<ol> <li>Hyperledger Fabric</li> <li>Docker</li> <li>Docker Compose</li> <li>Node.js</li> <li>Express.js</li> <li>CouchDB</li> <li>Swagger / OpenAPI</li> <li>Postman (Optional, used during development)</li> <li>cURL</li> <li>Linux OS (e.g. Ubuntu)</li> </ol>"},{"location":"modules/DRM/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ol> <li>Download and Setup Hyperledger Fabric</li> <li>Start the DRM Blockchain Core</li> <li>Run DRM Blockchain Manager</li> <li>Build and run DRM UI Backend as a Docker container</li> <li>Build and run DRM Ledger Extractor as a Docker container</li> <li>Build and run DRM connector monitoring as a Docker container</li> </ol>"},{"location":"modules/DRM/#detailed-steps","title":"Detailed steps","text":""},{"location":"modules/DRM/#download-fabric-samples-docker-images-and-binaries","title":"Download Fabric samples, Docker images, and binaries","text":"<p>For a detailed view of the Hyperledger Fabric installation, make sure you read the official documentation: Hyperledger Fabric documentation</p> <p>To get the install script:</p> <pre><code>curl -sSLO https://raw.githubusercontent.com/hyperledger/fabric/main/scripts/install-fabric.sh &amp;&amp; chmod +x install-fabric.sh\n</code></pre> <p>To pull the Docker containers and clone the samples repo, run one of these commands for example</p> <pre><code>./install-fabric.sh docker samples binary\n</code></pre> <p>or</p> <pre><code>./install-fabric.sh d s b\n</code></pre> <p>If no arguments are supplied, then the arguments docker binary samples are assumed.</p> <p>Make sure the Fabric binaries are added to your system PATH for CLI access.</p> <p>Here are instructions for each subcomponent. Make sure you are in the direcory of each sub-component before running the following commands.</p>"},{"location":"modules/DRM/#drm-blockchain-core","title":"DRM Blockchain Core","text":"<p>To start the network you simple need to run the script:</p>"},{"location":"modules/DRM/#cd-fabric-samplestest-network-deploynetworksh","title":"<pre><code>cd ./fabric-samples/test-network/\n./deploynetwork.sh\n</code></pre>","text":""},{"location":"modules/DRM/#drm-blockchain-manager","title":"DRM Blockchain Manager","text":""},{"location":"modules/DRM/#cd-server-pm2-start-npm-name-drm-blockchain-manager-run-dev-pm2-logs-drm-blockchain-manager-timestamp","title":"<pre><code>cd server\npm2 start npm --name drm-blockchain-manager -- run dev\n\npm2 logs drm-blockchain-manager --timestamp\n</code></pre>","text":""},{"location":"modules/DRM/#drm-ui-backend","title":"DRM UI Backend","text":""},{"location":"modules/DRM/#docker-build-t-drm-ui-backenddevelop-docker-run-p-30003000-drm-ui-backenddevelop-drm-ledger-extractor-bash-docker-build-t-drm-ledger-extractordevelop-docker-run-p-30023002-drm-ledger-extractordevelop","title":"<pre><code>docker build -t drm-ui-backend:develop .\ndocker run -p 3000:3000 drm-ui-backend:develop\n  ```  \n-------------------------------------------------\n#### DRM Ledger Extractor\n```bash\ndocker build -t drm-ledger-extractor:develop .\ndocker run -p 3002:3002 drm-ledger-extractor:develop\n</code></pre>","text":""},{"location":"modules/DRM/#drm-connector-monitoring","title":"DRM connector monitoring","text":"<pre><code>docker build -t drm-connector-monitoring:develop .\ndocker run -p 3001:3001 drm-connector-monitoring:develop\n</code></pre>"},{"location":"modules/DRM/#how-to-use","title":"How To Use","text":"<p>You can import the Postman collection you can find here to interact with the DRM subcomponents. Keep in mind that, due to IT restrictions, not all endpoints are currently available. Future updates will offer the full DRM capabilities.</p>"},{"location":"modules/DRM/#step-1-accessing-drm","title":"Step 1. Accessing DRM:","text":"<p>Click on the Dash button within the PORTAL. This will open the Digital Ledger Dashboard, displaying all logged information clearly.</p>"},{"location":"modules/DRM/#step-2-tracking-and-monitoring-logs","title":"Step 2. Tracking and Monitoring Logs:","text":"<p>Use the dashboard to track, monitor, and filter the following log types: * Component Logs * Connector Logs * Policy Logs </p>"},{"location":"modules/DRM/#step-3-filtering-logged-actions","title":"Step 3. Filtering Logged Actions:","text":"<p>Filter the logs based on a specific timeframe. More filtering options will be available in the future. </p>"},{"location":"modules/DRM/#step-4-exploring-logged-actions","title":"Step 4. Exploring Logged Actions:","text":"<p>To deep dive into details: Select a specific log entry. Review comprehensive payload details and information about each logged action.  </p>"},{"location":"modules/DRM/#step-5-enrolling-users-to-the-blockchain-network","title":"Step 5. Enrolling Users to the Blockchain Network:","text":"<p>Note: This feature is currently a work in progress. When available, follow prompts to enrol new users into the blockchain network seamlessly.</p>"},{"location":"modules/DRM/#step-6-performing-crud-operations","title":"Step 6. Performing CRUD Operations:","text":"<p>Note: This functionality is under development. Once active, you will be able to: Create, Read, Update, and Delete logs efficiently.</p> <p>Stay updated with module enhancements as new features are implemented.</p>"},{"location":"modules/DRM/#other-information","title":"Other Information","text":"<p>No other information at the moment for DRM.</p>"},{"location":"modules/DRM/#openapi-specification","title":"OpenAPI Specification","text":"<p>\u2022   DRM UI: N/A</p> <p>\u2022   DRM UI Backend: open-api/swagger-ui-backend.json</p> <p>\u2022   Ledger Extractor: open-api/swagger-ledger-extractor.json</p> <p>\u2022   Connector monitoring: To Be Done</p> <p>\u2022   DRM Blockchain Core: N/A</p> <p>\u2022   DRM Blockchain Manager: open-api/swagger-blockchain-manager.json</p>"},{"location":"modules/DRM/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/DSHARE/","title":"Datashare (DSHARE)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/dshare.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/3"},{"location":"modules/DSHARE/#general-description","title":"General Description","text":"<p>Purpose: To provide a user-orientated view of control plane information related to a specific exchange of data to monitor its status and to potentially limit or block it. It will access data through a Data Interceptor component which it shares with the DS2 Data Inspection component (DINS) which operates more at the data level. It can be seen as an In-Dataspace enablement module. Its role is especially important in an Inter-DS environment to provide extra monitoring and control of the data exchanges when partners are less known. Description: The DS2 DSHARE is for it to access control data regarding an exchange via the common Data Interceptor component and an API to the used connector - either within IDT or a specific Dataspace one. It will then log and monitor this information and allow it to be presented in user-friendly form. For short duration one-shot type transactions, this is more of an after-the-event easy-viewer. However, for longer duration transactions (e.g., querying records over a period of time) then it allows the user themselves to monitor the flow and perform control-type actions such as limiting or blocking the transaction.</p>"},{"location":"modules/DSHARE/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/DSHARE/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponents and other functions (as detailed in Data Share.pdf, pages 3-4): * Data Share Controller     * Data Share Manager: The primary module that onboards control data (from Connector, Interceptor, Trust environment), stores it in the DSC DB, correlates it, and handles triggers for data actions (limit/block).     * Data Share UI: For configuration, visualization of exchange-related data, and control actions (limiting, blocking).     * DSC DB: Stores component data for use by the UI and Data Share Management. * Connector and API: Primarily the connector within IDT; other local connectors will be explored. APIs (existing or extensions) service data to the Data Share Manager. * Tier 1 Service Stack for Marketplace and deployment and API: Generic DS2 stack implementation (Platform not used by DSHARE). * Tier 2: Data Inspector Manager and API: DINS may trigger DSHARE if anomalies suggest blocking data transfers. * Tier 3: Trust Environment and API: Feeds static agreement information to the Data Share Manager for visualization and control decisions. * Data Share Interceptor and API     * Interceptor: Intercepts data/query streams between IDT/Connector and participant's Business Application/Datastores. Interfaces with DSHARE (DSC) and DINS. Capable of receiving block/limit commands. Research ongoing for interception techniques (man-in-the-middle vs. duplicator).     * Interceptor UI: For configuring the Interceptor (I/O). * Participant DB/Application: Represents business applications feeding data to/receiving queries from the connector.</p>"},{"location":"modules/DSHARE/#screenshots","title":"Screenshots","text":""},{"location":"modules/DSHARE/#commercial-information","title":"Commercial Information","text":"<p>Table with the organisation, license nature (Open Source, Commercial ... ) and the license. Replace with the values of your module.</p> Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/DSHARE/#top-features","title":"Top Features","text":"<ul> <li>Consumer analytics: Comprehensive Data Exchange Monitoring, tracking Real-time of data exchanges between a provider and a consumer. The provider of the app can:<ul> <li>Know active contracts of the consumer</li> <li>Know how much data has been exchanged associated to a given contract</li> <li>Granular Control: Ability to monitor, limit, or block data transfers based on defined policies or user intervention.</li> <li>Advanced Analytics: Features weekly data charts, consumption pattern monitoring, and a consumer ranking system.</li> </ul> </li> <li>Assets analytics: DSHARE provides a cockpit to identify most used assets offered by the provider, weekly and monthly consumption trends helpful for identifying inner problems</li> <li>Alerting System: Notifies users or administrators about approaching or exceeded transfer limits. Alerts are also raisen when consumers have problems accessing an asset (insufficient credentials, etc)</li> </ul>"},{"location":"modules/DSHARE/#how-to-install","title":"How To Install","text":"<p>The module is installed as part of the IDT.</p>"},{"location":"modules/DSHARE/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/DSHARE/#software","title":"Software","text":"<ul> <li>Eclipse Dataspace Connector (EDC) - Specify version if applicable.</li> <li>PostgreSQL Database - Specify version if applicable.</li> <li>Java Development Kit (JDK) - Specify version.</li> <li>Apache Maven - Specify version.</li> </ul>"},{"location":"modules/DSHARE/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/DSHARE/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/DSHARE/#how-to-use","title":"How To Use","text":"<p>Accessing the DSHARE Dashboard: * Navigate to the DSHARE UI URL (e.g., http://:/dshare-ui).  * Analyse consumer data usage: * View real-time data on active transfers. * Analyze weekly data charts for usage trends.  * Data Assets Analytics:  * Alerts Manager:"},{"location":"modules/DSHARE/#other-information","title":"Other Information","text":"<p>No other information at the moment for IDM</p>"},{"location":"modules/DSHARE/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/DSHARE/#additional-links","title":"Additional Links","text":"<p>TBC</p>"},{"location":"modules/DVM/","title":"Data Visualisation Module (DVM)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/dvm.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/56"},{"location":"modules/DVM/#general-description","title":"General Description","text":"<p>NOTE: DVM is a new module and pending an Amendment, hence this document is preliminary and will be subject to change once development starts.</p> <p>Purpose:  DVM provides visualisation functionalities in order to enhance the  presentation and communication capabilities and  facilitate a better understanding of raw data by means of graphical  analysis.</p>"},{"location":"modules/DVM/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p>"},{"location":"modules/DVM/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li>Core (Designtime)</li> <li> <p>User interface to define dashboards, configure data sources for widgets, access control, etc.</p> </li> <li> <p>Core (Runtime)</p> </li> <li>User interface for showing dashboards.</li> <li>Backend for data ingestion.</li> </ul> <p>External Components Used * Data Source   * Data may be streamed from use case partners or developers.   * Module E2C can be configured to stream data to DVM.   * Module DDT can be configured to stream data to DVM.</p> <ul> <li>Information Consumer</li> <li>Data providers</li> <li>Data consumers</li> <li>Module developers</li> </ul> <p>External Interaction   * User:     DVM provides dashboarding functionalities to it's users. Users can     create dashboards, configure widgets, manage data ingestion and view      data using the created dashboards.</p>"},{"location":"modules/DVM/#screenshots","title":"Screenshots","text":""},{"location":"modules/DVM/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License bluebridgesolutions UG (pending Amendment) Open Source Apache 2.0 (pending)"},{"location":"modules/DVM/#top-features","title":"Top Features","text":"<p>1. Data visualisation:  Visualisation of data for data producers and consumers.</p> <p>2. Flexible configuration of dashboards: Adding of new charts for time series data and KPIs by configuration of widgets.</p> <p>3. Enhanced visual analytics: Visualisation simplifies the analysis of data, trends and outlier detection for humans.</p> <p>4. Visualisation of processing results: Modules like DDT produce data such as smoothing of sensor data. DVM is an easy way of  visualizing such results.</p>"},{"location":"modules/DVM/#how-to-install","title":"How To Install","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#requirements","title":"Requirements","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#software","title":"Software","text":"<p>N/A</p>"},{"location":"modules/DVM/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#detailed-steps","title":"Detailed steps","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#how-to-use","title":"How To Use","text":"<p>To Be Done</p>"},{"location":"modules/DVM/#other-information","title":"Other Information","text":"<p>No other information at the moment for DVM.</p>"},{"location":"modules/DVM/#openapi-specification","title":"OpenAPI Specification","text":"<p>N/A</p>"},{"location":"modules/DVM/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/E2C/","title":"Edge to Cloud Connector (E2C)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/e2c.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/24"},{"location":"modules/E2C/#general-description","title":"General Description","text":"<p>Purpose: Dataspaces allow for data to be shared between data providers and data consumers. This includes data coming from sensors and devices at a high rate. This module is used to establish a secure edge-to-cloud connectivity for data providers to cloud-based IoT platforms like Azure IoT or Cumulocity IoT or AWS IoT via a MQTT bridge. The data providers will decide which data to share and with whom. In addition, the data quality can be monitored as well.</p> <p>Description: The DS2 Edge-to-Cloud Connector module (DS2 E2C) will use Software AG\u2019s open-source product thin-edge.io as background. The underlying communication relies on MQTT, a standards-based messaging protocol used for machine-to-machine communication. The data is locally collected on the edge device, mainly IoT-based but not exclusively, and published to the MQTT broker supplied by this module. Depending on the specific cloud platform used by the data provider or data consumer, appropriate rules will get applied to map the generic format to the specific cloud format. The data will constantly be monitored for loss of data quality which will be reported. Using the MQTT-bridge functionality the data and quality information gets mapped to the chosen cloud platform. The communication is encrypted via SSL/TSL using local certificates.</p>"},{"location":"modules/E2C/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/E2C/#component-definition","title":"Component Definition","text":"<p>In E2C, data is collected via a MQTT broker and can be sent to different cloud environments such as Cumulocity, Azure or AWS via adapters built on top of the open-source software thinedge-io. Thin-edge.io is supporting any Linux distribution, in most cases even with the appropriate package manager. In DS2, thin-edge.io will mainly be used to connect on-premises edge devices to cloud environments of the Data Providers.</p> <p>This module has the following subcomponent and other functions:</p> <p>Edge-to-Cloud Connector Module \u2013 Core * MQTT Broker   * This is the central message bus of the module, based on the open-source Mosquitto implementation. * Data Converter   * It reads data, which was published under a certain broker topic, and transforms it to the specific format for the configured cloud platform   * The data converter can use a pipeline of custom routines for specific tasks before ...   * The transformed data is re-published to the MQTT broker under a different topic * Data Quality Module   * The data read by the Data Converter component is constantly analysed by the Analyze Data component of this module to check if the data quality goals are met. The information is collected and sent back to the MQTT broker via the Report Data Quality component. * Other Custom Routines   * Users can write their own routines to perform tasks on their data published to the MQTT broker before it is sent to the cloud platform via the cloud connector. This could be e.g. calculate moving averages, many examples are available in the thin-edge.io plugins directory. * Cloud Connector   * Is a message bus where data can be published under a topic and data consumer can subscribe to these topics</p> <p>External Components Used * Data Source   * The data provider configures and selects where the data to be offered and shared comes from, most likely an edge device * Tier 1 Service Stack for Marketplace and Development   * The module uses the marketplace especially for the configuration of the desired cloud adapter * Tier 3 Inter-Dataspace Sharing   * The module uses the Discover and Trust components for the configuration of the desired cloud adapter  * Cumulocity, Azure, AWS, Custom   * These components represent the possible cloud adapters that can be configured. Besides connecting to the commercial clouds Cumulocity, Azure and AWS, one can write a custom adapter for any IoT platform having a MQTT connector, e.g. the open-source ThingsBoards platform.</p>"},{"location":"modules/E2C/#screenshots","title":"Screenshots","text":""},{"location":"modules/E2C/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Software GmbH Open Source Apache 2.0 (pending)"},{"location":"modules/E2C/#top-features","title":"Top Features","text":"<p>1. Data streaming: Upload relevant data to be shared in dataspaces from devices or existing APIs.</p> <p>2. Cloud platform agnostic: E2C allows to stream data to any cloud platform by it's extensible connectivity feature.</p> <p>3. No vendor lock-in: Most platforms provide dedicated software to connect devices and data sources - by using those, migrating to another platform is a tedious process. In E2C, switching to another supported platform can be as easy as UI-led configuration.</p> <p>4. OpenAPI specification: Import your existing OpenAPI specs to define new data streams from existing APIs.</p> <p>5. Query Builder: Build queries and filters in the user interface to select only the data you need on an attribute level.</p> <p>6. Time windows: Define time windows for data streaming.</p> <p>7. Data mapping: Map data from source to target data models.</p> <p>8. Configure once: Once the streaming services are configured, they can be executed on demand.</p>"},{"location":"modules/E2C/#how-to-install","title":"How To Install","text":""},{"location":"modules/E2C/#requirements","title":"Requirements","text":"<p>N/A</p>"},{"location":"modules/E2C/#software","title":"Software","text":"<ul> <li>Docker runtime</li> <li>Python3, pip</li> <li>Git</li> </ul>"},{"location":"modules/E2C/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ol> <li>Clone Git repository</li> <li>Set cloud platform user credentials</li> <li>Build &amp; run E2C streamer container</li> <li>Execute E2C streamer</li> <li>Configure E2C streamer using E2C UI</li> </ol>"},{"location":"modules/E2C/#detailed-steps","title":"Detailed steps","text":""},{"location":"modules/E2C/#run-container","title":"Run container","text":"<p>Clone E2C Git repository. <code>docker-compose.yaml</code> runs two containers. One contains an MQTT-Broker while the other contains the E2C streamer component. Both containers have access to the 'thin-edge' environment and can pub/sub to the broker.</p> <p>Prior to starting the infrastructure, credentials to target cloud platforms have to be set in the <code>docker-compose.yaml</code> file. Once the credentials have been set, the infrastructure can be started by executing <pre><code>docker compose up --build\n</code></pre></p>"},{"location":"modules/E2C/#running-e2c-stream-component","title":"Running E2C stream component","text":"<p>Currently, E2C stream component is to be started manually.</p> <p>To start the E2C streamer, switch into the <code>tedge-container</code> by running:</p> <pre><code>docker exec -it tedge-container bash\n</code></pre> <p>Once inside the container, the E2C streamer can be started using the following command:</p> <pre><code>/venv/bin/python3 /app/main.py --config config.json\n</code></pre> <p>The parameter <code>--config config.json</code> is optional and allows to pass an existing configuration file, while the streamer can be configured entirely using the E2C UI.</p> <p>Once the E2C streamer is running, it's configuration can be retrieved and updated by E2C UI. To start the UI, clone the UI Git repository and execute <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>E2C streamer is now ready to stream data to the configured cloud platform.</p>"},{"location":"modules/E2C/#how-to-use","title":"How To Use","text":"<p>To display the configuration UI, run <pre><code>python main.py\n</code></pre></p>"},{"location":"modules/E2C/#other-information","title":"Other Information","text":"<p>No other information at the moment for E2C.</p>"},{"location":"modules/E2C/#openapi-specification","title":"OpenAPI Specification","text":"<p>N/A</p>"},{"location":"modules/E2C/#additional-links","title":"Additional Links","text":"<p>N/A</p>"},{"location":"modules/IDM/","title":"Identity Management Module (IDM)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/idm.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/3"},{"location":"modules/IDM/#general-description","title":"General Description","text":"<p>Purpose: The DS2 Identity Module (IDM) is a Foundation module that provides a practical framework for creating, managing, and validating participant and module identities for inter-Dataspace activities. It leverages Verifiable Credentials (VCs) and integrates with technologies like the EDC IdentityHub to ensure secure and trustworthy identity management. The IDM aims to reuse existing identities from different companies and provide robust mechanisms to verify their membership and participation in various dataspaces. Description: The IDM facilitates the secure interaction between dataspaces by managing identities and access rights. Key development efforts have focused on: Verifiable Credentials (VCs): Core to the IDM, VCs allow companies to prove their identity and attributes in a secure and verifiable manner. Wallet Visualisation: Functionality enabling companies to view and manage their VCs. Credential Issuer: DS2 has developed its own credential issuer capable of generating signed VCs for companies, based on dataspace agreements. IDM Portal: A comprehensive web interface (backend and frontend) for managing dataspaces, collaboration agreements, and company registrations within DS2. EDC IdentityHub Integration: Successful integration allows for the hosting and management of VCs. The module supports the registration of dataspaces, the establishment and management of collaboration agreements between them, and the registration of companies as DS2 members, all underpinned by a system of verifiable digital credentials.</p>"},{"location":"modules/IDM/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/IDM/#component-definition","title":"Component Definition","text":"<p>the IDM comprises the following key components: IDM Portal: * Backend: Manages the business logic for dataspace registration, agreement workflows, company registration, and interactions with the credential issuer and wallet functionalities. * Frontend (UIs): Provides user interfaces for:     * Registration of dataspaces in DS2.     * Requesting collaboration and forming agreements between dataspaces.     * Searching for registered dataspaces.     * Accepting or rejecting collaboration agreements.     * Visualizing dataspace policies.     * Registering companies as members of DS2. * DS2 Credentials Issuer:     * An implemented service capable of generating cryptographically signed Verifiable Credentials.     * Issues credentials to companies based on established dataspace agreements and their DS2 membership. * Wallet Visualisation Functionality:     * A user-facing component allowing companies to view and manage their issued Verifiable Credentials.     * Integrated with the EDC IdentityHub for hosting credentials. * EDC IdentityHub Integration Layer:     * Manages the interaction with the EDC IdentityHub, enabling the storage and retrieval of Verifiable Credentials. * Core Identity Logic:     * Underlying mechanisms for verifying membership of companies in their respective dataspaces and managing the lifecycle of identities and credentials within the DS2 ecosystem.</p>"},{"location":"modules/IDM/#screenshots","title":"Screenshots","text":""},{"location":"modules/IDM/#commercial-information","title":"Commercial Information","text":"<p>Table with the organisation, license nature (Open Source, Commercial ... ) and the license. Replace with the values of your module.</p> Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/IDM/#top-features","title":"Top Features","text":"<ul> <li>Verifiable Credential Management: Core functionality for issuing, hosting (via EDC IdentityHub), and visualizing VCs for companies.</li> <li>DS2 Native Credential Issuer: Custom-built service to generate signed credentials based on dataspace agreements.</li> <li>Comprehensive IDM Portal:<ul> <li>Dataspace registration and discovery.</li> <li>Inter-dataspace collaboration agreement management (requests, acceptance/rejection).</li> <li>Visualization of dataspace policies.</li> <li>Company registration for DS2 membership.</li> </ul> </li> <li>EDC IdentityHub Integration: Successfully achieved for robust VC hosting.</li> <li>Wallet Functionality: Allows companies to manage and present their digital credentials.</li> <li>Decentralized Identity Principles: Aligns with modern approaches to identity management, enhancing security and user control.</li> <li>Membership Verification: Mechanisms to verify company membership within specific dataspaces.</li> <li>Secure Inter-Dataspace Communication: Facilitates trust by ensuring participants are authenticated and authorized.</li> </ul>"},{"location":"modules/IDM/#how-to-install","title":"How To Install","text":"<p>The module is installed as part of the IDT.</p>"},{"location":"modules/IDM/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/IDM/#software","title":"Software","text":"<p>TBC</p>"},{"location":"modules/IDM/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/IDM/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/IDM/#how-to-use","title":"How To Use","text":"<p>The IDM is primarily interacted with via the global portal. After the user logs in it provides two main functionality: Management of Dataspaces and Company Verifiable Credentials: * Dataspace Authorities:     * Use the IDM Portal to register their dataspace within the DS2 ecosystem.          * Search for other registered dataspaces to explore potential collaborations.     * Initiate collaboration requests with other dataspaces.          * Review and accept/reject incoming collaboration requests.          * View policies associated with registered dataspaces.      * Companies:     * Register their organization as a member of DS2 via the IDM Portal.          * Once approved and relevant dataspace agreements are in place, the DS2 Credentials Issuer will generate Verifiable Credentials for the company.     * Access the Wallet Visualisation Functionality to view and manage their issued VCs. These VCs can then be used to prove their identity and membership in inter-dataspace interactions.      *System Administrators:     * Manage the overall configuration and operation of the IDM components.</p>"},{"location":"modules/IDM/#other-information","title":"Other Information","text":"<p>No other information at the moment for IDM</p>"},{"location":"modules/IDM/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/IDM/#additional-links","title":"Additional Links","text":"<p>TBC</p>"},{"location":"modules/IDT/","title":"IDT - IDT Broker","text":"<p>Powered by</p> <p></p> Project Links Software GitHub Repository https://github.com/ds2-eu/idt.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/3"},{"location":"modules/IDT/#general-description","title":"General Description","text":"<p>IDT is the core enabler of DS2 who purpose is to be deployed in front of participants data source/spaces and network connected to any other IDT-enabled data source. As such its aim is to run all DS2 modules, including the DS2 Connector, the core module for Inter-Dataspace communication and data transfer, and the Containerisation module for DS2 module deployment. The IDT contains the core Kubernetes runtime to run all containerised modules and a series of additional open-source software for module management.</p> <p>The IDT contains the Kubernetes runtime that is the core service to run all modules in a containerised way. Those modules descriptors are uploaded to the DS2 Portal Marketplace and then, using the IDT Kubernetes UI, are deployed to the Kubernetes runtime. The Containerisation module kicks-in and then converts those module descriptors to full module charts effectively deploying the DS2 modules. The Kubernetes UI, alongside the Management and Monitoring Controller, will be used to manage and monitor all DS modules running on a participants IDT. Additional services will also be run as part of the IDT such as certificate management, ingress network traffic management using traditional ingress controllers with ingress resources to expose modules and eventually transitioning to service mesh and gateway API, storage management and possibly other useful open-source tools. The other key component of the IDT is the DS2 Connector used for Dataspace like communications following current IDSA and Gaia-X standards. An additional IDT UI will be provided for module navigation and Connector management.</p>"},{"location":"modules/IDT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/IDT/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li> <p>Kubernetes (Module Runtime): Kubernetes is the leading technology in container orchestration and the choice and key component of the IDT for deployment and integration of the DS2 modules. This is the core IDT subcomponent that runs and orchestrates the DS2 modules and all the other IDT subcomponents, as containers. This is Open-Source software and the current distribution being used is K3s, a lightweight version of Kubernetes easy to install, half the memory, all in a binary, in less than 100 MB among other enhancements. One of the main advantages is the flexibility of installation, since it can be deployed at a participant edge, onPremise, InCloud, etc.</p> </li> <li> <p>Kubernetes UI: The Kubernetes UI is open-source software based on Rancher that allows to deploy and manage Kubernetes in a more user-friendly way both onPremise and InCloud. The Kubernetes UI will provide the management interface for platform administrators and the module deployment interface for participants running the IDT. This interface will be used to deploy the module ChartDescriptors,  configuration files that describe how a module runs on Kubernetes, from the DS2 Portal Marketplace in the IDT. The Containerisation module will then transform the descriptors into full Helm Charts and deploy them to the Kubernetes subcomponent. The Kubernetes UI will also provide the monitoring interface to the IDT Kubernetes subcomponent and the DS2 modules.</p> </li> <li> <p>Management and Monitoring Controller: This is the main interface from the Kubernetes UI to the Kubernetes subcomponent and also for external integrations. The Management and Monitoring Controller is open-source software based on the Kubernetes and Rancher API and the Rancher agents. It is used as the primary interface to the IDT Kubernetes subcomponent for management and deployment of modules. It will also be used as the primary interface for monitoring which will potentially be integrated with the DRM Blockchain module for traceability. In addition, further research on using other modules for monitoring such as Prometheus-Grafana will be conducted for enhanced monitoring.</p> </li> <li> <p>Ingress \u2013 Gateway: The ingress or gateway resource provides the entry point to the IDT Kubernetes subcomponent via the Ingress controller, thus, the IDT network, for all network traffic from external apps, being an external app, any system external to the IDT. It describes how the DS2 modules are exposed outside of the IDT. Initially the modules will use a Kubernetes Ingress resource to expose the modules but further research will be conducted to examine the use of the Gateway API and Service Mesh technology.</p> </li> <li> <p>Ingress Controller \u2013 Service Mesh: Based on Open Source, the Ingress Controller is the Kubernetes controller dealing with Ingress resources, that is, managing the entry point to the IDT and how the DS2 modules are exposed outside of IDT. Further research is expected for replacement of the Ingress Controller with Service Mesh technology and the Kubernetes Gateway API, that adds a transparent layer to provide the IDT with enhanced connectivity, security, control and observability. The use of the Service Mesh could also be a key feature for using more secure communication via mutual TLS protocol (mTLS) in all DS2 communications which provides and additional trust layer. This could also be integrated with the DS2 trust and identity system.</p> </li> <li> <p>Storage Manager: Open-source software to provide the interface between the IDT Kubernetes subcomponent and the physical storage for DS2 stateful modules. This will use Kubernetes native storage technology to allow highly available stateful module deployments in IDT. When data from DS2 modules need to be persisted in a participant backend storage system, the Storage Manager will be used to map current deployment and Kubernetes Persistent Volumes to external storage systems. This is not a storage system or technology for modules\u2026 If DS2 modules need to use storage, the DS2 modules need to provide them by packaging them in their module Chart.</p> </li> <li> <p>CertManager: Based on Open-source software, it provides management of SSL certificates for secure connectivity ie. HTTPS, with verified signed certificates using Let\u2019s Encrypt CertificateAuthority (CA) and configures them for the Ingress or Gateway resource. The CertManager integrates with the Ingress Controller and/or Service Mesh subcomponents and in addition, further research on integration with DS2 Trust system will be explored.</p> </li> <li> <p>DS2 Connector: The DS2 Connector in the IDT is the key element that will allow for DS2 transactions and data exchange, following the IDSA and Gaia-X standards. The Open-source Eclipse EDC Connector (or the Tractus-X extension) will be used to provide interoperability between Dataspaces and secure, trustworthy exchange of data. Following existing Dataspace principles and protocols, the DS2 Connector will use the DS2 Trust system for identity management and will connect to other participants IDT DS2 Connectors in other Dataspaces for data exchange. The Connector will also integrate with the DS2 Catalog or a Dataspace level Metadata Broker for participant and data discovery. </p> </li> <li> <p>Local Identity: This module is optional and it provides local identity, authentication and authorization to access a participant IDT and its modules by the various users types within a company. Based on Open Source Keycloak identity provider software, further research will be done in order to explore the possibility of linking the Local Identity with the DS2 Trust system.</p> </li> <li> <p>Tier 0 Support Service Stack:</p> <ul> <li>DRM and API: For further exploration integration of the monitoring controller with the Blockchain will be considered.</li> </ul> </li> <li> <p>Tier 1 Service Stack for Marketplace and deployment and API: The full stack will be implemented as generically described elsewhere in this document. Exceptions: The IDT uses the DS2 Portal and Marketplace to retrieve the ChartDescriptors of modules and deploy them via the Kubernetes UI. Then the Containerisation module uses the descriptors to deploy the full module Helm Chart. In addition, The DS2 Connector in the IDT integrates with other IDT DS2 Connectors for data exchange.</p> </li> <li> <p>Tier 3 Trust Stack and Catalog and API: The IDT will make use of the relevant parts of the DS2 Trust Stack for certificates in the Ingress Controller \u2013 Service Mesh and identities in the DS2 Connector. The IDT will also connect via the DS2 Connector to the Catalog.</p> </li> <li> <p>External Apps: External Apps refer to any software application external to the IDT and DS2 ecosystem that uses the DS2 Connector in the IDT for any DS2 data transaction. It\u2019s the application that can trigger a data exchange via the Connector, either as a consumer or producer.</p> </li> <li> <p>External Storage Systems: This refers to any external storage system, physical or software defined, that a participant has already in place and where data from the IDT and DS2 ecosystem can be persisted, thus, is mapped via the Storage Manager into the IDT Kubernetes</p> </li> </ul>"},{"location":"modules/IDT/#screenshots","title":"Screenshots","text":""},{"location":"modules/IDT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/IDT/#top-features","title":"Top Features","text":"<ol> <li>Kubernetes Platform: The IDT provides a Kubernetes based platform for ease of integration and deployment of the modules</li> <li>Flexible Installation Process: The IDT provides a user friendly installation process for easy installation to non-experienced users.    In any case, management of the platform itself, will require some expertise. In addition, it supports different installation modes from on-cloud to on-prem and edge.</li> <li>Management Interface: A Rancher based UI is provided for the Kubernetes cluster management. Management of Kubernetes itself will require some expertise. The Management UI  provides the interface and API for module management : deployment, deletion, upgrade</li> <li>Monitoring of Platfom and Apps: Provides the interface and API to monitor the cluster itself and the modules</li> <li>Seamless Integration with Containerisation Deployment: IDT integrates seamlessly with the Containerisation module for ease of module deployment</li> <li>Networking: Provides secure networking and connectivity among the installed apps and to and from outside the cluster</li> <li>Log management: Ability to retrieve module logs for troubleshooting and debugging</li> <li>Native Storage: Provides Kubernetes native storage for stateful applications</li> <li>IDT Portal: Provides a local version of the DS2 Portal as the entry point to the IDT, single sign on and module navigation </li> <li>DS2 Connector: The IDT incorporates the DS2 Connector for DS2 data exchange</li> </ol>"},{"location":"modules/IDT/#how-to-install","title":"How To Install","text":"<p>The IDT installs a pre-packaged enterprise ready Kubernetes cluster along with some extra features for management and deployment.</p>"},{"location":"modules/IDT/#requirements","title":"Requirements","text":"<p>Provision a Linux VM (Ubuntu 18.04 or 20.04) Resources:</p> <p>Kubernetes Node</p> <ul> <li>Minimum: 2 cpu cores, 4 GB RAM and 10 GB disk capacity.</li> <li>Recommended: 4 cpu cores, 8 GB RAM and 50 GB disk capacity.</li> </ul> <p>These numbers may change since a number of IDT components will be deployed, check specific requirements for specific components.</p>"},{"location":"modules/IDT/#software","title":"Software","text":"<p>IDT installs these software utilities and specific tested compatible versions:</p> <ul> <li>Docker</li> <li>K3s (Kubernetes)</li> <li>Helm</li> <li>Cert-manager</li> <li>Rancher</li> <li>Creates a self signed certificate to use by the ingress controller</li> <li>Nginx Ingress Controller</li> <li>Nginx docker (load balancer - optional)</li> <li>DS2 Connector (to do)</li> <li>Core DS2 modules (to do)</li> </ul>"},{"location":"modules/IDT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ol> <li>Clone the repo</li> <li>Deploy IDT<ul> <li>Run the command mini-idt.sh nodeip iface as per instructions. This deploys the Kubernetes platform</li> <li>nodeip: the ip of the node</li> <li>iface: the network interface of the node ip</li> </ul> </li> <li>Access Rancher at https://rancher.$nodeip.ds2.sslip.io where $nodeip uses '-' instead of '.' (The domain can be changed if needed)</li> <li>Register the catalog(s)</li> <li>If http access is needed, run patch_nginx.sh</li> <li>Deploy IDT modules from the Rancher UI (Using Containerisation module at a later stage)<ul> <li>Any script required before deploying any component should be in the idt/modules folder</li> <li>Upon deployment of first module import the self-signed certificate in the trusted CA's store</li> </ul> </li> </ol>"},{"location":"modules/IDT/#detailed-steps","title":"Detailed steps","text":"<ul> <li> <p>Clone the repository <pre><code>git clone https://github.com/ds2-eu/idt.git\n</code></pre></p> </li> <li> <p>Navigate to idt folder <pre><code>cd idt\n</code></pre></p> </li> <li> <p>Run idt <pre><code>./idt.sh ip iface\n</code></pre> where ip is the ip of the vm where k3s will be installed and iface is the network interface of the nodeip for instance ./idt.sh 192.168.50.5 enp0s8</p> </li> </ul> <p>The script will install the software utilities in this order:</p> <ul> <li> <p>First, IDT installs Docker</p> </li> <li> <p>Then, k3s Kubernetes cluster is installed. Once installed, the installation process will wait and check for K3s to be up and running. Helm is installed together with K3s and kubectl.</p> </li> <li> <p>Next cert-manager is deployed in order to provide a ssl certificate for Rancher. Process will wait and check that cert-manager is running.</p> </li> <li> <p>Then Rancher is deployed in the cluster. Process will wait and check that Rancher is running.</p> </li> <li> <p>Next, the nginx ingress controller is deployed. Before this, a self signed ssl certificate is created using certmanager. Notice that the domain that is configured in the certificate, is the one to be used as domain for the modules when deployed to IDT which defaults to *.$nodeip.modules.ds2.sslip.io . This domain can be changed.</p> </li> <li> <p>DS2 IDT is now ready.</p> </li> </ul> <p></p> <ul> <li>Access Rancher by accessing the Rancher url in the browser (https://rancher.$nodeip.ds2.sslip.io)</li> <li>Once in the Rancher UI, the admin password is set</li> <li>Then navigate to the workloads in the system project</li> </ul> <p></p> <ul> <li> <p>The nginx ingress controller is by default set to only accept https connections and redirect to https. In order to use http, run the script patch_nginx.sh which will configure nginx ingress controller to accept http (optional). </p> </li> <li> <p>Now that the cluster is up and running, the Rancher UI can be accessed in order to manage the cluster and install modules. </p> </li> </ul>"},{"location":"modules/IDT/#how-to-use","title":"How To Use","text":"<p>Once the IDT has been installed, the Rancher UI along with the IDT Portal (local Portal) and the core modules (to be done) can be accessed.</p>"},{"location":"modules/IDT/#rancher-ui","title":"Rancher UI","text":"<p>The Rancher UI is the main entry point for Kubernetes cluster management and configuration. </p> <ul> <li> <p>Inspect the cluster and check number of nodes, resources, etc       </p> </li> <li> <p>Create a Module Repository: A user can register a module catalog or repository in order to be able to deploy modules from that module repository. A catalog is just a repository, git or helm, where helm charts are stored. In general, users won't need to create any new repository in the IDT since a default DS2 catalog will be created for the organisation pulling from the organisation repository in DS2 intermediary platform.  If needed, in order to create a new repository, navigate to the cluster, apps, repositories, and click on the \"Add Catalog\" button. Fill in the form with the credentials for a private repository and click \"Create\". The Repository is added to Rancher and the modules will be displayed in the Apps view       </p> </li> <li> <p>Module Deployment: modules can be deployed from the registered Repository, but in general this will be done via the Containerisation module. If needed, navigate to Apps, Charts and the list of available charts (apps) is displayed. Select the chart to be deployed, click on Install, select the Namespace and Name for the instance of the chart and select whether to customize the Helm options before install. If customization is selected, fill in the configuration form and or yaml. Click Next then click Install. The application will be deployed to the platform.       </p> </li> <li> <p>Module logs: In order to review the modules logs, navigate to Apps, Installed Apps, and the list of installed modules is displayed. Select the module to be monitored and the list of Kubernees resources of that module are displayed. Select the Deployment and the Pod is displayed. Click on the three dots on the right and select View Logs. The logs of the module are displayed.        </p> </li> <li> <p>Module deletion: modules can be deleted by navigating to Apps, Installed apps and clicking on the three dots on the right and click on Delete. In general, this will be done via the Containerisation module       </p> </li> </ul>"},{"location":"modules/IDT/#idt-portal","title":"IDT Portal","text":"<p>To Be Done</p>"},{"location":"modules/IDT/#connector-ui","title":"Connector UI","text":"<p>To Be Done</p>"},{"location":"modules/IDT/#other-information","title":"Other Information","text":"<p>No other information at the moment for IDT</p>"},{"location":"modules/IDT/#openapi-specification","title":"OpenAPI Specification","text":""},{"location":"modules/IDT/#additional-links","title":"Additional Links","text":"<p>Video https://youtube.com/idt</p> <p>Kubernetes https://v1-26.docs.kubernetes.io/docs/home/</p> <p>Helm https://helm.sh/</p> <p>K3s https://k3s.io/</p> <p>Rancher https://ranchermanager.docs.rancher.com/v2.7/getting-started/quick-start-guides</p> <p>EDC Connector https://eclipse-edc.github.io/</p> <p>Portal Repository https://github.com/ds2-eu/portal (Private Link for Project Members)</p>"},{"location":"modules/MCL/","title":"Multi-cloud Module (MCL)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/mcl_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/27"},{"location":"modules/MCL/#general-description","title":"General Description","text":"<p>Purpose: The DS2 Multi-cloud module (MCL) enables efficient transfer of discreet data, vast amounts of data, and streaming data between participants of dataspaces from data stores that are distributed across multi-cloud storage infrastructure. MCL includes intelligent data placement and caching at dataspace provider participants with a dataspace consumer participant requesting such data and provide services through use case applications(s). It will also ensure data exchange happens over secure connections using the DS2 Security Module (SEC). </p> <p>Description: When a use case application initiates a request for data through a dataspace consumer participant, the module ensures that the requested data is swiftly and accurately delivered from discovered and relevant provider participant(s). This process involves intelligent data placement by analysing access patterns and data requests (based on specific parameters) from the consumer participant that allows selecting the optimal data caching locations and employing predictive caching strategies to enhance data availability and retrieval speeds. In addition to push/pull style data sharing, this module introduces two novel extensions of the dataspace connector data plane for vast amount and streaming data sharing. Furthermore, it incorporates secure connectivity ensuring that all data exchanges are protected in transit. The module also aligns with the broader DS2 architecture, ensuring interoperability and synergy with various other modules and their sub-components. This enables the module to support a wide range of application scenarios and data exchange requirements.</p>"},{"location":"modules/MCL/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/MCL/#component-definition","title":"Component Definition","text":"<p>When a use case initiates data retrieval request, it queries the data offer discovery service to determine which provider participant(s) can serve the requested data. To support vast amount and streaming data, two novel sub-components to the dataspace connector data plane are introduced. The intelligent data placement and caching sub-component analyses access patterns ensuring that frequently accessed various data stores (stored in multi-cloud infrastructure) is cached closer to the consumer participant. The actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module are already depicted in the above architecture diagram.</p> <p>This module has the following subcomponents and other functions: </p> <ul> <li> <p>Tier 1: Catalogue module: It securely stores a description metadata of the dataspaces and implements an interconnected search and retrieval system for a consumer participant to discover data and then relevant provider participant(s) end points.  </p> </li> <li> <p>Use case app: These are the high-level applications of users such as those in  the use cases of DS2. Such applications can require data from multiple data stores, which are presumed to be distributed to multi-cloud data storage infrastructures. Also, the data sharing happens through multiple dataspace provider participants. Each use case can directly consume the obtained data and/or temporarily store them in a local storage for combining the data arriving in batches (in case of vast data transfer) or processing in future. </p> </li> <li> <p>Temporary data store: A local data storage which are used by the use case apps to store data for (very) short term. </p> </li> <li> <p>Secure connection: This sub-component is responsible for ensuring data sharing takes place over secure connections (e.g., VPN, SSL/TLS) and comes from the DS2 Security Module (SEC). </p> </li> <li> <p>Dataspace connector (data plane): It facilitates secure and efficient transfer of data between participants in the DS2 ecosystem while ensuring compliance with agreed-upon data governance policies and handling data routing. In DS2, the data place supports three types of data sharing \u2013 discrete data, vast amounts of data, and streaming data. </p> </li> <li> <p>Tier 1 Service for marketplace and deployment: The full stack will be implemented as generically described elsewhere in this document. </p> </li> </ul> <p>Dataspace consumer participant: </p> <ul> <li> <p>Consumer Participant data offer discovery service: This sub-component performs two tasks \u2013 (a) Publishes a description of the data offer of each participant to the Tier 1 catalogue module (for a centralised discovery) and (b) Enables a consumer participant within the DS2 ecosystem to discover the data offers for the use case applications. Furthermore, the data offer is published in the form of metadata (e.g., data type such as discrete, vast amount, or streaming data, accessibility conditions). This service exists as a background in DIGI\u2019s cloud-based Paradise platform and will be adopted to store and search metadata of participant data offers. </p> </li> <li> <p>Data retrieval service: This sub-component is triggered by the use case applications which require access to data stored in multi-cloud data stores. The dataspace consumer participant performs a discovery of the available provider participants and then proceeds to the retrieval of data stored from various data stores. This service allows dataspace consumer participants to request and obtain data needed for specific use case applications seamlessly. This service allows both push and pull type of data retrieval, supports multiple data type (such as discrete, vast amount, and streaming), and queries. Additionally, it ensures that data integrity and consistency are maintained throughout the retrieval process, providing reliable data access to user applications. The retrieved data may be directly used in such an application or may be temporarily stored in a data store. Integrity checks (hash verification) are performed especially for vast data transfer done through batches. </p> </li> </ul> <p>Dataspace provider participant: </p> <ul> <li> <p>Provider participant data offer discovery service: It publishes a description of the data offer of each participant to the Tier 1 catalogue module for a centralised discovery by the consumer participant. </p> </li> <li> <p>Intelligent data placement and caching: This aims to optimise data storage and retrieval by strategically placing data across multi-cloud storage locations by employing predictive caching mechanisms. This component analyses access patterns from dataspace consumer participants and predict future data requests (based on specific parameters), ensuring that frequently accessed data is cached closer to the consumer participant. By doing so, it reduces latency and improves data retrieval speeds. The intelligent data placement strategy ensures optimal use of storage resources by distributing data based on access frequency, storage costs, and performance requirements. This sub-component is comprised of: </p> <ul> <li> <p>Cache invalidation and consistency manager: In case of data changes, this sub-component acts to invalidate or update outdated cached entries ensuring that stale data is not transferred. It will implement strategies like time-to-live settings, write-through caching, or cache coherence protocols. </p> </li> <li> <p>Data placement controller: It enforces the decisions made by the data placement engine and handles the actual data movement in storage locations for data caching. </p> </li> <li> <p>Intelligent data placement engine: This sub-component uses intelligent algorithms to determine where data should be cached. The placement decision is taken based on latency, access patterns from the consumer, data size, cost, network bandwidth, and other relevant factors. </p> </li> <li> <p>Data caching: It covers the process of storing copies of frequently accessed data in a location closer to the consumer participant and the actual storage. </p> </li> </ul> </li> </ul> <p>Dataspace connector data plane: In DS2, it supports three types of data transfer: </p> <ul> <li> <p>Push/pull data transfer: Data can be delivered to consumer counterpart through this sub-component either via a push model (where the dataspace sends data automatically at intervals or when triggered) or a pull model (where consumer participant requests data when needed). While this is supported by default in the dataspace connector, it is needed to accomplish discrete or small amounts of data transfer. </p> </li> <li> <p>Vast data extension: Today, the dataspace connector data plane typically supports push-pull style of data transfer. DS2 introduces a novel sub-component called vast data extension to the data plane. It is designed to handle sharing of extremely large datasets stored across distributed data stores. It is composed of: </p> <ul> <li> <p>Data transfer, error checking, retries: Using this, the batch of data is transferred from a provider to a consumer participant. During the transfer, the provider side monitors for any errors or interruptions such as network connection lost. In case of an error detected, the sub-component retries the transfer or attempts to resume from the point of failure. </p> </li> <li> <p>Batch scheduling: This sub-component schedules batch data transfers to run at specific times (e.g., every hour). The schedules are automatically developed based on the use case application\u2019s needs. Scheduling can also occur during off-peak hours  (should the use case need it) to minimise impact of the network and DS2 resources. </p> </li> <li> <p>Data partitioning and compression: Extremely large datasets are split into smaller chunks or batches which are then compressed to save network bandwidth. </p> </li> </ul> </li> <li> <p>Data stream extension: This extension provides robust capabilities for handling real-time data streams within the DS2 architecture for use case(s) that require(s) data streaming. This is a novel sub-component introduced by DS2 for the dataspace connector data plane where data streaking is minimally supported in connectors today if at all. This component enables continuous data flows from various sources, such as IoT devices, sensors, and real-time applications. It supports high-throughput and low-latency data pipeline such as Apache Kafka, ensuring that streaming data is handled efficiently and reliably. This extension is composed of:  </p> <ul> <li> <p>Data transfer: Using this, the stream of data is transferred from the provider to the consumer participant. </p> </li> <li> <p>Stream processing engine: Processes continuous streams of data in (near) real time ensuring uninterrupted data flow, stateful processing capabilities, and ensure the stream is in right format needed by the use case applications. It can also be used for relatively simple data transformations and performing operations like data aggregation (if needed). </p> </li> <li> <p>Stream message broker: The message broker serves as a buffer and pipeline between the streaming data sources and consumer participants. Data sources publish data to message queues or topics that act as the entry point to the streaming pipeline. </p> </li> </ul> </li> <li> <p>Data store: These data stores represent data storage at the participant. Each participant data store may be designed to manage a variety of data types and formats, providing a robust and scalable storage solution. The subcomponent architecture supports multiple data stores leveraging multi-cloud environments, enabling data to be distributed and replicated across different geographic locations, enhancing accessibility and redundancy. </p> </li> <li> <p>Vast data store: Similar to the data store mentioned above, these are specific to storages with vast amounts of data. </p> </li> <li> <p>Data stream source: This refers to data sources that continuously produce data, such as IoT devices, sensors (such as video cameras), and real-time applications (e.g., weather apps). </p> </li> </ul>"},{"location":"modules/MCL/#screenshots","title":"Screenshots","text":"<p>User interface for this module will be developed in RP2.</p>"},{"location":"modules/MCL/#commercial-information","title":"Commercial Information","text":"<p>Table with the organisation, license nature (Open Source, Commercial ... ) and the license. Replace with the values of your module.</p> Organisation (s) License Nature License DIGI Open Source Apache 2.0"},{"location":"modules/MCL/#top-features","title":"Top Features","text":"<ol> <li>Intelligent Data Placement &amp; Predictive Caching: analyzes access patterns from dataspace consumer participants and pre-positions data in optimal storage locations.</li> <li>Supports Three Data Types: discreet, vast amounts, and streaming data.</li> <li>Secure Data Exchange: uses secure data transmission and integrates with the DS2 Security Module (SEC).</li> <li>Vast and Streaming Data Transfer Extensions: implements two new Data Space connector extensions for vast and streaming data.</li> <li>Stream Message Broker Integration: implements a broker acting as a buffer between streaming sources and consumer apps.</li> <li>Temporary and Vast Data Stores: supports both short term storage by use case apps as well as high-volume datasets distributed across multi-cloud environments.</li> <li>Data Offer Discovery Service: enables publishing and discovery of data offers by interacting with the DS2 Tier 1 Catalogue Module.</li> <li>Data Retrieval Service: enables seamless data access for use case apps.</li> <li>Modular, Extensible Architecture: supports cross-dataspace operations and is extensible to third-party systems.</li> </ol>"},{"location":"modules/MCL/#how-to-install","title":"How To Install","text":""},{"location":"modules/MCL/#requirements","title":"Requirements","text":"<p>Provision a Linux VM (Ubuntu 22.04 LTS or later) with 4vCPUs and 8GB RAM. The technical requirements might augment as the MCL module nears completion.</p>"},{"location":"modules/MCL/#software","title":"Software","text":"<p>Docker environment is necessary to run the MCL module.</p>"},{"location":"modules/MCL/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ol> <li>clone the github repo</li> <li>run the docker compose file</li> </ol>"},{"location":"modules/MCL/#detailed-steps","title":"Detailed steps","text":"<ul> <li> <p>Clone the repository <pre><code>git clone https://github.com/ds2-eu/mcl_module.git\n</code></pre></p> </li> <li> <p>Run the docker compose file <pre><code>sudo docker compose up\n</code></pre></p> </li> </ul>"},{"location":"modules/MCL/#how-to-use","title":"How To Use","text":"<p>To be done.</p>"},{"location":"modules/MCL/#other-information","title":"Other Information","text":"<p>No other information at the moment for MCL.</p>"},{"location":"modules/MCL/#openapi-specification","title":"OpenAPI Specification","text":"<p>To be done.</p>"},{"location":"modules/MCL/#additional-links","title":"Additional Links","text":"<p>To be done.</p>"},{"location":"modules/MDT/","title":"Model Development Toolkit (MDT)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/mdt_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/32"},{"location":"modules/MDT/#general-description","title":"General Description","text":"<p>The main purpose of the DS2 Model Development Toolkit Module (MDT) is to provide a set of tools to allow the users to develop Algorithms based on the CRISP-DM standard to assist in the whole development cycle (training, test, etc.) and package the algorithms which can be deployed as executable software component.</p> <p>MDT provides a suite of integrated tools to develop algorithms that adhere to CRISP-DM, a widely accepted methodology for data mining projects. It includes features for understanding the business objectives and the data that are the focus of the analysis. It offers tools for preparing datasets from raw data, developing, and training models, and testing them. It allows packaging models as containers with a REST API for easy deployment and integration into other systems. Additionally, it provides monitoring capabilities to evaluate the performance of the models in runtime when deployed. The module supports the use of external libraries for data analysis and comes with preloaded libraries that are particularly useful for anomaly detection.</p>"},{"location":"modules/MDT/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/MDT/#component-definition","title":"Component Definition","text":"<ul> <li>Module IDE: This set of components provides all the functionality needed to develop data models and algorithms during design time.</li> <li>Exploration &amp; Validation: This component allows visual exploration and profiling of data by analysts. It incorporates aspects of the Onesait Platform\u2019s profiling and dashboard engine components, which will be extended to enhance profiling and tagging of input data. It supports structured data in various formats (CSV, JSON, XML, etc.) as well as non-structured data such as text, images, and video. This module read data from the repositories directly. It supports the most used technologies as SQL database, MongoDB, S3, FTP, etc. This component temporally stores the data in a staging area. This component together with Wrangling component help to create datasets for training.</li> <li>Wrangling: This component enables the definition and execution of rules to clean data of malformed or undesirable values. It leverages an existing component from the Onesait Platform, based on the Open Refiner open-source project, which is useful for structured data. Additional development will be required to handle non-structured data. The Wrangling component can process data in the staging area of the Exploration &amp; Validation module. Both modules collaborate to define datasets for training. Once the cleaning and formatting process is complete, the resulting dataset is stored in the Minio Storage component.</li> <li>Monitoring, Logging, &amp; Alerts: Provides real-time monitoring and data visualization, including functionality for creating dashboards to track request flows and performance issues in model execution. It also defines alerts for detecting anomalous behaviour during model execution and is based on the Grafana software stack. Key tasks include creating monitoring dashboards, defining alerts, and integrating with the Unified User Interface. </li> <li>MinIO Storage: Datasets used for training and testing models will be stored in MinIO object storage. Minimal integration work is needed. The Wrangling component stores the datasets, and MinIO Storage provides them to the components that train and test the models. Any component that requires datasets can request them from MinIO Storage. For example, the Model Evaluation component will request data for validating models.</li> <li>Model Engineering: This is used to define and create models, based on two open-source projects. The first is Apache Zeppelin, a web-based notebook to perform interactive data analytics and visualization, supporting multiple programming languages such as Scala, Python, SQL, and more. The second is a tool for defining simple models using SQL language. Some minor enhancements will be necessary for better integration and pre-installed libraries, with additional tools required for picture and video analysis during the project execution. </li> <li>Model Evaluation: This component handles the training-evaluation cycle and is based on MLflow, an open-source platform for managing the end-to-end machine learning lifecycle. MLflow enables tracking experiments, packaging code into reproducible runs, and sharing and deploying models across different environments. The main task is to integrate MLflow with the Model Engineering and Model Packaging components. </li> <li>Model Packaging: Once a model is ready for production deployment, it will be packaged into a model container with clear interfaces for integration with other systems. This component is responsible for creating such deployable and executable modules. </li> <li>Model Repository: Stores model definitions, including versions and metadata about model development, such as historical information about datasets and tests performed in the training process. It also stores the necessary data for building the model, such as libraries and their versions, additional parameters, and more. </li> <li>Code Repository: Stores the code required to build the model as a deployable component, based on GitLab software, which provides Git repositories. Only configuration and integration tasks with the Unified User Interface are needed. </li> <li>Build &amp; Integration Testing: Defines and executes the construction of models as deployable software. It uses the code provided by the Code Repository together with the Model Repository data to build a software that can be deployed and executed.</li> <li>Model Deployed: As well as the containerised model, relevant code, each runtime-deployed model will include the following components: </li> <li>Model Controller: Manages the lifecycle of the module at runtime and provides an interface for external applications to use the module. </li> <li>Model: Executes the model definition, including all required software dependencies, runtime engines, and libraries for normal module execution. </li> <li>Unified User Interface: By interfacing with the other components of the module, this component enables the development of data models and algorithms during design time, as well as the visualization of alerts and monitoring data during runtime. Facilitates the usage and management of all integrated tools, providing single sign-on access to all module components. It is based on the existent Control Panel component of Onesait Platform that will be extended to support all the new capabilities introduced by this module and described in this section.</li> <li>Data Inspector: The analysis jobs executed by the T6.2 Data Inspector Module will have the capability to use models created and deployed with MDT. For more details, refer to the T6.2 Data Inspector description.</li> <li>Additional Libraries: As part of the execution of T4.2, DIGI will analyse and identify additional libraries to be included and supported for the MDT.</li> </ul>"},{"location":"modules/MDT/#screenshots","title":"Screenshots","text":""},{"location":"modules/MDT/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Indra Open Source Apache 2.0"},{"location":"modules/MDT/#top-features","title":"Top Features","text":"<ul> <li>Model Development: Gather, clean, and transform data for training.</li> <li>Storage: Store and manage datasets in MinIO.</li> <li>Experimentation: Run experiments and keep a record of all executions.</li> <li>Model Packaging: Package trained models into Docker images to facilitate deployment.</li> <li>Monitoring and Validation: Track metrics such as accuracy, latency, and throughput to detect when retraining is needed.</li> </ul>"},{"location":"modules/MDT/#how-to-install","title":"How To Install","text":"<p>TBC</p>"},{"location":"modules/MDT/#requirements","title":"Requirements","text":"<p>TBC</p>"},{"location":"modules/MDT/#software","title":"Software","text":"<p>TBC</p>"},{"location":"modules/MDT/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/MDT/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/MDT/#how-to-use","title":"How To Use","text":"<p>TBC</p>"},{"location":"modules/MDT/#other-information","title":"Other Information","text":"<p>TBC</p>"},{"location":"modules/MDT/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/MDT/#additional-links","title":"Additional Links","text":"<p>Onesait Platform GitHub https://github.com/onesaitplatform/onesaitplatform-cloud</p>"},{"location":"modules/ORC/","title":"ORC - Orchestration Module","text":"<p>Use the table below as an example and replace with the links to the module's DS2 GitHub repository and GitHub project.  </p> Project Links Software GitHub Repository https://github.com/ds2-eu/orchestration Progress GitHub Project https://github.com/orgs/ds2-eu/projects/9"},{"location":"modules/ORC/#general-description","title":"General Description","text":"<p>The Orchestration module is made to design and then orchestrate at runtime In-Dataspace, Inter-Dataspace, internal, and third-party services which facilitate common data-orientated operations such as transformation of data, checks on data, data updates etc. The orchestrator contains a flexible GUI to design workflows and decision points on these services, and run time component to implement the workflow</p> <p>Services are added to and then selected from a Service catalog from a participant\u2019s local service catalog (In-Dataspace deployment), the DS2 service intermediary catalog (Inner-Dataspace), or other available catalog/service knowledge. These services can be graphically linked together to form a workflow and where decision pathways, decision points, and other operators can be deployed to determine the workflow. Error and exit points should be predetermined with defaults ensuring that failures and error conditions allow flows to be closed automatically. One class of operator is for user defined forms for human input but most often the flows contain backend services. In the context of DS2 the operators will, if necessary, be expanded based on novel usecase peculiarities as will the forms designer. Primarily the design interface is orientated around service interconnectivity, but this will be augmented with a data pre-viewer to help interconnect and understand the results of interconnecting data-orientated services. The orchestrator will be available as a module and for interparticipant service orchestration will extend the connectors.</p>"},{"location":"modules/ORC/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/ORC/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <p>Orchestration Module - Core - Orchestration Core: This is the runtime heart of orchestration which conducts a process (workflow) triggering other services and via a BPMN module from the Service Composition designer repository. For the tier 1 standard connections (Portal etc) it can be perceived as the entry point. If new orchestration design methods are needed it will use them. Runtime events are connected to the logging components and for inter-participant dataflows it will interface with the DS2 Cross Participant Orchestration subcomponent. This is currently ICE background and will see little development except for a DS2 compliant UI.</p> <ul> <li>DS2 Service Registry: </li> <li>In-participant: This is a local registry of all services which a participant may potentially use in a workflow, composed together in the designer, and executed in the runtime. Registration can be automatic in the case of IDT installed services. It will also expose services in the inter-participant DS2 registry. It exists now but will be rebuilt in the context of DS2 and IDT.</li> <li>Between Participant: 95% the same functionality but can function similarly to a metadata broker to host services from multiple participants which can be shared in a controlled way to the In-participant registry to allow participant-participant service interactions</li> <li>Service Composition Designer (DS2 Upgrade): This is the main UI for the Orchestration Designer based on existing ICE background. It allows a user to select or drag various elements from a toolbox (services/APIs from the DS2 Service Registry, methods), which can be placed on a canvas where they can then begin to start designing their orchestration by dragging and connecting various elements together. The saved BPMN2.0 notation model will then be used by the runtime orchestration core. The DS2 upgrade will be mainly for UI and inclusion of New Data Previewer and Forms designer blocks.</li> <li>Forms designer: Many orchestrations have a need for user input and whilst some might come from other systems this can be complex when only limited information. The forms designer will allow the easy inclusion of simple form in any Service composition and also ensure that it respect the data flow as well as service needs.</li> <li>Data Previewer: This is also a new subcomponent which will be rendered via the Service composition designer. Currently services are connected but when designing it is useful to know at design time what might be the inputs and the expect result. In a data orientated project this is especially useful, and this utility will allow some rendering of data to help show flow operations between building blocks before they are deployed.</li> <li>New Orchestration methods (from pilots): Many methods \u2013 eg choice boxes, selections are already implemented in the orchestrator, but it is possible that the pilot might suggest further ones that could be interesting to implement \u2013 although at this stage of the analysis it seems there is not. The new methods will be exposed in the orchestrator runtime &amp; designer.</li> <li>Orchestration track/log: Currently this is rudimentary and especially in the trustworthy context of dataspace an major overhaul is necessary to extract more granular logging information at runtime. </li> <li>Services and API: These are the services that can be orchestrated, and the API block is the interface to each</li> <li>Other External (non DS2) Modules/Services:</li> <li>DS Service Intermediaries:</li> <li>Tier 2 In dataspace DS Modules/Service:</li> <li>Tier 0 Support Service Stack:</li> <li>DRM and API: For further exploration, but if room to implement and a match of requirements to feature the blockchain part of the DRM module to enhance logging</li> <li>DARC &amp; API: As with DRM but in this case to use DARC to help configuration</li> <li>Culture and Language Module and API: As with DRM but in this case to use this modules ontology engine to help auto-link services \u2022   Tier 1 Service Stack for Marketplace and deployment and API: The full stack will be implemented as generically described elsewhere in this document. Exceptions: The Platform will only be needed for inter-participant service orchestrations if used</li> </ul> <p>Inter-Participant orientated: - DS2 Cross DS Orchestration: This is a new runtime module which will act as a bridge between the orchestration within each participant through interconnections to the Inter-Participant Service Registry and the Orchestration core at each participant - Tier 3 Trust Stack and API: For interparticipant service the module will use relevant parts of the DS2 trust stack \u2013 see diagram below</p>"},{"location":"modules/ORC/#screenshots","title":"Screenshots","text":""},{"location":"modules/ORC/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/ORC/#top-features","title":"Top Features","text":"<ol> <li>Process Designer: A custom Camunda BPMN Modeller </li> <li>API Repository: repository of API services</li> <li>Service API: an API that allows use of services in the API Repository externally</li> <li>User Tasks: a unique type of task that allows for workflows to request user input</li> <li>User Task Manager: a front end that allows for management of user tasks</li> <li>Control Panel: a web app that allows users to view, manage and interact with workflows </li> <li>Process View: a view that allows the user to monitor the status of workflows</li> <li>Single-Sign On: integration with the Dash Button that allows the ORC module to be linked to the Portal</li> </ol>"},{"location":"modules/ORC/#how-to-install","title":"How To Install","text":"<p>To Be Done</p> <p>The how to install  has the following sections: requirements, software, summary of installation steps and detailed steps. In some cases, you may not have some of it, like for instance, the software section, or maybe you can't add this yet. Please leave the sections but add the text \"N/A\" or \"To Be Done\" depending on the module.</p> <p>So far, the how to install will be pretty much clone repo and run docker in some cases. Maybe in some others build software and run ...</p> <p>At a later stage, when idt integration is done, there will be a section for Module IDT installation.</p>"},{"location":"modules/ORC/#requirements","title":"Requirements","text":"<p>Add the minimum and recommended list of requirements in terms of CPU, RAM and Storage.</p>"},{"location":"modules/ORC/#software","title":"Software","text":"<p>Add a list of software utilities that form the module. This may not be necessary for most modules, but it is for the idt for instance.</p>"},{"location":"modules/ORC/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>Add a summary of installation steps.</p>"},{"location":"modules/ORC/#detailed-steps","title":"Detailed steps","text":"<p>Add the detailed steps of installation including screenshots or code snippets.</p>"},{"location":"modules/ORC/#how-to-use","title":"How To Use","text":"<p>Add in here the steps to use the module from the perspective of your target user. Similar to how to install, add screenshots and code snippets.</p>"},{"location":"modules/ORC/#general-overview","title":"General Overview","text":"<p>The Orchestration Module is made up of six sub comonents that all interact with each other in different ways. Together, they offer the full suite of features that the ORC tool offers independantly. </p>"},{"location":"modules/ORC/#process-designer","title":"Process Designer","text":"<p>The Process Designer is the largest single subcomponent of the ORC Module. It is a Camunda based BPMN Modeller that allows users to create workflows. </p> <p></p>"},{"location":"modules/ORC/#form-designer","title":"Form Designer","text":"<p>The Form Designer is where the user can create forms for use with User Tasks. User Tasks are tasks dicated by the Process Designer. When triggered in an active workflow, the User Task requires a user interact with a form created via the Form Designer through the Task Manager. </p> <p></p>"},{"location":"modules/ORC/#service-directory","title":"Service Directory","text":"<p>The Service Directory is a repository of APIs that can then be accessed through the Process Designer to pass data through a workflow or recieve data from a workflow. These services can also be accessed through the Service Directory API.</p> <p></p>"},{"location":"modules/ORC/#control-panel","title":"Control Panel","text":"<p>The Control Panel shows all the active workflows; active processes can be found here, and each instance of a process can also be found. Each process instance also has visual view that allows users to view various elements of a workflow after it has been created.</p> <p> A Screenshot of the Control Panel - List of Processes</p> <p> A Screenshot of the Control Panel - List of Process Instances</p> <p> A Screenshot of the Control Panel - List of Process Instance View</p>"},{"location":"modules/ORC/#task-manager","title":"Task Manager","text":"<p>The Task Manager is the component that handles the front of the User Tasks. When a User Task is triggered, the assigned user will be able to complete the User Task from this view. Administrators are also able to view all User Tasks.</p> <p> A Screenshot of the Task Manager Task List View</p> <p> A Screenshot of the Task Manager Task View</p>"},{"location":"modules/ORC/#making-and-running-a-simple-process","title":"Making and Running a Simple Process","text":""},{"location":"modules/ORC/#other-information","title":"Other Information","text":"<p>No other information at the moment for MODULE.</p>"},{"location":"modules/ORC/#openapi-specification","title":"OpenAPI Specification","text":"<p>Add your open api specification, if you have any. The idea is to be able to include here the swagger UI, but so far just a snapshot or the yaml specification.</p>"},{"location":"modules/ORC/#additional-links","title":"Additional Links","text":"<p>Add in here relevant links for your module. In this section we will also add the link to the module video.</p>"},{"location":"modules/PAE/","title":"Policy Agreement and Enforcement (PAE)","text":"<p>Use the table below as an example and replace with the links to the module's DS2 GitHub repository and GitHub project.  </p> Project Links Software GitHub Repository https://github.com/ds2-eu/pae_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/21"},{"location":"modules/PAE/#general-description","title":"General Description","text":"<p>The primary function of the Policy Agreement and Enforcement Module (DS2 PAE) is to ensure compliance with the established policies and regulations governing data exchange among users in different data spaces. Henceforth, policies, regulations, and agreements are synonymous with the term policy. The policies are evaluated as the control plane stage of data sharing in the Connector. The policies serve two main purposes: Access Control and for Usage Control. Access Control determines whether access to data is granted or denied. Usage Control dictates how the data can be used once access is granted.</p> <p>Policies in dataspaces define who can access the data and the restrictions on data use for those with access. A policy, in this context, is a set of rules governing data sharing within a dataspace and, more specifically for DS2, between dataspaces as well as their participants. The main function of this module is to enforce policies associated with a data-sharing contract, an agreement between a provider and a consumer. The agreed policy, the contract, specifies the rules both parties must follow. This module focuses on rules that can be automatically enforced by software. Policies are evaluated before data transmission begins. Rules that cannot be automatically enforced are logged alongside the contract agreement for accountability. There will be two types of rules, Access Control and Usage Control. </p> <ul> <li>Access Control rules define who is authorized to access the data. If these rules are not met, data sharing is not permitted. </li> <li>Usage Control rules define how the data can be used. Usage rules may require additional actions during or after data sharing, which might need to be executed by other software components or through human intervention. </li> </ul> <p>The Policy and Agreement Enforcement Module will notify the relevant modules about the need for these actions.</p>"},{"location":"modules/PAE/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment.</p> <p></p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module.</p> <p></p> <p></p> <p></p>"},{"location":"modules/PAE/#component-definition","title":"Component Definition","text":"<p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module.</p> <p>The following figure expands on the previous one by detailing the subcomponents for policy enforcement and their relationships with other modules.</p> <p>This module has the following subcomponent and other functions: Note that the large green box represents a connector. It is depicted this way to clarify which components are connector extensions deployed within the connector, and which are deployed outside it.</p> <ul> <li> <p>Policy and Agreement Enforcement Module:</p> </li> <li> <p>Policy Extension (PDP): This extension will be integrated into the Policy Enforcement Point of the Connector. It will oversee the coordination of all policies supported by DS2. </p> </li> <li>PAE Metadata Translator: This optional extension will translate metadata to ensure compatibility with rule enforcement across different dataspaces with varying participant and service descriptors. As of this document's edition, it is to be decided if this will be handled by the Connector itself, through the CLM Module, or through the Catalog Module therefore, it will be implemented as needed. </li> <li>Sovereignty Transformation Interceptor: This extension will enable policy-oriented data transformations during data exchange - for example anonymising data. It serves as the implementation of a Policy Execution Point. Note that the Data Transformation feature of DDT module is not used to do this since it is more generic</li> <li>Sovereignty Transformation Policy - Runtime: This component provides an example of Usage Control rule enforcement. It is expanded and described in detail later in this section.</li> <li>Policy Execution Point (PXP): This component interfaces with the execution of additional actions required when a policy is enforced and after policy evaluation. An example is the Sovereignty Transformation Interceptor, which will be used to enforce certain usage control rules.</li> <li>Policy Information Point (PIP): This component is responsible for implementing interfaces with all relevant modules and external software (e.g., retrieving participant information from a CRM application) from which additional context data is needed for policy evaluation. A mandatory interface is with the Data Marketplace module to validate asset purchases. </li> <li>Policy Logger: This component will handle all logging related to policy enforcement. In addition to internal logging, policy decisions can be recorded in the Data Rights Management module. </li> <li>Policy Management Point (PMP): This component stores all the policies that must be enforced for each share of data. Specifically, it will store the contract offers, including the reference to the data offered and the associated policies.</li> <li>Rule Implementation: Each rule within a policy must have a corresponding software component that supports its evaluation. A Rule Implementation component may support several rules and might rely on an existing rule engine or be implemented from scratch, depending on the rules it needs to support. Access Control rules will be enforced based on these implementations. ODRL will be used by T3.2 for defining the types of rules allowed. Within PAE an example of Usage Control rules will be implemented using the Sovereignty Transformation Policy. In these cases, the Rule Implementation will evaluate the rule, but the needed actions to enforce the rule are delegated, in the mentioned example to the Sovereignty Transformation Policy. The responsibility of PAE is to notify to the required component. If there is not such a component to automatically handle rule actions, the evaluation will be logged with the Policy Logger, but any further verification or action is out of the scope of this module.</li> <li>Policy Administration Point (PAP): This is a UI that allows for the definition of policies to be used in data sharing. Policy definitions are stored in the PMP. Policies will be defined using templates developed based on the types of rules identified in WP3. The tool will assist users in easily defining rules and will generate machine-readable policies.</li> <li>Sovereignty Transformation Policy: </li> <li>Sovereignty Transformation Policy - IDE: This group of components is shared with the T6.2 Data Inspector. Some of them will be developed based on existing technology, specifically the Dataflow component of the Onesait Platform open-source product. While T4.1 Policy Enforcement focuses on defining and executing transformation jobs for policy enforcement, the Data Inspector will enhance the current capabilities of Dataflow to include data inspection, monitoring, and notifications. T6.2 will lead this component development, and T4.1 will be built on the improved version from T6.2. Therefore, T4.1 has a dependency on T6.2, but the opposite is not true.<ul> <li>Sovereignty Transformation IDE UI: This graphical user interface allows users to define Sovereignty Transformation Jobs for enforcing policy rules that require data modifications. Examples include field removal and data anonymization. A Sovereignty Transformation Job definition is a data pipeline with an input, an output, and a set of transformation stages in between. This component is based on existing INDRA software, with updates needed to support new features for DS2. </li> <li>Sovereignty Transformation IDE Controller: This component manages Sovereignty Transformation Jobs during design time and oversees their deployment and monitoring at runtime. It is based on current INDRA software but requires significant upgrades to split the tool into the IDE component and the Runtime component (potentially several). </li> <li>Testing and Validation: This component will encompass test definitions and include the storage of small datasets specifically for automatic testing purposes as well as providing validation functionality. The automated tests will be designed to thoroughly validate the accuracy and correctness of the sovereignty transformation jobs. This validation process is essential to ensure that the jobs meet the required standards and perform as expected before they are deployed. By conducting these tests beforehand, potential issues can be identified and addressed, ensuring a smooth and reliable deployment of the sovereignty transformation jobs.</li> <li>Job Definition Storage: This component stores definitions of Sovereignty Transformation Jobs, based on INDRA software, with extensions planned to improve version control of the definitions. </li> </ul> </li> <li>Sovereignty Transformation Policy - Runtime:<ul> <li>Runtime Controller: This component will manage the execution of Sovereignty Transformation Jobs during runtime. It will define the interface for integrating job execution with the Sovereignty Transformation Interceptor and will handle the job lifecycle: deployment, upgrade, removal, start, and stop.</li> <li>Sovereignty Transformation Job: This component represents the runtime execution of a data pipeline definition. Each transformation supported will require a Sovereignty Transformation Job definition. One instance of this component will be created for each Sovereignty Transformation Job needed at runtime, even for the same definition. The creation of these instances will be managed by the Runtime Controller. </li> <li>Job Logger: This component logs all relevant information about each job execution. Based on INDRA software, it will require minimal development to adapt to changes in other module components. </li> <li>Job State Storage: This component stores the states of job executions throughout their lifecycle, enabling job resumption in the event of failures during execution. Based on INDRA software, it will require minimal development.</li> </ul> </li> </ul> <p>Interfaces with other modules:</p> <ul> <li>T3.2 Types of rules allowed (ODRL): This will define the types of rules that might be used in policy definition. The types of rules will be defined using the ORDL standard, which has been selected by IDSA for policy enforcement in dataspaces. Once these rules are defined, the Policy and Agreement Enforcement module will be responsible for providing software support to enforce all rules that can be evaluated without human intervention.</li> <li>Data Right Management (DRM): DRM will be used by the Policy and Agreement Enforcement to log the decisions about policy.</li> <li>Data Marketplace (DMK): The Data Marketplace will be queried by the Policy and Agreement Enforcement module to obtain additional information if a policy requires a purchase to obtain data.</li> <li>Culture and Language Module (CLM): Integration with the T5.1 Knowledge Base will be implemented to translate metadata, ensuring compatibility across different dataspaces. If it is determined later in the project that the Culture and Language Module is unsuitable for this task, simple mappings using the metadata from the project's use cases will be employed to prevent blocking the implementation.</li> <li>Other Software / Other DS2 Modules: The PAE module defines APIs to obtain additional information needed for the policy enforcement process, such as metadata about participants or services that may not be accessible via the connector. In such cases, PAE will acquire the necessary data. Additionally, if any rules require actions to be executed by other software or DS2 modules, PAE will notify the requirement when evaluating the rules.</li> </ul>"},{"location":"modules/PAE/#screenshots","title":"Screenshots","text":"<p>N/A</p>"},{"location":"modules/PAE/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License Indra Open Source Apache 2.0"},{"location":"modules/PAE/#top-features","title":"Top Features","text":"<ul> <li>Integrated with EDC Connector</li> <li>Enforcement of policies defined in DS2</li> <li>Use of contextual information about participants obtained from other DS2 modules.</li> <li>Capabilities to enforce not only access rules but also some usage rules.</li> </ul>"},{"location":"modules/PAE/#how-to-install","title":"How To Install","text":"<p>The Policy and Agreement Enforcement Module is provider together with the DS2 Connector.</p> <p>The software for Sovereignty Transformation Policy ...</p> <p>TBC</p>"},{"location":"modules/PAE/#requirements","title":"Requirements","text":"<p>The same that for DS2 Connector.</p> <p>The software for Sovereignty Transformation Policy is highly dependent on the volume of data to process. It is based on DINS module. </p>"},{"location":"modules/PAE/#software","title":"Software","text":"<ul> <li>DS2 Connector</li> <li>DINS module for Sovereignty Transformation Policy</li> </ul>"},{"location":"modules/PAE/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>TBC</p>"},{"location":"modules/PAE/#detailed-steps","title":"Detailed steps","text":"<p>TBC</p>"},{"location":"modules/PAE/#how-to-use","title":"How To Use","text":"<p>TBC</p>"},{"location":"modules/PAE/#other-information","title":"Other Information","text":"<p>TBC</p>"},{"location":"modules/PAE/#openapi-specification","title":"OpenAPI Specification","text":"<p>TBC</p>"},{"location":"modules/PAE/#additional-links","title":"Additional Links","text":"<p>TBC</p>"},{"location":"modules/PORTAL/","title":"Portal","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/portal Progress GitHub Project https://github.com/orgs/ds2-eu/projects/11"},{"location":"modules/PORTAL/#general-description","title":"General Description","text":"<p>To provide a user and developer friendly portal allowing dataspace participants to register and select DS2 modules which can then be packaged into a IDT environment subsequently deployed by participants enabling both In-Data Space and Inter-Data Space operations. As such it includes functionality for developers to include modules, users to find those modules, to trigger the packaging through links with the containerisation module, as well as supporting functionality for dataspace support, dataspace resources, registration and identity management, and administration. It also provides support for the Data Marketplace. </p> <p>The portal will operate in the DS2 cloud hosted by i4RI. Parties interested in DS may register \u2013 this includes participants, dataspace governance/operators, DS2 module developers as well as, potentially, service intermediaries. Developers will provide modules to the portal marketplace which can be selected/purchased by participants. Selected modules will then be  downloaded and installed by and at a participant IDT. All parties involved will be granted unique IDs including DS-Pair IDs where DS\u2019s agree to cooperate. If manpower resources allow ,a subcomponent will also support user-developer and developer-developer interactions as well as additional static resources such as useful materials or links to IDSA etc. The portal will run on the DS2 platform as a central node but in itself will not be involved in any participant-participant process either at the control or data plane levels. It operates at Tier 1, so is not a \u201cmodule\u201d in-itself, even though it will be deployed and run on top of IDT like a module, so participants would not deploy it. However, conceptually it could be run locally. It could be run by any one as an additional service \u2013 for example dataspace operators. Its main interfaces are to the Containerisation module for module description and packaging, IDT for deployment, the Platform where it is both deployed and will interface to some system modules, and the Data Marketplace which uses it for base marketplace functions. </p>"},{"location":"modules/PORTAL/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/PORTAL/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions:</p> <ul> <li> <p>Portal Front End: This is the initial web front end entry point seen by any user of the portal, regardless of participant type or if an external party (eg developer). If unregistered, it directs them to the Registration and Management component and if registered a login process follows. Dynamic menus then allow users to navigate to the elements they wish to see and have access for and as such there is an inferred link to all subcomponents with some form of UI. Of particular relevance is the Dashbutton which is a kind-of mini-Portal Front End embedded in all subcomponents and modules including the Portal itself.</p> </li> <li> <p>Registration and Management: This allows the registration of any actor which wishes to take advantage or use DS2. This includes human actors such as user\u2019s requiring IDT/modules and developers through to system actors such as dataspaces. This will be a customised version of ICE\u2019s existing portal registration system. It\u2019s primary link is to the DS2 Identity Manager.</p> </li> <li> <p>Dash button: The Dashbutton is a DS2 library feature used by most modules and is a dynamic shortcut menu to things of most relevance to the user (eg mainly their installed modules). It allows users to quickly get awareness of their personal DS2 environment as well as control it. The Dashbutton dynamic is detailed within the IDT Module. The base functionality of the Dashbutton exists now but needs to be highly customised for the DS2 environment. It relies on embedded HTML vs an API and enables a holistic environment for DS2 in an easy way. It is detailed in the IDT module.</p> </li> <li> <p>Resource Connect: This will have no deep functionality but allows the users to navigate to further web/document/video type resources both within DS2 and externally (eg IDSA). For example, to read more technical details, look up example dataspaces or any other material. This component is an \u201cif-time-allows\u201d option.</p> </li> <li> <p>Developer Connect: This is intended to simplify casual chat-like interaction between users and developers or in fact any set of participants subscribed to DS2. For example, for support or to discuss new ideas. If included it will be based on existing third part components. This component is an \u201cif-time-allows\u201d option.</p> </li> <li> <p>Module Marketplace: Listing, Cart, Purchase: DS2 modules need to be selected from a catalogue of all modules, purchased (which may be for 0 EUR), and licensed which then makes them available to be packaged into IDT. Except for the latter this is classic marketplace functionality and is mainly configuration of ICE\u2019s existing marketplace module. Its primary links are to the Module uploader to populate its shopfront and once a module is \u2019purchased\u2019 the License and Use Manager.</p> </li> <li> <p>Module Uploader: Beneficiaries (or third parties) develop data-orientated DS2 modules which can be put on DS2 Marketplace. A main link is to the DS2 Identity Manager to give unique identities to the module uploaded. This subcomponent ensures all relevant information is provided and packaged in the right way. This includes software, module details (eg price, licenses, description, logo...), and accompanying knowledge (eg How To\u2019s, Videos\u2026). It ensures a module is K8s/Helm compliant so that it can be potentially packaged into IDT. It will be based on ICE Asset upload being customised for the DS2 environment and all modules must confirm to its needs.</p> </li> <li> <p>Payment: Once agreed the product is paid for and both the portal and the developer compensated. An existing payment service (stripe) will be used and a cart-mediator service is used to link to that (strip link is not shown in architecture). This service will also be used by the Data Marketplace</p> </li> <li> <p>License and Use Manager: Following potential purchase from the marketplace the asset is then licensed for use (fixed licenses, subscription etc \u2013 however developer determines) and a Payment is then due. Once licensed it becomes available for the Containerisation and Deployment grouping to either issue a new IDT or to upgrade and existing IDT. The components is based on existing ICE Marketplace technology upgraded for DS2. Other modules can also use the licensing system \u2013 eg the Data Marketplace (TBD). Note that on first use core modules such as containerisation will also be packaged</p> </li> <li> <p>Administration: A UI for Portal (and Platform) administrator to use to configure other elements and functions of the portal based on access credentials.</p> </li> <li> <p>Portal Identity Manager: Provides identities for actors (parties and modules) relevant to the Portal. All DS2 users must have a DS2 ID, as should Dataspaces, and Dataspace Pairs. The Portal Identity Manager will where necessary interface with the DS2 Identity Module (IDM) to validate participant IDs which will run on the DS2 Platform</p> </li> <li> <p>DS2 Containerisation and API: This provides two basic services to the portal: A) Assisting and ensuring a developer uploads a containerisation compliant module. B) Once modules are selected and licensed, they can be packaged and deployed via this module to downloadable/upgradable IDTs</p> </li> <li> <p>DS2 S/W Platform and API: This platform is available to all modules and is the cloud home of the Portal as well as the Portal hosting an administrative interface to configure it</p> </li> <li> <p>DS2 Data Marketplace and API: The cloud base data marketplace may take advantage of some of the portal common administration function \u2013 user management, licensing, payment</p> </li> <li> <p>DS2 IDT Broker: The licensed modules are packaged via DS2 Containerisation and deployed as a bespoke IDT to the participant</p> </li> <li> <p>DS2 Portal Operator: This represents the administrator and operator of the system who use the Administration UI</p> </li> <li> <p>Modules, s/w, details, knowledge: This represents the upload of the module and accompanying information to become an asset in the portal which can be later explored and purchased. The upload is to the Module Uploaders.</p> </li> <li> <p>Developers: Technicians and business partners representing a developed module to be uploaded</p> </li> <li> <p>User: Any participant of any data space, or future potential participant who wishes to explorer/download modules available</p> </li> <li> <p>External Modules: Other modules which could be applicable to show on a users Dashbutton</p> </li> </ul>"},{"location":"modules/PORTAL/#screenshots","title":"Screenshots","text":"<p>To Be Done </p>"},{"location":"modules/PORTAL/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/PORTAL/#top-features","title":"Top Features","text":"<ol> <li>Organisation Registration and Management: The Portal provides the functionality to register an organisation in DS2 and manage its profile</li> <li>User and Role Management: Create additional users and roles to the organisation</li> <li>Main DS2 Entry Point: The Portal is the UI main entry point to DS2</li> <li>DS2 Navigation: Allows seamless navigation between intermediary modules in the DS2 Cloud Platform</li> <li>Global and Local Portal: The Portal has two different views, the Global Portal running as an intermediary service in the DS2 Cloud Platform and the Local Portal which is meant for the IDT as the IDT Portal</li> <li>Dash Button: A web component to be integrated by all modules that provide the interface to the DS2 security and identity system and allows module navigation</li> <li>Developer Access: Generate API Key to use the Portal API for integration</li> <li>Portal Marketplace: The Portal includes a fully featured Marketplace, which could be considered a module on its own, with functionalities to publish and purchase modules, module search, feedback and ratings, flexible license models, secure payment integration, etc.</li> <li>GitHub Backend Storage: The Marketplace feature is backed by GitHub storage, meaning that modules and container images are stored in GitHub</li> <li>Containerisation Integration: The Module creation view of the Containerisation module is integrated into the Portal</li> </ol>"},{"location":"modules/PORTAL/#how-to-install","title":"How To Install","text":"<p>Even though the Portal (Global Portal) is a DS2 intermediary service, and users don't need to install it, it is packaged as a regular DS2 module and can be installed on top of the IDT, same as the Local Portal. The Marketplace is also installed as an additional module. In addition, the Dash Button is a web component that needs to be integrated in all DS2 module UIs. Details on how to integrate it are explained next.</p>"},{"location":"modules/PORTAL/#requirements","title":"Requirements","text":"<p>N/A</p>"},{"location":"modules/PORTAL/#software","title":"Software","text":"<p>N/A</p>"},{"location":"modules/PORTAL/#summary-of-installation-steps","title":"Summary of installation steps","text":""},{"location":"modules/PORTAL/#portal_1","title":"Portal","text":"<ol> <li>Run a keycloak instance or use the ds2 one (recommended)</li> <li>Pull the docker images from the DS2 GitHub Container Registry</li> <li>Run the containers from the docker images: Portal API and Portal Frontend</li> </ol>"},{"location":"modules/PORTAL/#marketplace","title":"Marketplace","text":""},{"location":"modules/PORTAL/#dash-button","title":"Dash Button","text":"<ol> <li> <p>Add script file path to index.html</p> </li> <li> <p>Add the font-awesome library (starting from v0.0.16)</p> </li> <li> <p>Configure according frontend framework ie. Angular ...</p> </li> <li> <p>Place the web-component button tag at the top of the navigation bar </p> </li> </ol>"},{"location":"modules/PORTAL/#detailed-steps","title":"Detailed steps","text":""},{"location":"modules/PORTAL/#portal_2","title":"Portal","text":"<ol> <li>First, you need to start your own Keycloak instance or use the DS2 version. We recommend using the DS2-based Keycloak instance<ul> <li>Instructions on using own Keycloak instance TBD</li> <li>To use the DS2 Keycloak instance, you just need to configure the URL https://keycloak.ds2.icelab.cloud when running the containers - see next step </li> </ul> </li> <li> <p>Pull the docker images form the GitHub repository <pre><code>docker pull ghcr.io/ds2-eu/ds2charts/portal:1.2.6\n</code></pre></p> <p>You will need to log in the registry using docker login ghcr.io</p> </li> <li> <p>Run the docker images</p> <ul> <li>The Portal API <pre><code>docker run --name portal-api -d -p 3000:3000 -e IMAGE_GITHUB_REPONAME=\"test-avatar\" -e DASHBUTTON_LINKS='{\"local\": [{\"name\": \"IDT\", \"url\": \"idt.ds2\", \"icon\": \"fa-brands fa-uncharted\"}, {\"name\": \"Containerization\", \"url\": \"containerization.ds2\", \"icon\": \"fa-solid fa-server\"}], \"global\": [{\"name\": \"IDM\", \"url\": \"idm.ds2\", \"icon\": \"fa-solid fa-file-shield\"}, {\"name\": \"Catalog\", \"url\": \"catalog.ds2\", \"icon\": \"fa-solid fa-folder-tree\"}, {\"name\": \"Marketplace\", \"url\": \"https://marketplace.ds2.icelab.cloud\", \"icon\": \"fa-solid fa-cart-arrow-down\"}, {\"name\": \"Chat\", \"url\": \"chat.ds2\", \"icon\": \"fa-regular fa-comment-dots\"}] }' -e KEYCLOAK_URL='https://keycloak.ds2.icelab.cloud' -e REALM_NAME=ds2 -e CLIENT_ID=portal-api  -e CLIENT_SECRET=RT7FdTDKbSI6CsU8DKosHtnw5PHRIblM -e DASHBUTTON_MENU_LINKS='[{\"name\": \"Marketplace\", \"icon\": \"fa-solid fa-cart-shopping\", \"url\": \"https://marketplace.ds2.icelab.cloud/\"}, {\"name\": \"Chat\", \"icon\": \"fa-regular fa-comment-dots\", \"url\": \"http://chat\"}]' ghcr.io/ds2-eu/ds2charts/portal-api:1.2.0\n</code></pre></li> <li>The Portal Frontend <pre><code>docker run --name portal -d -p 4200:80 -e PORTAL_URL=\"https://portal.ds2.icelab.cloud\" -e PORTAL_API=\"https://portal-api.ds2.icelab.cloud\" -e DISABLE_APPLICATION_TAB=true -e REDIRECT_URL=\"https://portal.ds2.icelab.cloud/dashboard\" -e  CLIENT_ID=dashbtn -e REALM=ds2 -e SHOW_POST_LOGIN_TEXT=true -e APP_DISPLAY_NAME=\"Modules\" -e OTHER_LINK_DISPLAY_NAME=\"Default Modules\" -e KEYCLOAK_URL=\"https://keycloak.ds2.icelab.cloud\" -e DEPLOYMENT_MODE=global -e SHOW_ONBOARDING_MESSAGE=true portal:1.2.6\n</code></pre></li> </ul> </li> </ol>"},{"location":"modules/PORTAL/#marketplace_1","title":"Marketplace","text":""},{"location":"modules/PORTAL/#dash-button_1","title":"Dash Button","text":"<p>To install the dash-button web component, copy and paste it into your frontend application.</p> <p>Note: Please refer to the npm package page to find the latest version, and replace <code>X.X.X</code> in the import script accordingly: <pre><code>&lt;script type=\"module\" src=\"https://unpkg.com/dash-button-web@X.X.X/dist/esm/web-compnont.js\"&gt;&lt;/script&gt;\n</code></pre></p> <p>Note: Starting from v0.0.16, you need to add the FontAwesome stylesheet file to your application. If your application is already using this package, you can ignore this.</p> <pre><code>&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css\" /&gt;\n</code></pre> <p>You can find the NPM package named dash-button-web.</p> <p>If it is a framework, you will need to configure more options. Therefore, follow the Stencil.js documentation. </p> <ul> <li>Setup Keycloak Server Application</li> </ul> <p>First, you need to start your own Keycloak instance or use the DS2 version. We recommend using the DS2-based Keycloak instance.</p> <pre><code>* Option 1 \u2013 Use DS2 Keycloak details\n\n  You can use predefined Keycloak credentials to configure DashButton. The required details are summarized below.\n\n  - Keyclaok URL: **https://keycloak.ds2.icelab.cloud**\n  - Realm: **ds2**\n  - Client ID: **dashbtn**\n\n* Option 2 \u2013 Configure your own Keycloak client\n\n  If you use your own version, you need to configure the realm, client, and test users.  \n  Make sure the client is set up with a **client ID** and that **client authentication is disabled**.\n</code></pre> <ul> <li>Configure frontend application</li> </ul> <p>Here, we are demonstrating how to configure the application on an Angular-based application and a basic HTML-based application. If you need to install React or Vue.js, follow the Stencil.js documentation.</p> <p>First, you need to enable the custom component support feature in the Angular project. To do that, the following code block needs to be added to the <code>src/app/app.module.ts</code> file.</p> <pre><code>@NgModule({\n  \u2026\n  schemas: [\n    CUSTOM_ELEMENTS_SCHEMA\n  ],\n})\n</code></pre> <p>Once the above addition is completed, it is required to import the below module as follows.</p> <pre><code>import { CUSTOM_ELEMENTS_SCHEMA} from '@angular/core';\n</code></pre> <p>Next, you need to put the script file path in the src/index.html file header section as follows.</p> <pre><code>&lt;head&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n  &lt;link rel=\"icon\" type=\"image/x-icon\" href=\"favicon.ico\"&gt;\n\n &lt;script type='module' src='https://unpkg.com/dash-button-web@X.X.X/dist/esm/web-compnont.js'&gt;&lt;/script&gt;\n &lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css\" /&gt;\n\n...\n</code></pre> <p>After that, you can pass configuration options as follows using the web component.</p> <ul> <li> <p>Configuration options</p> Attribute Description Type Default <code>client-id</code> <code>Keycloak client ID</code> <code>string</code> `` <code>keycloak-uri</code> <code>Keycloak server URI</code> <code>string</code> <code>\"http://localhost:8080\"</code> <code>realm</code> <code>Keycloak realm</code> <code>string</code> `` <code>redirect-uri</code> <code>Application redirect URI</code> <code>string</code> `` <code>show-post-login-text</code> <code>Successfully logged in shows a custom button message</code> <code>boolean</code> <code>false</code> <code>auth-method</code> <code>Authentication method</code> <code>string</code> <code>check-sso</code>, <code>(login-required)</code> <code>app-id</code> <code>Application unique ID</code> <code>string</code> `` <code>portal-url</code> <code>Portal URL</code> <code>string</code> `` <code>portal-api-url</code> <code>Portal API URL</code> <code>string</code> `` <code>show-unauthorized-modal</code> <code>Show unauthorized modal</code> <code>boolean</code> <code>false</code> <code>other-link-type</code> <code>Add new links for the menu</code> <code>string</code> <code>local or global</code> <code>application-display-name</code> <code>Menu application display name</code> <code>string</code> <code>Applications</code> <code>disable-application-tab</code> <code>disable application tab</code> <code>boolean</code> <code>false</code> <code>menu-view-type</code> <code>the style of the menu</code> <code>string</code> <code>grid/list</code> </li> <li> <p>Theme configuration options</p> Attribute Description Type Default <code>primary-color</code> <code>Set primary colour</code> <code>string</code> `` <code>accent-color</code> <code>Set secondary or accent color</code> <code>string</code> `` <p>AuthMethod Using the auth-method attribute updates the application's authentication workflow. <code>login-required</code>:  User is forced to the login screen if they are not logged in. <code>check-sso</code>: User is not forced to the login screen; instead, when the user clicks the login button, they are redirected to the login screen.</p> </li> </ul> <p>Now, you are almost done. In the final stage, you can place the web-component button tag at the top of the navigation bar using the following code block.</p> <p>Note: Replace DashButton configuration with actual values.</p> <p>To get the module link for the Dashbutton, use the portal-api-url attribute and provide the Portal backend API endpoint URL.   For the DS2 platform, you can use https://portal-api.ds2.icelab.cloud as the Portal API URL..</p> <p><pre><code>&lt;dash-button keycloak-uri=\"http://localhost:8080\" realm=\"demo\" client-id=\"testapp\" portal-api-url=\"replace_url\" auth-method=\"login-required\" show-post-login-text=\"false\"&gt;&lt;/dash-button&gt;\n</code></pre>   After that test your application can be test with user credentials and then you can get an output similar to this.</p> <p>Note: You need to create a new user. Please access the portal and create an account associated with your organization.</p> <p>Additionally you can test some of the demo applications located in the Portal repository dash-button/demo-app/ folder (see additinal links)</p>"},{"location":"modules/PORTAL/#how-to-use","title":"How To Use","text":"<p>The next sections describe how to use the different components part of the Portal</p>"},{"location":"modules/PORTAL/#portal_3","title":"Portal","text":"<p>In order to use the Portal, first navigate to the main entry page at https://portal.ds2.icelab.cloud/.  </p> <p>You need to first register your organisation to be able to log in. Click on Create an account and enter the required details. First enter the name of the new organisation and click Next to check whether it already exists or not in the system. </p> <p>Then enter all the details required for the organisation registration and click Register.  </p> <p>After that, organisation is registered. </p> <p>User can now log in with newly created organisation and username. </p> <p>User will receive a verification email upon first login attempt </p> <p>Once logged in there is a guided tour on the different options available that uses can follow. </p> <p>The main Dashboard of the Portal shows the list of default central modules running in the DS2 Cloud Platform ie. IDM, Catalog, Marketplace and Chat at the moment. The list will evolve as new features are added to DS2. </p> <p>In the left navigation bar users can access and edit the profile information. </p> <p>The Dataspaces option which links to the IDM (see IDM documentation). So far this is a link but researching to embedd the functionality inside the Portal.</p> <p>The Users option allows creating new users for this organisation. Click on Add new user and enter the details. </p> <p>In addition, the Portal already includes the DashButton which is described in the DashButton section.</p>"},{"location":"modules/PORTAL/#marketplace_2","title":"Marketplace","text":"<p>The Marketplace includes two main different views. The first one is the so called Marketplace for users to be able to purchase modules. The second one is the Product License Manager aimed at module developers to be able to publish their modules and monitor sales.</p> <ul> <li> <p>Marketplace </p> </li> <li> <p>Product License Manager</p> </li> </ul>"},{"location":"modules/PORTAL/#dash-button_2","title":"Dash Button","text":"<p>To Be Done</p>"},{"location":"modules/PORTAL/#other-information","title":"Other Information","text":"<p>No other information at the moment for Portal</p>"},{"location":"modules/PORTAL/#openapi-specification","title":"OpenAPI Specification","text":"<p>To Be Done</p>"},{"location":"modules/PORTAL/#additional-links","title":"Additional Links","text":"<p>Video https://youtube.com/portal</p> <p>Portal Repository https://github.com/ds2-eu/portal (Private Link for Project Members)</p>"},{"location":"modules/RET/","title":"DS2 Data discovery and transformation module (RET)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/cur.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/3"},{"location":"modules/RET/#general-description","title":"General Description","text":"<p>The RET module uses machine learning technology to help a data consumer obtain access to a data store. The obtain data can either be saved to a file, or pipelined to an application with transformations between the source and target schemas automatically executed.</p>"},{"location":"modules/RET/#description","title":"Description","text":"<p>Data offered by a data provider may be either a fix set of data, or, more typically for large sets of data, a slice extracted from a larger set of data, selectable through parameters such as a time window, or the ID of a specific entity.  While in the former case the creation of the REST call to obtain the data is straightforward, the latter case can be challenging, especially if special formatting of the request parameters \u2013 such as creating timestamp in a given RFC format \u2013 is needed.  Additionally, the data in the data offering may not conform to the requirements of the data requester (format, data field names, units etc.).  Both of these cases can benefit from technical features to allow a data consumer to obtain and use data, which in the worst case will prevent non-technical users from taking advantage of the data space concept.</p> <p>The Data Retrieval module addresses both of these issues through the use of a machine learning Large Language Model (LLM).  Using an OpenAPI spec for the Provider data (obtained through the Catalogue), the user is guided to enter whatever request parameters are required by the REST call to the provider.  The LLM is supplemented with the relevant portions of the OpenAPI spec and will compose the correct REST call, automatically formatting the input parameters to conform to the requirements of the spec.  The REST command can then be automatically executed from the Data Retrieval GUI.</p> <p>Optionally, to supply the consumer with data in the expected format, a data schema for the target must be supplied.  The OpenAI spec supplies the data schema for the source data.  Additionally, the consumer will require a library of various commonly required transformations, such as changing column names.  Once again, LLM RAG technology is used, supplying the LLM with the source and destination schemas, as well as a description of the functions in the transformation library.  The LLM is then able to determine what transformations are required on the source data, and the Data Retrieval module then automatically adds to the data acquisition pipeline the required transformations on the data.  Optionally, a target application can be specified, and the Data Retrieval module will automatically add this to its pipeline and execute the target application with the obtained and transformed data.</p> <p>Function generation (i.e. the creation of the REST call) works without any errors for use case data sets that take parameters.  In the GreenDeal use case, for example, the module will correctly create timestamps entered in natural language to the required RFC 3339 format. Similarly, the selection of a transformation routine from the transformation library works properly for our test.</p> <p>More experimentation is required, however with a larger number of required transformations from actual user requirements.</p>"},{"location":"modules/RET/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/RET/#component-definition","title":"Component Definition","text":""},{"location":"modules/RET/#screenshots","title":"Screenshots","text":"<p>The picture below represents how RET guides the user through the parameter input process: </p> <p>The following picture shows the result of executing the RET, requesting transformations and application execution. </p>"},{"location":"modules/RET/#commercial-information","title":"Commercial Information","text":"Organisation (s) License Nature License IBM Consortium Confidential TBD"},{"location":"modules/RET/#top-features","title":"Top Features","text":"<ul> <li>GUI to guide through the input of command line parameters</li> <li>LLM function calling to properly compose the REST call, including formatting of command line parameters</li> <li>Automatic execution of the REST call and either storing the returned results to a file or pipelining the results to an application.</li> <li>Automatic selection of required data transformations to bring the downloaded data to the format of a target schema.</li> <li>Automatic injection of API token into the composed REST call.</li> </ul>"},{"location":"modules/RET/#how-to-install","title":"How To Install","text":"<p>Clone this git module.  </p> <p>In order to run the RET module, the correct python environment needs to be created.  It is recommended that you create a virtual environment as follows:</p> <ul> <li>python -m venv [name]  where \"name\" will be the name of the virtual environment, for example, \"venv\".</li> <li>Activate the virtual environment: On Mac or Linux this will be: <code>source &lt;name&gt;/bin/activate</code>, where the value of \"name\" is from the previous step.</li> <li>Install inside the virtual environment the required modules: <code>pip install requirements.txtx</code></li> </ul>"},{"location":"modules/RET/#running-the-module","title":"Running the module","text":"<p>Run the RET module as: <code>python data_discovery_main.py</code> with the command line options described in the following table:</p> Option Explanation Required or Optional --dir The pathname to the directory for the OpenAPI files for the source data. Required --transform The pathname to the file defining configuration for the transformation and application execution pipeline. Optional --url The base URL for the LLM. Required --name The model name for the LLN.  It is recommended not to change the default value. Optional --id The model id for the LLM.  It is recommended not to change the default value. Optional --llmkey The API key for the LLM. Required --token The API token to be passed to the data source server. Optional --outdir The starting directory in the directory selection box when export to a file is requested. Optional --appconfig The pathname of the application configuration file Optional"},{"location":"modules/RET/#in-operation","title":"In operation","text":"<p>The current version of RET requires an OpenAPI specification for the source data.  OpenAPI specifications have become the industrial standard for describing RESTful APIs and provide information on the parameters supplied to the REST call as well as the expected format (schema) for the data to be returned.  However, while OpenAPI specifications are designed to be human-readable, it is not always easy to understand what is required for input (e.g. the format of a timestamp).  </p> <p>The RET module is configurable to point to a list of OpenAPI specifications, and the user is able to select one or more specifications in the application GUI.  Selecting an OpenAPI spec will display a list of all the REST endpoints (functions) contained within that specification.  Selecting a function will display the text description of the function from the OpenAPI file, as well as list of all possible parameters for that function in \"Associated Parameters\" listbox.  The user can then select a parameter and enter a parameter value.  If the OpenAPI spec contains a description of the parameter, this will be displayed too.</p> <p>The destination for the output for the data source is selectable in the GUI.  Picking the \"File\" radio button in the \"Data to:\" box will bring up a directory selector for output file.  After selecting an output directory, the user should then complete the displayed path with the name of the desired output file.</p> <p>The RET module also allows data obtained from a data source to be transformed before storing to a file, based on the requirements of a target application.</p> <p>When all desired parameters have been entered, the user can press the \"Create request for data\" button which will take all the input values and use an LLM to properly format them and construct a REST call and obtain the data. The code will check if the generated REST call succeeded or failed, and if failed, it will supply the LLM with the originally composed REST call and the error and prompt it to correct the REST call.  This loop of REST generation, execution and validation will occur up to three times before an error is announced and human intervention is requested to correct the generated REST call displayed in the GUI.</p> <p>The retrieved data will then either be written to a file or piplined to an application, as explained above.</p>"},{"location":"modules/RET/#creating-the-configuration-files","title":"Creating the configuration files","text":"<p>The \"schemas\" directory holds two files, \"application_schema.json\" and \"mappings.json\".</p> <p>The \"application_schema.json\" file describes the user applications which can be selected from the RET GUI to become part of the data download, transform and application execution pipeline.  This file emulates the format used by the OpenAPI specification to describe data sources. The name of this file is specified on the command line with \"--transform\" option.</p> <p>The \"mappings.json\" file defines the mapping between a field in the source data schema and the target data schema, as well as the transformation required, where the transmation needs to be defined in the \"transformations.py\" file in the code directory.  Note that in the future this file may be automatically generated by RET, however currently it must be manually created.</p>"},{"location":"modules/SDS/","title":"Sovereignty-Decision-Support-System","text":"<p>Use the table below as an example and replace with the links to the module's DS2 GitHub repository and GitHub project.  </p> Project Links Software GitHub Repository --&gt; Spyderisk software https://github.com/Spyderisk Progress GitHub Project https://github.com/orgs/ds2-eu/projects/47"},{"location":"modules/SDS/#general-description","title":"General Description","text":"<p>This module supports the identification of risks in data management through a comprehensive analysis of the user system. It has a front-end for the user to interact with and a back end with a rich knowledge base where the risks are calculated. It is based on ISO 27005 methodology , and it includes concepts associated with data sovereignty. In an ideal scenario the analysis should be performed including the data provider and data consumer together, but this is possible only in limited context because it requires the sharing of sensitive information. The tool will therefore have an additional module where reports about risks created locally to a user can be analysed by the counterpart in order to take an informed decision about the data sharing</p>"},{"location":"modules/SDS/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the sequence diagram of the risk assessment. </p>"},{"location":"modules/SDS/#component-definition","title":"Component Definition","text":"<p>This module has the following subcomponent and other functions (most of them appear two times in a symmetric fashion in the figure but are discussed only once): DSS Core: * GUI (System Model Creation): This is the graphical interface (front-end) where a user can describe its system in a canvas, provide all the necessary information (a.k.a. system modelling) and perform a risk assessment. For the tier 1 standard connections (Portal etc) it can be perceived as the entry point. This is currently UOS background and will involve minimum development except for a DS2 compliant UI splash page and Dash button integration. * Security Layer: This module oversees managing user authentication and authorization. It is based on Keycloak and will interface with the Identity provider to enable Single Sign On functionalities. The roles of the user will be managed locally. This is currently UOS background and will see little development in DS2, mainly devoted to integrating the security layer with the other identity services in DS2.  * Risk Assessment Engine: This back-end module performs the risk assessment based on the provided system model and exploiting the DS2 risk Knowledge Base. This engine is currently UOS background and will see little development, mainly related to support new concepts defined in the Knowledge Base. This module is also available via a REST API. * DS2 Risk Knowledge Base: This is the knowledge base (KB) underpinning the risk assessment. In the knowledge initialization phase, it will be improved with risks associated to the data sharing and sovereignty aspects. It will also be used by the DARC module for providing the support to configuration feature.  * System Model Repository: This is the repository hosting the saved system models and the local users. This is currently UOS background and no foreseen development in DS2. * Reporting Module: One of the outcomes of DS2 will be the definition of the set of information to be shared with the counterpart to establish mutual trust. The reporting module extracts such info from the risk assessment and provide them to the end-users, so he can share them. Development will include automatic content extraction, formatting, and adherence to established formats (e.g. cyber-essentials). A reporting module already exists in Spyderisk, but it will be improved and enhanced during DS2.</p> <p>SDS Analysis * Report Analysis: The end-user will access a User Interface where he will upload the report received by the counterpart and the information extracted by their own system. The component will provide an analysis and a comparison of the 2 reports. This component will be developed from scratch in DS2.</p> <p>SDS API * The module provides three sets of externally available APIs: * The \u201cRemote Interface for risk assessment\u201d allows the execution of risk assessment calculation via remote interaction, without the use of the GUI. It is provided by the system, but currently not foreseen to be used at the by other components. * The \u201cRemote User Interface to analyse risk report\u201d allows the execution of the remote execution of the risk report analysis. It is provided by the system, but not foreseen to be used by other components. * The \u201cInterface to the KB\u201d provides remote access to the KB and will be used by the DARC module to query the KB.</p> <p>External Components  * DARC. This is the automatic discovery and confirmation module. * Identity Provider. This is the Identity provider used by the DSS internal security layer to get user identities. * Tier1. Service Stack for Marketplace and deployment and API: The full stack will be implemented as generically described elsewhere in this document. Exceptions: The Platform will only be needed for inter-participant service orchestrations if used * System User. This represents a system user that might want to perform risk assessment via a remote interface, rather than via GUI.  </p>"},{"location":"modules/SDS/#screenshots","title":"Screenshots","text":""},{"location":"modules/SDS/#commercial-information","title":"Commercial Information","text":"<p>Table with the organisation, license nature (Open Source, Commercial ... ) and the license. Replace with the values of your module.</p> Organisation (s) License Nature License ICE Open Source Apache 2.0"},{"location":"modules/SDS/#top-features","title":"Top Features","text":"<ul> <li>Risk Assessment</li> <li>currently available at https://github.com/Spyderisk</li> <li>Risk report Comparison.</li> <li>currently under development and not yet available. </li> </ul>"},{"location":"modules/SDS/#how-to-install","title":"How To Install","text":"<p>DS2 version of Spyderisk is maintend as an instance of the SPyderisk project that is available with all the instruction in https://github.com/Spyderisk</p>"},{"location":"modules/SDS/#requirements","title":"Requirements","text":"<p>To be defined. </p>"},{"location":"modules/SDS/#software","title":"Software","text":"<p>n/a</p>"},{"location":"modules/SDS/#summary-of-installation-steps","title":"Summary of installation steps","text":"<p>Currently offered as a service at https://ds2.it-innovation.soton.ac.uk/system-modeller/. </p> <p>To obtain a login write an email to sdm2d11@soton.ac.uk. </p>"},{"location":"modules/SDS/#detailed-steps","title":"Detailed steps","text":"<p>The custom version for DS2 is still uneder development. The software is accessible at https://github.com/Spyderisk </p>"},{"location":"modules/SDS/#how-to-use","title":"How To Use","text":"<ul> <li>Spyderisk online documentation: https://docs.spyderisk.org/system-modeller/latest/</li> </ul>"},{"location":"modules/SDS/#other-information","title":"Other Information","text":"<p>n/a</p>"},{"location":"modules/SDS/#openapi-specification","title":"OpenAPI Specification","text":"<p>n/a</p>"},{"location":"modules/SDS/#additional-links","title":"Additional Links","text":"<p>n/a</p>"},{"location":"modules/SEC/","title":"Multi-cloud Module (MCL)","text":"Project Links Software GitHub Repository https://github.com/ds2-eu/sec_module.git Progress GitHub Project https://github.com/orgs/ds2-eu/projects/23"},{"location":"modules/SEC/#general-description","title":"General Description","text":"<p>Purpose: The DS2 E2C security module (SEC) covers data security, data protection, and privacy with a focus on securing the edge-to-cloud data enablement and ensuring data quality and privacy. This involves implementing secure communication protocols, robust authentication mechanisms, encryption, anonymisation, and continuous monitoring of events in the DS2 ecosystem. The DS2 architecture is designed to handle large volumes of data, facilitating data-driven decision-making while maintaining stringent data security and data privacy standards. The proposed component works in conjunction with all DS2 architecture components. </p> <p>Description: Data collection and preprocessing (e.g., initial data filtering, noise reduction) often happens at the edge at devices level. Depending on the use case for DS2, an edge gateway can be attached to a data source or sensor for securely transmitting data over a secure channel, using an end-to-end encryption (E2EE) protocol. OAuth 2.0 or OpenID Connect is used for authenticating and access control. The infrastructure provides encrypted data storage and tools for privacy aware data management including anonymisation. SEC manages data encryption, both in transit and at rest. A continuous monitoring of security incidents is performed using a Security Information and Event Management (SIEM) system which allows responding to potential security threats.</p>"},{"location":"modules/SEC/#architecture","title":"Architecture","text":"<p>The figure below represents the module fit into the DS-DS environment. </p> <p>The figure below represents the actors, internal structure, primary sub-components, primary DS2 module interfaces, and primary other interfaces of the module. </p>"},{"location":"modules/SEC/#component-definition","title":"Component Definition","text":"<p>This module has the following sub-components and other functions: </p> <p>Authentication and access control:     OAuth 2.0: Provides secure authorisation for users and applications accessing the data. OAuth 2.0 allows third-party services to exchange user information securely without exposing user credentials. This module will be used as is and upon successful authentication, it will return an access token. More specifically, SEC will utilise OAuth 2.0 with Client Credentials Grant since the clients (i.e., sensors, gateways) will use it to obtain an access token outside the context of a (human) user. </p> <p>Audit logging: Every authentication request and subsequent (successful and unsuccessful) authentication access are logged. Audit logs are monitored to detect and respond to block unauthorised access attempts. The log information is stored in the Blockchain of DS2 DRM module.</p> <p>Secure storage: This stores authentication tokens with access control information. Data stored in the storage is encrypted. Note it does not store sensor data. </p> <p>Security information and event management (SIEM): It is a sub-component that aggregates logs and event data generated by all users, servers, edge devices, and firewalls participating in the DS2 infrastructure to monitor and analyse events for security-related incidents detection and response. Like the audit logs, SIEM also stores the incoming log and event data to the DS2 DRM module. This additional logging in blockchain is optional. </p> <p>Secure communication: It establishes and maintains a secure communication channel using secure virtual private cloud (VPC) peering (when communicating among cloud servers) and/or a VPN. All data in transit are protected using SSL/TLS. This module exists in Digiotouch\u2019s cloud based Paradise platform and Digiotouch Edge (which is an Edge Computing platform) and will be used as is in this module. </p> <p>Tokenisation: Replaces sensitive data elements with a non-sensitive equivalent, known as a token, which can be mapped back to the original data. This process helps protect sensitive information by ensuring that tokens, rather than actual data, are used during transactions or storage.  </p> <p>Data anonymisation: This optional sub-component provides the following functions:      Differential privacy: Techniques such as differential privacy add noise to data in a way that statistical properties of the data set are preserved while individual data points are obfuscated. This protects user privacy when data is shared or analysed.      Generalisation and suppression: These techniques modify or remove specific data points to reduce the risk of identifying individuals in a dataset. </p> <p>Security key management for E2EE: This covers the lifecycle of operations which are needed to ensure the security keys (e.g., AES) are created, stored, used, rotated, and destroyed securely. A key management system (KMS) is employed in this sub-component and encrypted keys are stored in the above-mentioned secure storage. These security keys are used for end-to-end data encryption. </p> <p>Sensor: This acts as a data source (e.g., IoT sensor, camera etc.) in the DS2 ecosystem. It is assumed that some sensors are capable of performing authentication, data encryption, and interaction with web services on its own while some sensors do not have such capabilities and are assisted by an Edge gateway. </p> <p>Virtual sensor: It is a software component that mimics the data behaviour of a real sensor. Synthetic data is an output of a virtual sensor. </p> <p>Other data sources: They include public or private repositories of data that are useful for the use case applications. For example, weather data from a region which is available in a public repository. </p> <p>Edge gateway: It is a device which assists sensors with limited processing power with authentication, data encryption, secure data communication etc. </p> <p>Dataspace connector: Enables encrypted data communication through it. </p> <p>Data consumer: It belongs to the consumer participant which consumes encrypted and/or anonymised data coming from the provider participant.  </p>"},{"location":"modules/SEC/#screenshots","title":"Screenshots","text":"<p>This module does not have an user interface.</p>"},{"location":"modules/SEC/#commercial-information","title":"Commercial Information","text":"<p>Table with the organisation, license nature (Open Source, Commercial ... ) and the license. Replace with the values of your module.</p> Organisation (s) License Nature License DIGI Open Source Apache 2.0"},{"location":"modules/SEC/#top-features","title":"Top Features","text":"<ol> <li>Authentication &amp; Access Control: Utilizing OAuth 2.0, the module handles secure authentication for users, devices (e.g., sensors, edge gateways), and applications. It issues access tokens to enforce role-based access control (RBAC), allowing only authorized entities to access or manipulate data.</li> <li>Data Anonymisation &amp; Tokenisation: It includes optional data anonymisation and suppression techniques to protect sensitive data. It also provides tokenisation, replacing sensitive data with reversible, non-sensitive equivalents for secure processing.</li> <li>Audit Logging and Blockchain Integration: Every authentication and access event is logged in a blockchain ledger (via the DS2 DRM module) to ensure tamper-proof tracking of access history, enhancing traceability and trust.</li> <li>Security Information and Event Management (SIEM): It continuously monitors the DS2 infrastructure for security incidents. It aggregates event logs from devices, servers, and services, enabling real-time threat detection and incident response. This module is work in progress</li> </ol>"},{"location":"modules/SEC/#how-to-install","title":"How To Install","text":""},{"location":"modules/SEC/#requirements","title":"Requirements","text":"<p>Provision a Linux VM (Ubuntu 22.04 LTS or later) with 4vCPUs and 8GB RAM. The technical requirements might augment as the MCL module nears completion.</p>"},{"location":"modules/SEC/#software","title":"Software","text":"<p>Docker environment is necessary to run the SEC module.</p>"},{"location":"modules/SEC/#summary-of-installation-steps","title":"Summary of installation steps","text":"<ol> <li>clone the github repo</li> <li>run the docker compose file</li> </ol>"},{"location":"modules/SEC/#detailed-steps","title":"Detailed steps","text":"<ul> <li> <p>Clone the repository <pre><code>git clone https://github.com/ds2-eu/sec_module.git\n</code></pre></p> </li> <li> <p>Run the docker compose file <pre><code>sudo docker compose up\n</code></pre></p> </li> </ul>"},{"location":"modules/SEC/#how-to-use","title":"How To Use","text":"<p>To be done.</p>"},{"location":"modules/SEC/#other-information","title":"Other Information","text":"<p>No other information at the moment for SEC.</p>"},{"location":"modules/SEC/#openapi-specification","title":"OpenAPI Specification","text":"<p>To be done.</p>"},{"location":"modules/SEC/#additional-links","title":"Additional Links","text":"<p>To be done.</p>"}]}